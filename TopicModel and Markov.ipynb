{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore\n",
    "from collections import defaultdict\n",
    "import copy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import test files for topic modelling, will need many more\n",
    "path = 'G:/My Documents/EAGER/FirstPages/' #where are files stored\n",
    "files = os.listdir(path)\n",
    "files = [item for item in files if item.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate pooled text and list of processed documents for topic model\n",
    "combined_text = []\n",
    "documents = []\n",
    "for i in range(len(files)):\n",
    "    with open(path + files[i], 'r') as my_file:\n",
    "        text = my_file.readlines() #this is each separate paragraph including headers etc\n",
    "        for i in [\"\\n\", \"\\t\", \" \"]: #remove the end of sentence tags\n",
    "            text = [item.strip(i) for item in text]\n",
    "        text = filter(lambda x : not x in [' ', '', '\\t'],text) #remove the paragraphs that are just blank space\n",
    "        #loop over text and add title elements to the paragraph they describe\n",
    "        joined = []\n",
    "        join = False\n",
    "        for i in range(len(text)):\n",
    "            if len(text[i]) < 40:\n",
    "                join = True\n",
    "            else:\n",
    "                if join:\n",
    "                    joined.append(text[i-1] + \" \" + text[i])\n",
    "                else:\n",
    "                    joined.append(text[i])\n",
    "        combined_text +=joined # make a big list\n",
    "        documents.append(joined) #make a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenize text for topic modelling\n",
    "def encode_item(text):\n",
    "    clean = \"\"\n",
    "    for item in text.split(\" \"):\n",
    "        try:\n",
    "            clean += str(item).encode('ascii','ignore') + \" \"\n",
    "        except: #just skip things we can't encode for now\n",
    "            pass\n",
    "    return clean.rstrip()\n",
    "stopwords = nltk.corpus.stopwords.words('english') + list(set(string.punctuation))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stem_tokenizer(text):\n",
    "    words = [word for word in nltk.word_tokenize(text)]\n",
    "    words_no_stop = [word for word in words if not word in stopwords]\n",
    "    words_stemmed = [stemmer.stem(word) for word in words_no_stop]\n",
    "    return words_stemmed\n",
    "def return_for_model(text):\n",
    "    all_content = [encode_item(item) for item in text]\n",
    "    all_content_stem_token = map(stem_tokenizer, all_content)\n",
    "    return all_content_stem_token\n",
    "all_tokenized = return_for_model(combined_text)\n",
    "by_document = [return_for_model(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.009*\"use\" + 0.007*\"nram\" + 0.006*\"materi\" + 0.006*\"heat\" + 0.006*\"technolog\" + 0.006*\"thermoelectr\" + 0.006*\"electr\" + 0.005*\"teg\" + 0.005*\"nantero\" + 0.005*\"industri\"'),\n",
       " (1,\n",
       "  u'0.022*\"thermoelectr\" + 0.018*\"materi\" + 0.014*\"technolog\" + 0.009*\"power\" + 0.009*\"heat\" + 0.008*\"product\" + 0.008*\"teg\" + 0.008*\"develop\" + 0.008*\"use\" + 0.008*\"applic\"'),\n",
       " (2,\n",
       "  u'0.021*\"nanotub\" + 0.018*\"technolog\" + 0.010*\"product\" + 0.009*\"applic\" + 0.009*\"carbon\" + 0.008*\"use\" + 0.008*\"the\" + 0.007*\"develop\" + 0.007*\"electr\" + 0.006*\"catalysi\"'),\n",
       " (3,\n",
       "  u'0.013*\"heat\" + 0.012*\"teg\" + 0.012*\"technolog\" + 0.011*\"product\" + 0.010*\"applic\" + 0.010*\"wast\" + 0.010*\"power\" + 0.008*\"temperatur\" + 0.007*\"system\" + 0.006*\"the\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(1)\n",
    "#create topic model on full text\n",
    "#will need to run on server when we have more data\n",
    "full_dict = corpora.Dictionary(all_tokenized)\n",
    "DT_matrix = [full_dict.doc2bow(doc) for doc in all_tokenized]\n",
    "lda = LdaMulticore(DT_matrix, id2word=full_dict, num_topics=4) \n",
    "lda.print_topics(num_topics=4, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#identify topics by paragraph\n",
    "topic_vector = []\n",
    "for doc in by_document:\n",
    "    main_topic = []\n",
    "    #later possibly expand to take into account topic probabilities\n",
    "    for para in doc:\n",
    "        topic_by_prob = sorted([(i[1],i[0]) for i in lda.get_document_topics(full_dict.doc2bow(para))], reverse = True)\n",
    "        main_topic.append(topic_by_prob[0][1])\n",
    "        topic_prob = topic_by_prob[0][0] #in case we want it later\n",
    "    topic_vector.append(main_topic)\n",
    "#add start and end keys for topic_vector\n",
    "for doc in topic_vector:\n",
    "    doc.insert(0, \"start\")\n",
    "    doc.append(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['start', 1, 0, 2, 1, 1, 1, 3, 3, 1, 3, 2, 1, 1, 0, 3, 3, 0, 3, 0, 0, 'end']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_vector[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class markov_representation():\n",
    "    def __init__(self):\n",
    "        self.tokens = defaultdict(lambda: 0)\n",
    "        self.transitions = defaultdict(lambda: defaultdict(lambda:0))\n",
    "        self.for_matrix = defaultdict(lambda: 0)\n",
    "    def add(self, list_of_tokens):\n",
    "        for i in range(len(list_of_tokens) ):\n",
    "            if i <= (len(list_of_tokens)-2):\n",
    "                self.tokens[str(list_of_tokens[i])]+=1\n",
    "                self.transitions[str(list_of_tokens[i])][str(list_of_tokens[i + 1])]+=1\n",
    "                self.for_matrix[(str(list_of_tokens[i]),str(list_of_tokens[i + 1]))] +=1\n",
    "            else: #separate block to add the last token to the list of tokens\n",
    "                self.tokens[str(list_of_tokens[i])]+=1\n",
    "                self.transitions[str(list_of_tokens[i])][None] +=1\n",
    "    def test(self):\n",
    "        #return self.transitions\n",
    "        return self.for_matrix\n",
    "    #to do: add transition matrix\n",
    "    def make_transition_matrix(self):\n",
    "        #tranform dictionary of tuples in self.for_matrix into matrix\n",
    "        pass\n",
    "    def generate_sequence(self):\n",
    "        topic = 'start'\n",
    "        document = ['start']\n",
    "        while topic != 'end' and len(document)<20:\n",
    "            options = self.transitions[topic]\n",
    "            total = sum(options.values())\n",
    "            temp = 0\n",
    "            threshold = random.randint(0, total-1)\n",
    "            for key, value in options.iteritems():\n",
    "                temp += value\n",
    "                if temp>threshold:\n",
    "                    topic = key\n",
    "                    break\n",
    "            document.append(topic)\n",
    "        if document[-1] != 'end':\n",
    "            document.append('end')\n",
    "        return document\n",
    "        #I want to recursively call this to generate a sequence of the required length, but pass on this for now\n",
    "#         if len(document) > min_length:\n",
    "#             return document\n",
    "#         else:\n",
    "#             return self.generate_sequence(self, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['start', '2', '1', '1', '0', 'end']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = markov_representation()\n",
    "for doc in topic_vector:\n",
    "    model.add(doc)\n",
    "model.generate_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>>,\n",
       "            {('0', '0'): 7,\n",
       "             ('0', '1'): 4,\n",
       "             ('0', '2'): 4,\n",
       "             ('0', '3'): 5,\n",
       "             ('0', 'end'): 2,\n",
       "             ('1', '0'): 9,\n",
       "             ('1', '1'): 25,\n",
       "             ('1', '2'): 4,\n",
       "             ('1', '3'): 5,\n",
       "             ('1', 'end'): 4,\n",
       "             ('2', '0'): 1,\n",
       "             ('2', '1'): 8,\n",
       "             ('2', '2'): 17,\n",
       "             ('2', '3'): 4,\n",
       "             ('2', 'end'): 3,\n",
       "             ('3', '0'): 5,\n",
       "             ('3', '1'): 5,\n",
       "             ('3', '2'): 4,\n",
       "             ('3', '3'): 11,\n",
       "             ('3', 'end'): 1,\n",
       "             ('start', '1'): 5,\n",
       "             ('start', '2'): 4,\n",
       "             ('start', '3'): 1})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
