{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract-pages-from-mongo\n",
    "SanjayKAroraPhD@gmail.com <br>\n",
    "November 2018\n",
    "\n",
    "## Description\n",
    "This notebook extracts groups of pages from mongodb by firm_name to create firm-centric page output files that can later be topic modeled.  In doing so, it removes repetitive content (e.g., repeated menu items) and garbage content (e.g., improperly parsed HTML code). \n",
    "\n",
    "## Change log\n",
    "v3 adds the python boilerplate  api for web page cleaning.\n",
    "\n",
    "## TODO:\n",
    "* Need to make better use of all pages in the site, e.g., to improve data quality and use additional paragraph data found on non-homepages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data processing and other libraries\n",
    "import csv\n",
    "import sys\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pprint\n",
    "import pymongo\n",
    "import traceback\n",
    "from time import sleep\n",
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "from IPython.display import display\n",
    "import time\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boilerpipe.extract import Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGODB_DB = \"FirmDB\"\n",
    "MONGODB_COLLECTION = \"pages_co_urls\"\n",
    "CONNECTION_STRING = \"mongodb://localhost\"\n",
    "username = \"scrapy\"\n",
    "password = \"eager\"\n",
    "authSource = \"FirmDB\"\n",
    "authMechanism='SCRAM-SHA-1'\n",
    "\n",
    "client = pymongo.MongoClient(CONNECTION_STRING, username=username, password=password, authSource=authSource, authMechanism=authMechanism)\n",
    "db = client[MONGODB_DB]\n",
    "col = db[MONGODB_COLLECTION]\n",
    "\n",
    "DATA_DIR = '/home/eager/EAGER/data/orgs/workshop/depth0_boilerpipe/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "dict_keys(['Plant Sensory Systems', 'CyboEnergy', 'EMD Technologies Inc.', 'Iogen Corporation', 'NA', 'ACACIA RESEARCH GROUP LLC', 'Immunolight', 'Genesco Inc.', 'Ferro Corporation', 'NanoOncology', 'Lux Bio Group', 'Kinetech Power Company LLC', 'Glucan Biorenewables LLC', 'Pharmatrophix', 'FULL CIRCLE BIOCHAR', 'Ablexis', 'GOAL ZERO LLC', 'Matrix Genetics', 'Gilead Connecticut', 'Mattson Technology', 'Alliance for Sustainable Energy', 'Nanoquantum Sciences', 'Ticona LLC'])\n"
     ]
    }
   ],
   "source": [
    "# gather unique firm_names from mongodb\n",
    "\n",
    "def get_firm_aggregates ():\n",
    "    query = [ { \"$group\": {\"_id\":\"$firm_name\" , \"number\":{\"$sum\":1}} } ]\n",
    "    results = col.aggregate(query)\n",
    "\n",
    "    mongo_dict = {}\n",
    "    for result in results:\n",
    "        key = (result['_id'])\n",
    "        if key:\n",
    "            mongo_dict[key[0]] = result['number']\n",
    "        else:\n",
    "            mongo_dict['NA'] = result['number']\n",
    "    \n",
    "    return mongo_dict\n",
    "\n",
    "results_dict = get_firm_aggregates()\n",
    "firm_names = results_dict.keys()\n",
    "print (len(firm_names))\n",
    "pp = pprint.PrettyPrinter()\n",
    "pp.pprint(firm_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove html content\n",
    "def is_javascript (x):\n",
    "    match_string = r\"(CDATA|return\\s+true|return\\s+false|getelementbyid|function|\\w+\\(.*?\\);|\\w{2,}[\\\\.|:]+\\w{2,}|'\\w+':\\s+'\\w+|\\\\|{|}|\\r|\\n|\\/\\/')\"\n",
    "    # capture CDATA; function declarations; function calls; word sequences separated by a period (e.g., denoting paths)\n",
    "    regex = re.findall(match_string, x) \n",
    "    # check to see if the regex finds some percentage of the words look like javascript patterns\n",
    "    if (len(regex) / float(len(x.split())) > .10):\n",
    "        return True \n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def clean_page_content (text_list):\n",
    "    # remove whatever we think is html\n",
    "    removed_html = filter(lambda x: not( bool(BeautifulSoup(x, \"html.parser\").find()) ), text_list)\n",
    "    # remove content that looks like javascript \n",
    "    removed_js = filter(lambda x: not (is_javascript(x)), removed_html)\n",
    "    # add other checks here as needed\n",
    "\n",
    "    return removed_js\n",
    "    \n",
    "\n",
    "# iterate through each firm, get all pages associated with a firm, and produce data structure\n",
    "# url --> depth\n",
    "#     --> content (list)\n",
    "# return data structure\n",
    "def process_firm (firm_name): \n",
    "    regex = '^' + re.escape(firm_name) + '$'\n",
    "    results = col.find( {\"firm_name\": re.compile(firm_name, re.IGNORECASE) } )\n",
    "    firm_pages_dict = {}\n",
    "    depth0_page_text = [] # home page\n",
    "    for result in results:\n",
    "        key = result['url'][0]\n",
    "        if key:\n",
    "            page_dict = {}\n",
    "            depth = result['depth'][0]\n",
    "            page_dict['depth'] = depth\n",
    "            page_dict['domain'] = result['domain'][0]\n",
    "            result['domain'][0]\n",
    "            page_dict['firm_name'] = firm_name\n",
    "            clnd_text = clean_page_content(result['full_text'])\n",
    "            page_dict['clnd_text'] = clnd_text\n",
    "            extractor = Extractor(extractor='DefaultExtractor', html = result['body'][0])\n",
    "            page_dict['boilerpipe'] = extractor.getText()\n",
    "            firm_pages_dict[key] = page_dict\n",
    "            \n",
    "            if depth == -1:\n",
    "                # depth0_page_text = clnd_text\n",
    "                depth0_page_text = page_dict['boilerpipe']\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return firm_pages_dict, depth0_page_text\n",
    "# TODO: identify which pieces of content are common across all sites, and remove those\n",
    "# def clean_content(firm_dict): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CDATA', 'function', 'getelementbyid', 'javascript.function', 'linker:autoLink', 'www.littlekidsinc', 'fxnCall(param.param);', 'dextr.us', \"'type': 'image\", 'return true', 'return false', 'rev7bynlh\\\\u00252bvcgrjg', '\\\\', '{', '}']\n"
     ]
    }
   ],
   "source": [
    "# regex test \n",
    "regex = re.findall(r\"(CDATA|return\\s+true|return\\s+false|getelementbyid|function|\\w+\\(.*?\\);|\\w{2,}[\\\\.|:]+\\w{2,}|'\\w+':\\s+'\\w+|\\\\|{|}|\\r|\\n|\\/\\/')\", \n",
    "                   \"CDATA function contact-us getelementbyid javascript.function linker:autoLink www.littlekidsinc.com fxnCall(param.param); email@dextr.us 'type': 'image' return true return false rev7bynlh\\\\u00252bvcgrjg\\\\ {height}\") # last part is words sequences separated by punct\n",
    "print (regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<filter object at 0x7f27740ceba8>\n"
     ]
    }
   ],
   "source": [
    "firm_pages_dict, depth0_page_text = process_firm (\"Kinetech Power Company LLC\")\n",
    "print (depth0_page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eager/EAGER/data/orgs/workshop/depth0_boilerpipe/\n"
     ]
    }
   ],
   "source": [
    "print (DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Plant Sensory Systems\n",
      "Working on CyboEnergy\n",
      "Working on EMD Technologies Inc.\n",
      "Working on Iogen Corporation\n",
      "Working on NA\n",
      "Working on ACACIA RESEARCH GROUP LLC\n",
      "Working on Immunolight\n",
      "Working on Genesco Inc.\n",
      "Working on Ferro Corporation\n",
      "Working on NanoOncology\n",
      "Working on Lux Bio Group\n",
      "Working on Kinetech Power Company LLC\n",
      "Working on Glucan Biorenewables LLC\n",
      "Working on Pharmatrophix\n",
      "Working on FULL CIRCLE BIOCHAR\n",
      "Working on Ablexis\n",
      "Working on GOAL ZERO LLC\n",
      "Working on Matrix Genetics\n",
      "Working on Gilead Connecticut\n",
      "Working on Mattson Technology\n",
      "Working on Alliance for Sustainable Energy\n",
      "Working on Nanoquantum Sciences\n",
      "Working on Ticona LLC\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "pp = pprint.PrettyPrinter()\n",
    "for firm_name in firm_names: \n",
    "    print (\"Working on \" + firm_name)\n",
    "    firm_pages_dict, depth0_page_text = process_firm (firm_name)\n",
    "    # pp.pprint(depth0_page_text)\n",
    "    if depth0_page_text: \n",
    "        file = re.sub('\\.|\\/', '_', firm_name) + '.txt'\n",
    "        with io.open(DATA_DIR + file,'w',encoding='utf8') as f:\n",
    "            f.write ('\\n'.join (depth0_page_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
