{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract-pages-from-mongo-v6\n",
    "SanjayKAroraPhD@gmail.com <br>\n",
    "December 2018\n",
    "\n",
    "## Description\n",
    "This version of the notebook extracts groups of pages from mongodb by firm_name to create firm-centric <b>about</b> page output files that can later be topic modeled.  In doing so, it removes repetitive content (e.g., repeated menu items) and garbage content (e.g., improperly parsed HTML code). \n",
    "\n",
    "## Change log\n",
    "v4 focuses on about pages\n",
    "\n",
    "## TODO:\n",
    "* Whole process: get data, topic model and see if it looks sufficiently interesting/different\n",
    "* Enhance data collection, per the following: \n",
    "    * Select a region or country â€” WAIT \n",
    "        * http://www.ivoclarvivadent.com: Please select your region\n",
    "        * https://www.enersys.com/: PLEASE SELECT A REGION\n",
    "        * https://www.m-petfilm.com/: ENGLISH\n",
    "    * Crawl from focal about page only following links that look like part of the about story, maintaining ordering.  Check to see if the other links identified above are also there? \n",
    "        * http://xtalsolar.com/investors_partners.html\n",
    "* Order known about us pages in the same way the links are found on a home page or about us landing page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data processing and other libraries\n",
    "import csv\n",
    "import sys\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pprint\n",
    "import pymongo\n",
    "import traceback\n",
    "from time import sleep\n",
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "from IPython.display import display\n",
    "import time\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import random\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boilerpipe.extract import Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGODB_DB = \"FirmDB_20181226\"\n",
    "MONGODB_COLLECTION = \"pages_ABOUT2\"\n",
    "CONNECTION_STRING = \"mongodb://localhost\"\n",
    "\n",
    "client = pymongo.MongoClient(CONNECTION_STRING)\n",
    "db = client[MONGODB_DB]\n",
    "col = db[MONGODB_COLLECTION]\n",
    "\n",
    "ABOUT_DIR = '/Users/sarora/dev/EAGER/data/orgs/about/'\n",
    "DATA_DIR = '/Users/sarora/dev/EAGER/data/orgs/parsed_page_output/'\n",
    "TRAINING_PERCENT = .10\n",
    "\n",
    "PHRASE_LENGTH = 60\n",
    "MIN_PARA_LEN = 5\n",
    "\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output urls for labeling of training data\n",
    "results = col.find({},{\"url\": 1, \"firm_name\": 1})\n",
    "df = pd.DataFrame(columns = ('firm_name', 'url'))\n",
    "for i in range(results.count()):\n",
    "    result = results.next()\n",
    "    url = result['url'][0]\n",
    "    firm_name = result['firm_name'][0] if 'firm_name' in result else ''\n",
    "    df.loc[i] = [firm_name, url]\n",
    "    \n",
    "df['gid'] = df.groupby(['firm_name']).ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gid.nunique()\n",
    "label_ids = random.sample(range(1, df.gid.nunique()), 200)\n",
    "df_label = df[df['gid'].isin(label_ids)]\n",
    "with open(ABOUT_DIR + 'about_pages_to_label.csv', mode='w') as to_label:\n",
    "    df_label.to_csv(to_label, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>about_lbl</th>\n",
       "      <th>mgmt_lbl</th>\n",
       "      <th>partners_lbl</th>\n",
       "      <th>ip_lbl</th>\n",
       "      <th>about_agg_lbl</th>\n",
       "      <th>gid</th>\n",
       "      <th>pages_in_domain_ftr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>firm_name</th>\n",
       "      <th>url</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">3M Innovative Properties Company</th>\n",
       "      <th>https://www.3m.com/</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.3m.com/3M/en_US/company-us/3m-science-applied-to-life/</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.3m.com/3M/en_US/company-us/about-3m/</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.3m.com/3M/en_US/company-us/about-3m/state-of-science-index-survey/</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.3m.com/3M/en_US/company-us/about-3m/technologies/</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     about_lbl  \\\n",
       "firm_name                        url                                                             \n",
       "3M Innovative Properties Company https://www.3m.com/                                       0.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/3m-scien...        0.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/about-3m/          1.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/about-3m...        0.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/about-3m...        1.0   \n",
       "\n",
       "                                                                                     mgmt_lbl  \\\n",
       "firm_name                        url                                                            \n",
       "3M Innovative Properties Company https://www.3m.com/                                      0.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/3m-scien...       0.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/about-3m/         0.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/about-3m...       0.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/about-3m...       0.0   \n",
       "\n",
       "                                                                                     partners_lbl  \\\n",
       "firm_name                        url                                                                \n",
       "3M Innovative Properties Company https://www.3m.com/                                          0.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/3m-scien...           0.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/about-3m/             0.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/about-3m...           0.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/about-3m...           0.0   \n",
       "\n",
       "                                                                                     ip_lbl  \\\n",
       "firm_name                        url                                                          \n",
       "3M Innovative Properties Company https://www.3m.com/                                    0.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/3m-scien...     0.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/about-3m/       0.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/about-3m...     0.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/about-3m...     0.0   \n",
       "\n",
       "                                                                                     about_agg_lbl  \\\n",
       "firm_name                        url                                                                 \n",
       "3M Innovative Properties Company https://www.3m.com/                                           0.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/3m-scien...            0.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/about-3m/              1.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/about-3m...            0.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/about-3m...            0.0   \n",
       "\n",
       "                                                                                     gid  \\\n",
       "firm_name                        url                                                       \n",
       "3M Innovative Properties Company https://www.3m.com/                                 1.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/3m-scien...  0.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/about-3m/    1.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/about-3m...  0.0   \n",
       "                                 https://www.3m.com/3M/en_US/company-us/about-3m...  0.0   \n",
       "\n",
       "                                                                                     pages_in_domain_ftr  \n",
       "firm_name                        url                                                                      \n",
       "3M Innovative Properties Company https://www.3m.com/                                                  16  \n",
       "                                 https://www.3m.com/3M/en_US/company-us/3m-scien...                   16  \n",
       "                                 https://www.3m.com/3M/en_US/company-us/about-3m/                     16  \n",
       "                                 https://www.3m.com/3M/en_US/company-us/about-3m...                   16  \n",
       "                                 https://www.3m.com/3M/en_US/company-us/about-3m...                   16  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read back labeled data (note that about, management/team and partners, are dichotomous)\n",
    "df_about_labeled = pd.read_csv(ABOUT_DIR + 'about_pages_labeled_v4.csv')\n",
    "df_about_labeled = df_about_labeled.fillna(0)\n",
    "df_about_labeled['pages_in_domain_ftr'] = df_about_labeled.groupby([\"firm_name\"])[\"url\"].transform(\"count\")\n",
    "\n",
    "labeled_urls = list(df_about_labeled['url']) # for training models on labeled urls below\n",
    "df_about_labeled = df_about_labeled.set_index(['firm_name', 'url'])\n",
    "df_about_labeled.head()\n",
    "\n",
    "# final test set is the rows of the original data frame without the urls in df_about_labeled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1455, 7)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_about_labeled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features to predict about pages\n",
    "Create features:\n",
    "1. number of domain pages (identified above)\n",
    "2. whether a given page is an about us page (as opposed to a home page)\n",
    "3. is home page and doesn't have any other pages (is_sole_page_ftr)\n",
    "4. number of words on a page\n",
    "5. number of sentences\n",
    "6. title and url path fragment unigrams (also tried n-grams) with worse results\n",
    "7. descriptor text around the focal link (as identified upstream when crawling as as persisted to mongodb then)\n",
    "\n",
    "Other text-based ideas for features may be found here: https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern regex to remove unwanted words that show up in topic models\n",
    "p = re.compile(r\"(\\(\\)|''|``|\\\"|null|ul|li|ol|^\\.|^:|^/|\\\\|--|cooki|'s|corpor|busi|inc\\.|ltd|co\\.|compan|keyboard|product|technolog)\", flags=re.IGNORECASE)\n",
    "\n",
    "# remove html content\n",
    "def is_javascript (x):\n",
    "    match_string = r\"(CDATA|return\\s+true|return\\s+false|getelementbyid|function|\\w+\\(.*?\\);|\\w{2,}[\\\\.|:]+\\w{2,}|header|hover|'\\w+':\\s+'\\w+|\\\\|{|}|\\r|\\n|\\/\\/')\"\n",
    "    # capture CDATA; function declarations; function calls; word sequences separated by a period (e.g., denoting paths)\n",
    "    regex = re.findall(match_string, x) \n",
    "    # check to see if the regex finds some percentage of the words look like javascript patterns\n",
    "    if (len(regex) / float(len(x.split())) > .10):\n",
    "        return True \n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def clean_page_content (text_list):\n",
    "    # remove whatever we think is html\n",
    "    removed_html = filter(lambda x: not( bool(BeautifulSoup(x, \"html.parser\").find()) ), text_list)\n",
    "    # remove content that looks like javascript \n",
    "    removed_js = filter(lambda x: not (is_javascript(x)), removed_html)\n",
    "    # add other checks here as needed\n",
    "\n",
    "    return removed_js\n",
    "\n",
    "# this method called from below\n",
    "def count_page_features (result): \n",
    "    if not result:\n",
    "        return 0, 0\n",
    "    \n",
    "    # get number of words\n",
    "    running_text = ''\n",
    "    clnd_text = clean_page_content(result['full_text'])\n",
    "    clnd_text = '\\n'.join(clnd_text)\n",
    "    boilerpipe = None\n",
    "\n",
    "    if 'body' in result:\n",
    "        extractor = Extractor(extractor='DefaultExtractor', html = result['body'][0])\n",
    "        lines = extractor.getText().replace(u'\\xa0', u' ').split('\\n')\n",
    "        filtered = filter(lambda x: not re.match(r'^\\s*$', x), lines)\n",
    "        boilerpipe = '\\n'.join(filtered)\n",
    "\n",
    "    # TODO fix to split().  Counting characters currently \n",
    "    if boilerpipe and (len(boilerpipe) > .5 * len(clnd_text)):\n",
    "        running_text += boilerpipe\n",
    "    else:\n",
    "        running_text += clnd_text\n",
    "    \n",
    "    num_words = len(running_text.split())\n",
    "    num_sentences = 0\n",
    "    \n",
    "    # loop over text and add title elements to the paragraph they describe\n",
    "    document = running_text.split('\\n')\n",
    "    for i in range(len(document)): # figure out a way to chunk groups of content\n",
    "        if len(document[i]) <= 12 or len(document[i].split()) < MIN_PARA_LEN : # maybe a menu or simple pagragraph heading? \n",
    "            continue\n",
    "        num_sentences += len(re.findall(r'(\\.|;|\\!)( |$)', document[i])) # count what appears to be number of sentences sentence\n",
    "\n",
    "    # pp.pprint (joined)\n",
    "    return num_words, num_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove simple article words and punctuation (need to keep 'about')\n",
    "stop_words = ['the','a'] + list(string.punctuation) \n",
    "# remove known company names for model training and evaluation in the labeled data \n",
    "remove_regex = re.compile(r'^(3m|united|states|menu|en_us|algeternal|s\\d+|sarepta|skygen|nexgen|abbott|adlens|errorpage|\\d{1,3}|\\d{5,}|\\w+\\d+|\\d+\\w+|asten|johnson|baker|hughes|ge|bhge|biocon|egfr|gcsf|biocon|pegfilgrastim|bostik|canon|chevron|phillips|coloplast|cyberonics|microsoft|evoqua|ford|hitachi|glucanbio|hunter|douglas|kimberly|clark|lextar|fisher|lockheed|martin |lux|nec|nanocopoeia|cisco|schlumberger|weccamerica|inanobio|nanocomposix|zoetis|zygo)$', re.IGNORECASE)\n",
    "# used to filter top-level header content\n",
    "header_in = re.compile('(about|company|corporate|who.we.are|(^|/)vision|awards|profile|corporate|management|team|history|values|strategy|our |technology|research|commercialization)', flags=re.IGNORECASE)\n",
    "header_regex = re.compile(r'h[1-9]+')\n",
    "\n",
    "def get_domain (url):\n",
    "    o = urlparse(url.lower())\n",
    "    domain = o.netloc.strip('www.')\n",
    "    return domain\n",
    "\n",
    "def strip_firm_name (firm_name, text):\n",
    "    strip_regex = re.compile(r\"(\" + \"\\s|\".join(firm_name.split()) + \"\\s)\", re.IGNORECASE)\n",
    "    clnd_text = strip_regex.sub ('', text)\n",
    "    \n",
    "    more_regex = re.compile(r\"([A-Z]\\.?){1,} \")\n",
    "    clnd_text = more_regex.sub ('', clnd_text)\n",
    "    \n",
    "    return ' '.join(clnd_text.split(' '))\n",
    "\n",
    "# standard firm cleaning regex\n",
    "def clean_firm_name (firm):\n",
    "    firm_clnd = re.sub('(\\.|,| corporation| incorporated| llc| inc| international| gmbh| ltd)', '', firm, flags=re.IGNORECASE).rstrip()\n",
    "    return firm_clnd\n",
    "\n",
    "def clean_string(in_string):\n",
    "    if not in_string:\n",
    "        return in_string\n",
    "    split_words = in_string.lower().split()\n",
    "    result_words  = [word for word in split_words if word not in stop_words]\n",
    "    result_words  = [word for word in result_words if not remove_regex.search(word)]\n",
    "    result = ' '.join(result_words)\n",
    "    return ' ' + result\n",
    "\n",
    "def get_page_path_text (url):\n",
    "    o = urlparse(url.lower())\n",
    "    path = o.path\n",
    "    path_parts = path.split ('/')\n",
    "    path_parts = [part.split('.')[0] for part in path_parts] # remove page names\n",
    "    path_parts = [split for part in path_parts for split in part.split('-') ] # split on underscores, hyphens, et al\n",
    "    path_parts = [split for part in path_parts for split in part.split('_') ] \n",
    "    clnd_string = clean_string(' '.join(path_parts))\n",
    "    return clnd_string\n",
    "\n",
    "# recurse through the header text to add into feature grams\n",
    "def get_header_text (headers, names, index):\n",
    "    texts = [clean_string(header.text) for header in headers if header.name == names[index]]\n",
    "    texts = list(filter(header_in.search, texts))\n",
    "    if texts and len(texts[0].split()) > 4:\n",
    "        if(len(names) > (index + 1)):\n",
    "            return get_header_text (headers, names, index + 1)\n",
    "        else:\n",
    "            return ''\n",
    "    else: \n",
    "        return ' '.join (texts)\n",
    "\n",
    "# load page data and create features (this method kicks everything else off)\n",
    "def process_firms (urls): \n",
    "    firm_text_features = {}\n",
    "    firm_count_features = {} # ['is_about', 'num_words', 'num_sentences']\n",
    "    \n",
    "    for url in urls: \n",
    "        result = col.find_one({\"url\": url})\n",
    "        if not result:\n",
    "            result = col.find_one({\"orig_url\": url})\n",
    "        if not result: # just can't find the page\n",
    "            print ('Cannot find ' + url)\n",
    "            continue\n",
    "        \n",
    "        # --------------------\n",
    "        # text based features\n",
    "        # --------------------\n",
    "        firm_name = result\n",
    "        domain = get_domain(url)\n",
    "        \n",
    "        if 'html' not in result:\n",
    "            print ('Cannot find html for', url)\n",
    "            continue\n",
    "            \n",
    "        html = result['html'][0]\n",
    "        \n",
    "        running_text = ''\n",
    "        \n",
    "        # text from the text wrapping the link\n",
    "        descriptor = result['descriptor'][0].replace ('|', '').replace('None', '')\n",
    "        if descriptor: \n",
    "            running_text += clean_string(descriptor)\n",
    "        \n",
    "        # path text within the url \n",
    "        path_text = get_page_path_text(url)\n",
    "        if path_text:\n",
    "            # print (path_text)\n",
    "            running_text += path_text\n",
    "         \n",
    "        # title from the html page\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        if soup.title and soup.title.string:\n",
    "            # print (soup.title.string)\n",
    "            running_text += clean_string(soup.title.string)\n",
    "        \n",
    "        # headers from the page\n",
    "        headers = soup.find_all(header_regex, text=True)\n",
    "        names = sorted(set ([header.name for header in headers]))\n",
    "        running_text += get_header_text (headers, names, 0)\n",
    "\n",
    "        firm_name = result['firm_name'][0]\n",
    "        running_text_clnd = strip_firm_name (firm_name, running_text)\n",
    "        \n",
    "        firm_text_features[url] = running_text_clnd\n",
    "        \n",
    "        # --------------------\n",
    "        # count based features\n",
    "        # --------------------        \n",
    "        is_about = int(result['is_about'][0])\n",
    "        num_words, num_sentences = count_page_features (result)\n",
    "        firm_count_features[url] = [is_about, num_words, num_sentences]\n",
    "        \n",
    "    return firm_text_features, firm_count_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " aboutus businesses\n",
      " path en path to page\n",
      "['Testing', 'this', 'thing']\n",
      " about us\n",
      " about us\n",
      "['about us', 'company']\n",
      "This has been around for a while.\n",
      "is a company.  It has been building vehicles for over a century. H.W.F_ was a nice guy.\n"
     ]
    }
   ],
   "source": [
    "# Test various methods and regex used above \n",
    "print (get_page_path_text ('http://biocon.com/biocon_aboutus_businesses.asp'))\n",
    "\n",
    "print (get_page_path_text('http://www.google.com/path-en/path_to/page.html'))\n",
    "print (re.split(\"\\W+|_\", \"Testing this_thing\"))\n",
    "print (clean_string('3m 01\t08\t100\t10m ford 235 1990 s129 188209 0913lk the ? about us'))\n",
    "print (clean_string('3m 01\t08\t100\t10m ford 235 1990 s129 188209 0913lk the ? about us'))\n",
    "pp.pprint (list(filter(header_in.search, ['about us', 'not found', 'company'])))\n",
    "\n",
    "print (strip_firm_name('Ford Motor Company', 'This Ford Motor Company has been around for a while.'))\n",
    "print (strip_firm_name (clean_firm_name('Ford Motor Company'), 'Ford is a motor company.  It has been building vehicles for over a century. H.W.F_ Ford was a nice guy.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find http://www.genomichealth.com/en-US/\n"
     ]
    }
   ],
   "source": [
    "# get firm website data for n-gram processing AND grab count features\n",
    "labeled_firm_text_features, labeled_firm_count_features = process_firms (labeled_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing, should be  \n",
    "# pages about us about us â€“ about us \n",
    "# [1, 882, 19]\n",
    "print(labeled_firm_text_features['https://nanocomposix.com/pages/about-us'])\n",
    "print(labeled_firm_count_features['https://nanocomposix.com/pages/about-us'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = labeled_firm_text_features.keys() # create in an order\n",
    "print (len(urls))\n",
    "corpus = []\n",
    "for url in urls:\n",
    "    corpus.append (labeled_firm_text_features[url])\n",
    "    \n",
    "# unigram\n",
    "ubv = TfidfVectorizer(min_df=0., max_df=1.)\n",
    "# you can set the n-gram range to 1,2 to get unigrams as well as bigrams (performs worse than just unigrams)\n",
    "# ubv = TfidfVectorizer(ngram_range=(1,2)) \n",
    "\n",
    "ubv_matrix = ubv.fit_transform(corpus)\n",
    "\n",
    "ubv_matrix = ubv_matrix.toarray()\n",
    "vocab = ubv.get_feature_names()\n",
    "ubv_df = pd.DataFrame(ubv_matrix, columns=vocab)\n",
    "ubv_df.index = urls\n",
    "ubv_df.index.name='url'\n",
    "ubv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = pd.DataFrame.from_dict(labeled_firm_count_features, orient='index', columns = ['is_about_ftr', 'num_words_ftr', 'num_sentences_ftr'])\n",
    "count_df.index.name = 'url'\n",
    "count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed above but output here for clarity\n",
    "df_about_labeled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge labeled and feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets (features and labeled data)\n",
    "print(ubv_df.shape)\n",
    "print(df_about_labeled.shape)\n",
    "print(count_df.shape)\n",
    "\n",
    "merged = ubv_df.join(df_about_labeled, how='inner')\n",
    "\n",
    "labeled = merged.join(count_df, how='inner')\n",
    "labeled['num_words_firm_ftr'] = labeled['num_words_ftr'].groupby(level=0).transform('sum')\n",
    "labeled['share_of_words_ftr'] = labeled['num_words_ftr'] / labeled['num_words_firm_ftr']\n",
    "\n",
    "labeled['num_sentences_firm_ftr'] = labeled['num_sentences_ftr'].groupby(level=0).transform('sum')\n",
    "labeled['share_of_sentences_ftr'] = labeled['num_sentences_ftr'] / labeled['num_sentences_firm_ftr']\n",
    "\n",
    "print(labeled.shape)\n",
    "labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing urls in the final labeled data frame\n",
    "for url in labeled_urls: # labeled data\n",
    "    if url not in list(labeled.index.levels[1]): # feature data\n",
    "        print ('Missing', url)\n",
    "\n",
    "labeled.xs('https://nanocomposix.com/pages/about-us', level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled train/test split\n",
    "print (len(ubv_df.columns))\n",
    "X = labeled.iloc[:,1:len(ubv_df.columns)]\n",
    "       \n",
    "# other in other non text-based features\n",
    "# 1. number of domain pages (identified above)\n",
    "# 2. is home page and doesn't have any other pages (is_sole_page)\n",
    "# 3. whether a given page is an about us page (as opposed to a home page)\n",
    "# 4. number of words on a page (and share of words)\n",
    "# 5. number of sentences (and share of sentences)\n",
    "\n",
    "X['pages_in_domain_ftr'] = np.reciprocal(labeled['pages_in_domain_ftr'])\n",
    "# X['is_sole_page_ftr'] = labeled['is_sole_page_ftr']\n",
    "X['is_about_ftr'] = labeled['is_about_ftr']\n",
    "# X['share_of_words_ftr'] = labeled['share_of_words_ftr']\n",
    "# X['num_sentences_ftr'] = labeled['num_sentences_ftr']\n",
    "X['share_of_sentences_ftr'] = labeled['share_of_sentences_ftr']\n",
    "X.to_csv(ABOUT_DIR + 'X.csv', index = True) # for manual inspection\n",
    "\n",
    "# normalize\n",
    "# X = (X - X.mean()) / X.std()\n",
    "# X = (X - X.min()) / (X.max() - X.min())\n",
    "\n",
    "y = labeled.loc[:,'about_lbl']\n",
    "print (X.shape)\n",
    "print (y.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the model\n",
    "On just the labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a few models\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"SVC\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    SVC(gamma=0.001, C=100.), \n",
    "    QuadraticDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataframe for output metrics \n",
    "eval_df = pd.DataFrame (names,index=(range(len(names))), columns=[\"Name\"])\n",
    "eval_df['Accuracy'] = np.float64(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build evaluation outputs (currently limited to accuracy)\n",
    "i = np.int64(0)\n",
    "for name, clf in zip(names, classifiers):\n",
    "    display (name)\n",
    "    scores = cross_val_score(clf, X, y)\n",
    "    avg_score = np.mean(scores)\n",
    "    eval_df.set_value(i, 'Accuracy', avg_score)\n",
    "    i = i + 1\n",
    "    \n",
    "display(eval_df)\n",
    "eval_df.to_clipboard()\n",
    "# Neural net work best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search using MLPClassifier to tune hyperparameters\n",
    "The above results clearly show that a type of feed-forward neural network is the most accurate type of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls = []\n",
    "# hls.append([20,])\n",
    "# hls.append([70,])\n",
    "hls.append([100,])\n",
    "# hls.append([50,50])\n",
    "# hls.append([70,70,70])\n",
    "# hls.append([40,40,40])\n",
    "# hls.append([10,10,10])\n",
    "# hls.append([50,50,50,50])\n",
    "pp.pprint(hls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'solver': ['adam'], 'max_iter': [40], 'alpha': 10.0 ** -np.arange(1, 3), 'hidden_layer_sizes': hls, 'random_state':[20]}\n",
    "clf_grid = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)\n",
    "clf_grid.fit(X,y)\n",
    "\n",
    "print(\"Best score: %0.4f\" % clf_grid.best_score_)\n",
    "print(\"Using the following parameters:\")\n",
    "print(clf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train neural net model with best hyperparameter configuration\n",
    "clf = MLPClassifier(alpha=0.1, hidden_layer_sizes=(100,), max_iter=40, random_state=20, solver='adam')\n",
    "clf.fit(X, y)\n",
    "\n",
    "y_hat = clf.predict(X)\n",
    "print(confusion_matrix(y, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [ {'C': [100, 500, 1000, 2000], 'gamma': [.01, 0.001], 'kernel': ['rbf']},]\n",
    "clf_grid = GridSearchCV(SVC(), parameters, n_jobs=-1)\n",
    "clf_grid.fit(X,y)\n",
    "\n",
    "print(\"Best score: %0.4f\" % clf_grid.best_score_)\n",
    "print(\"Using the following parameters:\")\n",
    "print(clf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train SVC model with best hyperparameter configuration\n",
    "clf = SVC(C=1000., gamma=0.001, kernel='rbf')\n",
    "clf.fit(X, y)\n",
    "\n",
    "y_hat = clf.predict(X)\n",
    "print(confusion_matrix(y, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print all instances where predictions don't match labels (for inspection)\n",
    "for key, y_i, y_hat_i in zip(list(X.index), y, y_hat):\n",
    "    if y_i != y_hat_i:\n",
    "        print(key[1], 'has been classified as ', y_hat_i, 'but should be ', y_i) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict about pages for unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare domain level features \n",
    "df_predict = df[~df['url'].isin(labeled_urls)] \n",
    "df_predict['pages_in_domain_ftr'] = df_predict.groupby([\"firm_name\"])[\"url\"].transform(\"count\")\n",
    "\n",
    "df_predict = df_predict.set_index(['firm_name', 'url'])\n",
    "df_predict = df_predict.fillna(0)\n",
    "\n",
    "df_predict.sort_values(by=['gid']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see whether there are duplicate urls\n",
    "# note: there should be because different assignees may map to the same domain (see error above)\n",
    "counter=collections.Counter(df_predict.index)\n",
    "most_common = counter.most_common(5)\n",
    "pp.pprint (most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare n-gram and count features\n",
    "unlabeled_firm_text_features, unlabeled_firm_count_features = process_firms (set(df_predict.index.get_level_values('url')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_urls = unlabeled_firm_text_features.keys()\n",
    "\n",
    "pred_corpus = []\n",
    "for url in prediction_urls:\n",
    "    pred_corpus.append (unlabeled_firm_text_features[url])\n",
    "\n",
    "ubv_prediction_matrix = ubv.transform(pred_corpus)\n",
    "\n",
    "ubv_prediction_matrix = ubv_prediction_matrix.toarray()\n",
    "vocab = ubv.get_feature_names()\n",
    "ubv_prediction_df = pd.DataFrame(ubv_prediction_matrix, columns=vocab)\n",
    "ubv_prediction_df.index = prediction_urls\n",
    "ubv_prediction_df.index.name='url'\n",
    "ubv_prediction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_pred_df = pd.DataFrame.from_dict(unlabeled_firm_count_features, orient='index', columns = ['is_about_ftr', 'num_words_ftr', 'num_sentences_ftr'])\n",
    "count_pred_df.index.name = 'url'\n",
    "count_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets (features and labeled data)\n",
    "print(ubv_prediction_df.shape)\n",
    "print(df_predict.shape)\n",
    "print(count_pred_df.shape)\n",
    "\n",
    "predict_merged = ubv_prediction_df.join(df_predict, how='inner')\n",
    "\n",
    "unlabeled = predict_merged.join(count_pred_df, how='inner')\n",
    "unlabeled['num_sentences_firm_ftr'] = unlabeled['num_sentences_ftr'].groupby(level=0).transform('sum')\n",
    "unlabeled['share_of_sentences_ftr'] = unlabeled['num_sentences_ftr'] / unlabeled['num_sentences_firm_ftr']\n",
    "\n",
    "unlabeled = unlabeled.sort_values(by=['gid']).head()\n",
    "print(unlabeled.shape)\n",
    "unlabeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge\n",
    "X_test = unlabeled.iloc[:,1:len(ubv_prediction_df.columns)]\n",
    "\n",
    "print (X.shape)\n",
    "print (X_test.shape) # should be the same number of cols\n",
    "\n",
    "X_test['pages_in_domain_ftr'] = np.reciprocal(unlabeled['pages_in_domain_ftr'])\n",
    "X_test['is_about_ftr'] = unlabeled['is_about_ftr']\n",
    "X_test['share_of_sentences_ftr'] = unlabeled['share_of_sentences_ftr']\n",
    "X_test.to_csv(ABOUT_DIR + 'X_test.csv', index = True) # for manual inspection\n",
    "\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with newly constructed X\n",
    "y_predicted = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "with open(ABOUT_DIR + 'about_predicted_and_labels.csv', mode='w') as about_file:\n",
    "    about_writer = csv.writer(about_file, delimiter=',', quotechar='\"')\n",
    "    about_writer.writerow(['firm_name', 'url', 'is_about'])\n",
    "    # output predicted values to file\n",
    "    for fn, u, predicted_value in zip(X_test.index.get_level_values('firm_name'), X_test.index.get_level_values('url'), y_predicted):\n",
    "        # print (fn + ' with url ' + u + ' has predicted value ' + str(predicted_value))\n",
    "        about_writer.writerow([clean_firm_name(fn), u, predicted_value])\n",
    "    # and the labeled ones too...\n",
    "    for fn, u, labeled_value in zip(X.index.get_level_values('firm_name'), X.index.get_level_values('url'), y):\n",
    "        # print (fn + ' with url ' + u + ' has predicted value ' + str(labeled_value))\n",
    "        about_writer.writerow([clean_firm_name(fn), u, labeled_value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from mongodb\n",
    "* Now that we know which pages are about pages, extract from mongodb and output for topic modeling\n",
    "* For now, construct paragraphs from different pages by ordering urls by their length.  In the future, might want to contruct paragraphs in their 'natural' sequential order as they would appear on a home page or landing page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine both labeled and predicted frames\n",
    "print (X_test.shape)\n",
    "print(X.shape)\n",
    "\n",
    "combined = X_test.append(X)\n",
    "print (combined.shape)\n",
    "print (len(y_predicted))\n",
    "print (len(y))\n",
    "abouts = pd.DataFrame(index=combined.index)\n",
    "\n",
    "abouts['is_about'] = list(y_predicted) + list(y)\n",
    "abouts = abouts.reset_index()\n",
    "abouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather unique firm_names from mongodb\n",
    "firm_names = set(abouts['firm_name'])\n",
    "print (len(firm_names))\n",
    "pp = pprint.PrettyPrinter()\n",
    "pp.pprint(firm_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ordered_about_urls (firm_name):\n",
    "    urls = list (abouts.loc[(abouts['firm_name'] == firm_name) & (abouts['is_about'] == 1), 'url'])\n",
    "    urls.sort(key = len)\n",
    "    # print ('Original urls')\n",
    "    # pp.pprint(urls)\n",
    "\n",
    "    index = {}\n",
    "    for url in urls:\n",
    "        path_fragments = len(url.split('/'))\n",
    "        added = False\n",
    "        for i in range(1, path_fragments):\n",
    "            key_phrase = url.rsplit('/', maxsplit=i)[0]\n",
    "            if key_phrase in urls or (key_phrase + '/') in urls: \n",
    "                od = index.setdefault(key_phrase, OrderedDict())\n",
    "                od[url] = 1\n",
    "                added = True\n",
    "                continue\n",
    "        if not added:\n",
    "            od = index.setdefault(url, OrderedDict())\n",
    "            od[url] = 1\n",
    " \n",
    "    # pp.pprint (index)\n",
    "    \n",
    "    return_urls = [] \n",
    "    seen = set ()\n",
    "    for key in index.keys():\n",
    "        tree_urls = index[key]\n",
    "        for fu in tree_urls:\n",
    "            if fu not in seen:\n",
    "                return_urls.appju8j nend(fu)\n",
    "                seen.add(fu)\n",
    "    \n",
    "    # finally remove home page if it exists and if there are other pages to draw on\n",
    "    if not return_urls: \n",
    "        return None\n",
    "    else: \n",
    "        first_page = return_urls[0]\n",
    "        first_page_path = get_page_path_text (first_page)\n",
    "        if first_page_path == ' ' or first_page_path == '':\n",
    "            print (first_page_path + 'empty')\n",
    "            return_urls.pop(0)\n",
    "        return return_urls\n",
    "\n",
    "test_urls = get_ordered_about_urls ('Previvo Genetics')\n",
    "print ('Ordered urls')\n",
    "pp.pprint (test_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through firm urls and return concatenated string\n",
    "def get_content (urls): \n",
    "    running_text = ''\n",
    "    for url in urls:\n",
    "        print ('\\tWorking on ' + url)\n",
    "        result = col.find_one( {\"url\": url} )\n",
    "        if result:\n",
    "            clnd_text = clean_page_content(result['full_text'])\n",
    "            clnd_text = '\\n'.join(clnd_text)\n",
    "            boilerpipe = None\n",
    "            \n",
    "            if 'body' in result:\n",
    "                extractor = Extractor(extractor='DefaultExtractor', html = result['body'][0])\n",
    "                lines = extractor.getText().replace(u'\\xa0', u' ').split('\\n')\n",
    "                filtered = filter(lambda x: not re.match(r'^\\s*$', x), lines)\n",
    "                boilerpipe = '\\n'.join(filtered)\n",
    "\n",
    "            # TODO fix to split().  Counting characters currently \n",
    "            if boilerpipe and (len(boilerpipe) > .5 * len(clnd_text)):\n",
    "                print ('\\t\\tUsing boilerplate')\n",
    "                running_text += boilerpipe\n",
    "            else:\n",
    "                print ('\\t\\tUsing clnd_text')\n",
    "                running_text += clnd_text\n",
    "        else:\n",
    "            print ('Cannot find url: ' + url)\n",
    "\n",
    "    return running_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex test \n",
    "regex = re.findall(r\"(CDATA|return\\s+true|return\\s+false|getelementbyid|function|\\w+\\(.*?\\);|\\w{2,}[\\\\.|:]+\\w{2,}|'\\w+':\\s+'\\w+|\\\\|{|}|\\r|\\n|\\/\\/')\", \n",
    "                   \"CDATA function contact-us getelementbyid javascript.function linker:autoLink www.littlekidsinc.com fxnCall(param.param); email@dextr.us 'type': 'image' return true return false rev7bynlh\\\\u00252bvcgrjg\\\\ {height}\") # last part is words sequences separated by punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_site_text = get_content (test_urls)\n",
    "print (test_site_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run process_firm and write to file\n",
    "pp = pprint.PrettyPrinter()\n",
    "for firm_name in firm_names: \n",
    "    print (\"Working on \" + firm_name)\n",
    "    about_urls = get_ordered_about_urls(firm_name)\n",
    "    if not about_urls:\n",
    "        print (\"\\tCouldn't find any urls for firm!\")\n",
    "        firm_urls = df_predict.xs(firm_name, level=0)\n",
    "        home_page = firm_urls.loc[firm_urls['is_about_ftr'] == 0]\n",
    "        about_urls.append(home_page.index.get_level_values('url'))\n",
    "        \n",
    "    about_text = get_content (about_urls)\n",
    "    \n",
    "    if about_text: \n",
    "        firm_clnd = clean_firm_name(firm_name) # standard cleaning code throughout project\n",
    "        about_clnd = strip_firm_name (firm_name, about_text)\n",
    "        file = re.sub('\\/', '|', firm_clnd) + '.txt'\n",
    "        with io.open(DATA_DIR + file,'w',encoding='utf8') as f:\n",
    "            f.write (about_clnd)\n",
    "    else:\n",
    "        print (\"\\tCouldn't find any text for firm!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
