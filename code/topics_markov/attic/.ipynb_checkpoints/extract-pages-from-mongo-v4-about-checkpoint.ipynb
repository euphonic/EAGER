{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract-pages-from-mongo\n",
    "SanjayKAroraPhD@gmail.com <br>\n",
    "December 2018\n",
    "\n",
    "## Description\n",
    "This version of the notebook extracts groups of pages from mongodb by firm_name to create firm-centric <b>about</b> page output files that can later be topic modeled.  In doing so, it removes repetitive content (e.g., repeated menu items) and garbage content (e.g., improperly parsed HTML code). \n",
    "\n",
    "## Change log\n",
    "v4 focuses on about pages\n",
    "\n",
    "## TODO:\n",
    "* Whole process: get data, topic model and see if it looks sufficiently interesting/different\n",
    "* Enhance data collection, per the following: \n",
    "    * Train a model to see if home page (or any other page) looks like an about page — WAIT\n",
    "        * http://forestconcepts.com/index.php?page=01005\n",
    "        * https://www.itri.org.tw/eng/ \n",
    "        * http://paxscientific.com/history/ \n",
    "    * Select a region or country — WAIT \n",
    "        * http://www.ivoclarvivadent.com: Please select your region\n",
    "        * https://www.enersys.com/: PLEASE SELECT A REGION\n",
    "        * https://www.m-petfilm.com/: ENGLISH\n",
    "    * Crawl from focal about page only following links that look like part of the about story, maintaining ordering.  Check to see if the other links identified above are also there? \n",
    "        * http://xtalsolar.com/investors_partners.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data processing and other libraries\n",
    "import csv\n",
    "import sys\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pprint\n",
    "import pymongo\n",
    "import traceback\n",
    "from time import sleep\n",
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "from IPython.display import display\n",
    "import time\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import random\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boilerpipe.extract import Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first figure out what is an about page. need to label training data\n",
    "# identify features -- for right now, unigrams if just one word in the header, otherwise bi- or trigrams \n",
    "# predict about pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGODB_DB = \"FirmDB_20181226\"\n",
    "MONGODB_COLLECTION = \"pages_ABOUT\"\n",
    "CONNECTION_STRING = \"mongodb://localhost\"\n",
    "\n",
    "client = pymongo.MongoClient(CONNECTION_STRING)\n",
    "db = client[MONGODB_DB]\n",
    "col = db[MONGODB_COLLECTION]\n",
    "\n",
    "DATA_DIR = '/Users/sarora/dev/EAGER/data/orgs/about/'\n",
    "TRAINING_PERCENT = .10\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain (url):\n",
    "    o = urlparse(url.lower())\n",
    "    domain = o.netloc.strip('www.')\n",
    "    return domain\n",
    "\n",
    "# output urls for labeling of training data\n",
    "results = col.find({},{\"url\": 1, \"firm_name\": 1})\n",
    "df = pd.DataFrame(columns = ('firm_name', 'url', 'label'))\n",
    "domain_count = defaultdict(lambda:0,{})\n",
    "for i in range(results.count()):\n",
    "    result = results.next()\n",
    "    url = result['url'][0]\n",
    "    domain_count[get_domain(url)] += 1\n",
    "    firm_name = result['firm_name'][0] if 'firm_name' in result else ''\n",
    "    df.loc[i] = [firm_name, url, '']\n",
    "    \n",
    "df['gid'] = df.groupby(['firm_name']).ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gid.nunique()\n",
    "label_ids = random.sample(range(1, df.gid.nunique()), 200)\n",
    "df_label = df[df['gid'].isin(label_ids)]\n",
    "with open(DATA_DIR + 'about_pages_to_label.csv', mode='w') as to_label:\n",
    "    df_label.to_csv(to_label, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['firm_name', 'about', 'mgmt', 'partners', 'gid', 'pages_in_domain', 'is_sole_page']\n"
     ]
    }
   ],
   "source": [
    "# read back labeled data (note that about, management/team and partners, are dichotomous)\n",
    "df_about_labeled = pd.read_csv(DATA_DIR + 'about_pages_labeled.csv')\n",
    "df_about_labeled = df_about_labeled.fillna(0)\n",
    "\n",
    "# count pages per domain\n",
    "for index, row in df_about_labeled.iterrows():\n",
    "    pages_in_domain = domain_count[get_domain(row['url'])]\n",
    "    df_about_labeled.loc[index,'pages_in_domain'] = pages_in_domain\n",
    "    is_sole_page = 0 if pages_in_domain > 1 else 1\n",
    "    df_about_labeled.loc[index,'is_sole_page'] = is_sole_page\n",
    "    \n",
    "labeled_urls = list(df_about_labeled['url']) # for training models on labeled urls below\n",
    "df_about_labeled = df_about_labeled.set_index('url')\n",
    "print (df_about_labeled.columns.tolist())\n",
    "\n",
    "# final test set is the rows of the original data frame without the urls in df_about_labeled "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features to predict about pages\n",
    "Create features:\n",
    "1. title and url path fragment unigrams (also tried n-grams, as well as content from headers, with worse results) \n",
    "2. is home page and doesn't have any other pages\n",
    "3. other ideas here: https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.acebrand.com/3M/en_US/ace-brand/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.scotch-brite.com/3M/en_US/scotch-brite/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.scotchbrand.com/3M/en_US/scotch-brand/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.scotchgard.com/3M/en_US/scotchgard/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.command.com/3M/en_US/command/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.post-it.com/3M/en_US/post-it/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.futuro-usa.com/3M/en_US/futuro-us/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.filtrete.com/3M/en_US/filtrete/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.scotchblue.com/3M/en_US/scotchblue/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.nexcare.com/3M/en_US/nexcare/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.3m.com/3M/en_US/automotive-us/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.3m.com/3M/en_US/commercial-solutions-us/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.3m.com/3M/en_US/communications-us/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.3m.com/3M/en_US/consumer-us/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.3m.com/3M/en_US/design-construction-us/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.3m.com/3M/en_US/electronics-us/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.3m.com/3M/en_US/energy-us/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.3m.com/3M/en_US/health-care-us/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.3m.com/3M/en_US/manufacturing-us/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.3m.com/3M/en_US/mining-oil-gas-us/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.3m.com/3M/en_US/safety-us/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.3m.com/3M/en_US/transportation-us/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://pointofcare.abbott\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'/'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://med.stanford.edu/profiles/thomas-quertermous?tab=research-and-scholarship\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://biox.stanford.edu\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.stanford.edu/group/shatzlab/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://globalgenes.org/rare-diseases-facts-statistics/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.hemophilia.ca/files/All%20About%20VWD%202011%20-%20FINAL.pdf\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.cdc.gov/ncbddd/vwd/facts.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.biocon.com/docs/PolicyDocument_MaterialSubsidiary.pdf\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.cpchem.com\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://tools.google.com/dlpage/gaoptout\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.google.com/settings/ads/plugin\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://usa.baidu.com/privacy/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.addthis.com/privacy/opt-out\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.aboutads.info/choices/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.networkadvertising.org/choices/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://youronlinechoices.com/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.facebook.com/ads/settings\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://support.microsoft.com/en-gb/help/17442/windows-internet-explorer-delete-manage-cookies\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://support.google.com/chrome/answer/95647?co=GENIE.Platform%3DDesktop&hl=en\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://support.mozilla.org/en-US/kb/enable-and-disable-cookies-website-preferences\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://support.apple.com/kb/PH21411?locale=en_US\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'//'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://habitatcaz.org/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.gic.com.kw\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.dialogic.com/en/company/cookies.aspx\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.dialogic.com/den/default.aspx\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.dialogic.com/den/developer_forums/f/default.aspx\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.dialogic.com/company/privacy.aspx\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.openboard.info/fslr/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.nxp.com/webapp-signup/updateUser\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "# load page data and create features\n",
    "firm_page_features = {}\n",
    "# header_regex = re.compile(r'h[1-2]+')\n",
    "stop_words = ['the','a'] + list(string.punctuation) # remove simple article words and punctuation (need to keep 'about')\n",
    "# remove known company names for model training and evaluation in the labeled data \n",
    "remove_regex = re.compile(r'^(3m|united|states|en_us|algeternal|s\\d+|sarepta|skygen|nexgen|abbott|adlens|errorpage|\\d{1,3}|\\d{5,}|asten|johnson|baker|hughes|ge|bhge|biocon|egfr|gcsf|biocon|pegfilgrastim|bostik|canon|chevron|phillips|coloplast|cyberonics|microsoft|evoqua|ford|hitachi|glucanbio|hunter|douglas|kimberly|clark|lextar|fisher|lockheed|martin |lux|nec|nanocopoeia|cisco|schlumberger|weccamerica|inanobio|nanocomposix|zoetis|zygo)$')\n",
    "\n",
    "def clean_string(in_string):\n",
    "    if not in_string:\n",
    "        return in_string\n",
    "    split_words = in_string.lower().split()\n",
    "    result_words  = [word for word in split_words if word not in stop_words]\n",
    "    result_words  = [word for word in result_words if not remove_regex.search(word)]\n",
    "    result = ' '.join(result_words)\n",
    "    return ' ' + result\n",
    "\n",
    "def get_page_path_text (url):\n",
    "    o = urlparse(url.lower())\n",
    "    path = o.path\n",
    "    path_parts = path.split ('/')\n",
    "    path_parts = [part.split('.')[0] for part in path_parts] # remove page names\n",
    "    path_parts = [split for part in path_parts for split in part.split('-') ] # split on underscores, hyphens, et al\n",
    "    path_parts = [split for part in path_parts for split in part.split('_') ] # split on underscores, hyphens, et al\n",
    "    clnd_string = clean_string(' '.join(path_parts))\n",
    "    return clnd_string\n",
    "\n",
    "# remove html content\n",
    "def is_javascript (x):\n",
    "    match_string = r\"(CDATA|return\\s+true|return\\s+false|getelementbyid|function|\\w+\\(.*?\\);|\\w{2,}[\\\\.|:]+\\w{2,}|'\\w+':\\s+'\\w+|\\\\|{|}|\\r|\\n|\\/\\/')\"\n",
    "    # capture CDATA; function declarations; function calls; word sequences separated by a period (e.g., denoting paths)\n",
    "    regex = re.findall(match_string, x) \n",
    "    # check to see if the regex finds some percentage of the words look like javascript patterns\n",
    "    if (len(regex) / float(len(x.split())) > .10):\n",
    "        return True \n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def clean_page_content (text_list):\n",
    "    # remove whatever we think is html\n",
    "    removed_html = filter(lambda x: not( bool(BeautifulSoup(x, \"html.parser\").find()) ), text_list)\n",
    "    # remove content that looks like javascript \n",
    "    removed_js = filter(lambda x: not (is_javascript(x)), removed_html)\n",
    "    # add other checks here as needed\n",
    "\n",
    "    return removed_js\n",
    "# TODO with version 3 code \n",
    "# def get_words_on_page (url)\n",
    "     \n",
    "def process_firms (urls): \n",
    "    for url in urls: \n",
    "        result = col.find_one({\"url\": url})\n",
    "        \n",
    "        url = result['url'][0]\n",
    "        domain = get_domain(url)\n",
    "        html = result['html'][0]\n",
    "\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        running_text = ''\n",
    "        path_text = get_page_path_text(url)\n",
    "        if path_text:\n",
    "            running_text += path_text\n",
    "        if soup.title and soup.title.string:\n",
    "            running_text += clean_string(soup.title.string)\n",
    "#         headers = soup.find_all(header_regex, text=True)\n",
    "#         headers_text = [clean_string(header.text) for header in headers]\n",
    "                \n",
    "#         for header_text in (headers_text):\n",
    "#             running_text += ' ' + header_text\n",
    "\n",
    "        clnd_text = '\\n'.join(clean_page_content(result['full_text']))\n",
    "        if 'body' in result:\n",
    "            extractor = Extractor(extractor='DefaultExtractor', html = result['body'][0])\n",
    "            lines = extractor.getText().replace(u'\\xa0', u' ').split('\\n')\n",
    "            filtered = filter(lambda x: not re.match(r'^\\s*$', x), lines)\n",
    "            boilerpipe = '\\n'.join(filtered)\n",
    "\n",
    "        # TODO: write to a date frame as argument \n",
    "        if boilerpipe and (len(boilerpipe) > (.5 * len (clnd_text))):\n",
    "            df_about_labeled.loc[url,'page_words'] = len(boilerpipe.split())\n",
    "        else:\n",
    "            df_about_labeled.loc[url,'page_words'] = len(clnd_text.split())\n",
    "\n",
    "        firm_page_features[url] = running_text\n",
    "\n",
    "process_firms (labeled_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_about_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (get_page_path_text('http://www.google.com/path1-en/path2_to/page.html'))\n",
    "print (re.split(\"\\W+|_\", \"Testing this_thing\"))\n",
    "print (clean_string('3m 01\t08\t100\t10m ford 235 1990 s129 188209 0913lk the ? about us'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can set the n-gram range to 1,2 to get unigrams as well as bigrams\n",
    "# TODO: refactor into method\n",
    "urls = firm_page_features.keys()\n",
    "print (len(urls))\n",
    "corpus = []\n",
    "for url in urls:\n",
    "    corpus.append (firm_page_features[url])\n",
    "    \n",
    "# unigram\n",
    "ubv = TfidfVectorizer(min_df=0., max_df=1.)\n",
    "# ngrams (performs worse than just unigrams)\n",
    "# ubv = TfidfVectorizer(ngram_range=(3,3))\n",
    "ubv_matrix = ubv.fit_transform(corpus)\n",
    "\n",
    "ubv_matrix = ubv_matrix.toarray()\n",
    "vocab = ubv.get_feature_names()\n",
    "ubv_df = pd.DataFrame(ubv_matrix, columns=vocab)\n",
    "ubv_df.index = urls\n",
    "# print(ubv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge two datasets (features and labeled data)\n",
    "print(ubv_df.shape)\n",
    "print(df_about_labeled.shape)\n",
    "\n",
    "all_merged = ubv_df.join(df_about_labeled, how='outer', rsuffix='_lbl')\n",
    "print(all_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split labeled and predict datasets \n",
    "labeled = all_merged[all_merged['gid'].notnull()]\n",
    "print(labeled.shape)\n",
    "\n",
    "to_predict = all_merged[all_merged['gid'].isnull()]\n",
    "to_predict = to_predict.fillna(0)\n",
    "print(to_predict.shape)\n",
    "\n",
    "print (df_about_labeled.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled train/test split\n",
    "X = labeled.iloc[:,1:len(ubv_df.columns)]\n",
    "X['pages_in_domain'] = labeled['pages_in_domain']\n",
    "X['is_sole_page'] = labeled['is_sole_page']\n",
    "# X['page_words'] = labeled['page_words']\n",
    "\n",
    "y = labeled.loc[:,'about_lbl']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the model\n",
    "On just the labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a few models\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"SVC\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    SVC(gamma=0.001, C=100.), \n",
    "    QuadraticDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataframe for output metrics \n",
    "eval_df = pd.DataFrame (names,index=(range(len(names))), columns=[\"Name\"])\n",
    "eval_df['Accuracy'] = np.float64(0)\n",
    "display (eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build evaluation outputs (currently limited to accuracy)\n",
    "i = np.int64(0)\n",
    "for name, clf in zip(names, classifiers):\n",
    "    display (name)\n",
    "    scores = cross_val_score(clf, X, y)\n",
    "    avg_score = np.mean(scores)\n",
    "    eval_df.set_value(i, 'Accuracy', avg_score)\n",
    "    i = i + 1\n",
    "    \n",
    "display(eval_df)\n",
    "eval_df.to_clipboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search using MLPClassifier to tune hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'solver': ['lbfgs'], 'max_iter': [500,1000,1500], 'alpha': 10.0 ** -np.arange(1, 7), 'hidden_layer_sizes':np.arange(5, 12), 'random_state':[0,1,2,3,4,5,6,7,8,9]}\n",
    "clf_grid = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)\n",
    "clf_grid.fit(X,y)\n",
    "\n",
    "print(\"Best score: %0.4f\" % clf_grid.best_score_)\n",
    "print(\"Using the following parameters:\")\n",
    "print(clf_grid.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
