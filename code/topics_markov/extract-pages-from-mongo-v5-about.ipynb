{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract-pages-from-mongo-v5\n",
    "SanjayKAroraPhD@gmail.com <br>\n",
    "December 2018\n",
    "\n",
    "## Description\n",
    "This version of the notebook extracts groups of pages from mongodb by firm_name to create firm-centric <b>about</b> page output files that can later be topic modeled.  In doing so, it removes repetitive content (e.g., repeated menu items) and garbage content (e.g., improperly parsed HTML code). \n",
    "\n",
    "## Change log\n",
    "v4 focuses on about pages\n",
    "\n",
    "## TODO:\n",
    "* Whole process: get data, topic model and see if it looks sufficiently interesting/different\n",
    "* Enhance data collection, per the following: \n",
    "    * Select a region or country â€” WAIT \n",
    "        * http://www.ivoclarvivadent.com: Please select your region\n",
    "        * https://www.enersys.com/: PLEASE SELECT A REGION\n",
    "        * https://www.m-petfilm.com/: ENGLISH\n",
    "    * Crawl from focal about page only following links that look like part of the about story, maintaining ordering.  Check to see if the other links identified above are also there? \n",
    "        * http://xtalsolar.com/investors_partners.html\n",
    "* Order known about us pages in the same way the links are found on a home page or about us landing page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data processing and other libraries\n",
    "import csv\n",
    "import sys\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pprint\n",
    "import pymongo\n",
    "import traceback\n",
    "from time import sleep\n",
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "from IPython.display import display\n",
    "import time\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import random\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boilerpipe.extract import Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGODB_DB = \"FirmDB_20181226\"\n",
    "MONGODB_COLLECTION = \"pages_ABOUT\"\n",
    "CONNECTION_STRING = \"mongodb://localhost\"\n",
    "\n",
    "client = pymongo.MongoClient(CONNECTION_STRING)\n",
    "db = client[MONGODB_DB]\n",
    "col = db[MONGODB_COLLECTION]\n",
    "\n",
    "ABOUT_DIR = '/Users/sarora/dev/EAGER/data/orgs/about/'\n",
    "DATA_DIR = '/Users/sarora/dev/EAGER/data/orgs/parsed_page_output/'\n",
    "TRAINING_PERCENT = .10\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain (url):\n",
    "    o = urlparse(url.lower())\n",
    "    domain = o.netloc.strip('www.')\n",
    "    return domain\n",
    "\n",
    "# output urls for labeling of training data\n",
    "results = col.find({},{\"url\": 1, \"firm_name\": 1})\n",
    "df = pd.DataFrame(columns = ('firm_name', 'url', 'label'))\n",
    "domain_count = defaultdict(lambda:0,{})\n",
    "for i in range(results.count()):\n",
    "    result = results.next()\n",
    "    url = result['url'][0]\n",
    "    domain_count[get_domain(url)] += 1\n",
    "    firm_name = result['firm_name'][0] if 'firm_name' in result else ''\n",
    "    df.loc[i] = [firm_name, url, '']\n",
    "    \n",
    "df['gid'] = df.groupby(['firm_name']).ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gid.nunique()\n",
    "label_ids = random.sample(range(1, df.gid.nunique()), 200)\n",
    "df_label = df[df['gid'].isin(label_ids)]\n",
    "with open(ABOUT_DIR + 'about_pages_to_label.csv', mode='w') as to_label:\n",
    "    df_label.to_csv(to_label, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about_lbl', 'mgmt_lbl', 'partners_lbl', 'ip_lbl', 'about_agg_lbl', 'gid', 'pages_in_domain', 'is_sole_page']\n"
     ]
    }
   ],
   "source": [
    "# read back labeled data (note that about, management/team and partners, are dichotomous)\n",
    "df_about_labeled = pd.read_csv(ABOUT_DIR + 'about_pages_labeled-v2.csv')\n",
    "df_about_labeled = df_about_labeled.fillna(0)\n",
    "\n",
    "# count pages per domain\n",
    "for index, row in df_about_labeled.iterrows():\n",
    "    pages_in_domain = domain_count[get_domain(row['url'])]\n",
    "    df_about_labeled.loc[index,'pages_in_domain'] = pages_in_domain\n",
    "    is_sole_page = 0 if pages_in_domain > 1 else 1\n",
    "    df_about_labeled.loc[index,'is_sole_page'] = is_sole_page\n",
    "    \n",
    "labeled_urls = list(df_about_labeled['url']) # for training models on labeled urls below\n",
    "df_about_labeled = df_about_labeled.set_index(['firm_name', 'url'])\n",
    "print (df_about_labeled.columns.tolist())\n",
    "\n",
    "# final test set is the rows of the original data frame without the urls in df_about_labeled "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features to predict about pages\n",
    "Create features:\n",
    "1. title and url path fragment unigrams (also tried n-grams) with worse results) \n",
    "2. is home page and doesn't have any other pages\n",
    "3. other ideas here: https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load page data and create features\n",
    "\n",
    "# remove simple article words and punctuation (need to keep 'about')\n",
    "stop_words = ['the','a'] + list(string.punctuation) \n",
    "# remove known company names for model training and evaluation in the labeled data \n",
    "remove_regex = re.compile(r'^(3m|united|states|menu|en_us|algeternal|s\\d+|sarepta|skygen|nexgen|abbott|adlens|errorpage|\\d{1,3}|\\d{5,}|\\w+\\d+|\\d+\\w+|asten|johnson|baker|hughes|ge|bhge|biocon|egfr|gcsf|biocon|pegfilgrastim|bostik|canon|chevron|phillips|coloplast|cyberonics|microsoft|evoqua|ford|hitachi|glucanbio|hunter|douglas|kimberly|clark|lextar|fisher|lockheed|martin |lux|nec|nanocopoeia|cisco|schlumberger|weccamerica|inanobio|nanocomposix|zoetis|zygo)$', re.IGNORECASE)\n",
    "# used to filter top-level header content\n",
    "header_in = re.compile('(about|company|corporate|who.we.are|(^|/)vision|awards|profile|corporate|management|team|history|values|strategy|our |technology|research|commercialization)', flags=re.IGNORECASE)\n",
    "header_regex = re.compile(r'h[1-9]+')\n",
    "\n",
    "def clean_string(in_string):\n",
    "    if not in_string:\n",
    "        return in_string\n",
    "    split_words = in_string.lower().split()\n",
    "    result_words  = [word for word in split_words if word not in stop_words]\n",
    "    result_words  = [word for word in result_words if not remove_regex.search(word)]\n",
    "    result = ' '.join(result_words)\n",
    "    return ' ' + result\n",
    "\n",
    "def get_page_path_text (url):\n",
    "    o = urlparse(url.lower())\n",
    "    path = o.path\n",
    "    path_parts = path.split ('/')\n",
    "    path_parts = [part.split('.')[0] for part in path_parts] # remove page names\n",
    "    path_parts = [split for part in path_parts for split in part.split('-') ] # split on underscores, hyphens, et al\n",
    "    path_parts = [split for part in path_parts for split in part.split('_') ] # split on underscores, hyphens, et al\n",
    "    clnd_string = clean_string(' '.join(path_parts))\n",
    "    return clnd_string\n",
    "\n",
    "# recurse through the header text to add into feature grams\n",
    "def get_header_text (headers, names, index):\n",
    "    texts = [clean_string(header.text) for header in headers if header.name == names[index]]\n",
    "    texts = list(filter(header_in.search, texts))\n",
    "    if texts and len(texts[0].split()) > 4:\n",
    "        if(len(names) > (index + 1)):\n",
    "            return get_header_text (headers, names, index + 1)\n",
    "        else:\n",
    "            return ''\n",
    "    else: \n",
    "        return ' '.join (texts)\n",
    "    \n",
    "def process_firms (urls): \n",
    "    firm_page_features = {}\n",
    "    for url in urls: \n",
    "        result = col.find_one({\"url\": url})\n",
    "        domain = get_domain(url)\n",
    "        html = result['html'][0]\n",
    "        \n",
    "        # print (url)\n",
    "\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        running_text = ''\n",
    "        path_text = get_page_path_text(url)\n",
    "        \n",
    "        if path_text:\n",
    "            # print (path_text)\n",
    "            running_text += path_text\n",
    "            \n",
    "        if soup.title and soup.title.string:\n",
    "            # print (soup.title.string)\n",
    "            running_text += clean_string(soup.title.string)\n",
    "            \n",
    "        headers = soup.find_all(header_regex, text=True)\n",
    "        names = sorted(set ([header.name for header in headers]))\n",
    "        running_text += get_header_text (headers, names, 0)\n",
    "\n",
    "        firm_page_features[url] = running_text\n",
    "        \n",
    "    return firm_page_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing regex\n",
    "print (get_page_path_text('http://www.google.com/path-en/path_to/page.html'))\n",
    "print (re.split(\"\\W+|_\", \"Testing this_thing\"))\n",
    "print (clean_string('3m 01\t08\t100\t10m ford 235 1990 s129 188209 0913lk the ? about us'))\n",
    "pp.pprint (list(filter(header_in.search, ['about us', 'not found', 'company'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get firm website data for n-gram processing \n",
    "labeled_firm_page_features = process_firms (labeled_urls)\n",
    "\n",
    "urls = labeled_firm_page_features.keys()\n",
    "print (len(urls))\n",
    "corpus = []\n",
    "for url in urls:\n",
    "    corpus.append (labeled_firm_page_features[url])\n",
    "    \n",
    "# unigram\n",
    "ubv = TfidfVectorizer(min_df=0., max_df=1.)\n",
    "# you can set the n-gram range to 1,2 to get unigrams as well as bigrams (performs worse than just unigrams)\n",
    "# ubv = TfidfVectorizer(ngram_range=(1,2)) \n",
    "\n",
    "ubv_matrix = ubv.fit_transform(corpus)\n",
    "\n",
    "ubv_matrix = ubv_matrix.toarray()\n",
    "vocab = ubv.get_feature_names()\n",
    "ubv_df = pd.DataFrame(ubv_matrix, columns=vocab)\n",
    "ubv_df.index = urls\n",
    "ubv_df.index.name='url'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge two datasets (features and labeled data)\n",
    "print(ubv_df.shape)\n",
    "print(df_about_labeled.shape)\n",
    "\n",
    "all_merged = ubv_df.join(df_about_labeled, how='inner')\n",
    "print(all_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split labeled and predict datasets \n",
    "labeled = all_merged[all_merged['gid'].notnull()]\n",
    "print (df_about_labeled.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled train/test split\n",
    "X = labeled.iloc[:,1:len(ubv_df.columns)]\n",
    "X['pages_in_domain'] = labeled['pages_in_domain']\n",
    "X['is_sole_page'] = labeled['is_sole_page']\n",
    "X.to_csv(ABOUT_DIR + 'X.csv', index = True) # for manual inspection\n",
    "\n",
    "y = labeled.loc[:,'about_lbl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the model\n",
    "On just the labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a few models\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"SVC\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    SVC(gamma=0.001, C=100.), \n",
    "    QuadraticDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataframe for output metrics \n",
    "eval_df = pd.DataFrame (names,index=(range(len(names))), columns=[\"Name\"])\n",
    "eval_df['Accuracy'] = np.float64(0)\n",
    "display (eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# build evaluation outputs (currently limited to accuracy)\n",
    "i = np.int64(0)\n",
    "for name, clf in zip(names, classifiers):\n",
    "    display (name)\n",
    "    scores = cross_val_score(clf, X, y)\n",
    "    avg_score = np.mean(scores)\n",
    "    eval_df.set_value(i, 'Accuracy', avg_score)\n",
    "    i = i + 1\n",
    "    \n",
    "display(eval_df)\n",
    "eval_df.to_clipboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search using MLPClassifier to tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls = []\n",
    "hls.append([100,])\n",
    "hls.append([70,70,70])\n",
    "hls.append([40,40,40])\n",
    "hls.append([10,10,10])\n",
    "pp.pprint(hls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'solver': ['lbfgs'], 'max_iter': [300,500,700], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes': hls, 'random_state':[5,10,15]}\n",
    "clf_grid = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)\n",
    "clf_grid.fit(X,y)\n",
    "\n",
    "print(\"Best score: %0.4f\" % clf_grid.best_score_)\n",
    "print(\"Using the following parameters:\")\n",
    "print(clf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train neural net model with best hyperparameter configuration\n",
    "clf = MLPClassifier(alpha=0.01, hidden_layer_sizes=(70,70,70), max_iter=300, random_state=10, solver='lbfgs')\n",
    "clf.fit(X, y)\n",
    "\n",
    "y_hat = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print all instances where predictions don't match labels (for inspection)\n",
    "confusion_matrix(y, y_hat)\n",
    "\n",
    "for key, y_i, y_hat_i in zip(list(X.index), y, y_hat):\n",
    "    if y_i != y_hat_i:\n",
    "        print(key[1], 'has been classified as ', y_hat_i, 'but should be ', y_i) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict about pages for unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare domain level features \n",
    "df_predict = df[~df['url'].isin(labeled_urls)] # careful: index is firm_name and url now\n",
    "# count pages per domain\n",
    "for index, row in df_predict.iterrows():\n",
    "    pages_in_domain = domain_count[get_domain(row['url'])]\n",
    "    df_predict.loc[index,'pages_in_domain'] = pages_in_domain\n",
    "    is_sole_page = 0 if pages_in_domain > 1 else 1\n",
    "    df_predict.loc[index,'is_sole_page'] = is_sole_page\n",
    "    \n",
    "# set index \n",
    "df_predict = df_predict.set_index(['firm_name', 'url'])\n",
    "print (df_predict.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see whether there are duplicate urls\n",
    "# note: there should be because different assignees may map to the same domain (see error above)\n",
    "counter=collections.Counter(df_predict.index)\n",
    "most_common = counter.most_common(10)\n",
    "pp.pprint (most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare n-gram features\n",
    "unlabeled_firm_page_features = process_firms (set(df_predict.index.get_level_values('url')))\n",
    "\n",
    "prediction_urls = unlabeled_firm_page_features.keys()\n",
    "\n",
    "pred_corpus = []\n",
    "for url in prediction_urls:\n",
    "    pred_corpus.append (unlabeled_firm_page_features[url])\n",
    "\n",
    "ubv_prediction_matrix = ubv.transform(pred_corpus)\n",
    "\n",
    "ubv_prediction_matrix = ubv_prediction_matrix.toarray()\n",
    "vocab = ubv.get_feature_names()\n",
    "ubv_prediction_df = pd.DataFrame(ubv_prediction_matrix, columns=vocab)\n",
    "ubv_prediction_df.index = prediction_urls\n",
    "ubv_prediction_df.index.name='url'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ubv_prediction_df.shape)\n",
    "print(df_predict.shape)\n",
    "\n",
    "predict_merged = ubv_prediction_df.join(df_predict, how='right', rsuffix='_lbl')\n",
    "print(predict_merged.shape)\n",
    "\n",
    "# merge\n",
    "X_test = predict_merged.iloc[:,1:len(ubv_prediction_df.columns)]\n",
    "X_test['pages_in_domain'] = predict_merged['pages_in_domain']\n",
    "X_test['is_sole_page'] = predict_merged['is_sole_page']\n",
    "print (X.shape)\n",
    "print (X_test.shape) # should be the same number of cols\n",
    "\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with newly constructed X\n",
    "y_predicted = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard firm cleaning regex\n",
    "def clean_firm_name (firm):\n",
    "    firm_clnd = re.sub('(\\.|,| corporation| incorporated| llc| inc| international| gmbh| ltd)', '', firm, flags=re.IGNORECASE).rstrip()\n",
    "    return firm_clnd\n",
    "\n",
    "# write to file\n",
    "with open(ABOUT_DIR + 'about_predicted_and_labels.csv', mode='w') as about_file:\n",
    "    about_writer = csv.writer(about_file, delimiter=',', quotechar='\"')\n",
    "    about_writer.writerow(['firm_name', 'url', 'is_about'])\n",
    "    # output predicted values to file\n",
    "    for fn, u, predicted_value in zip(X_test.index.get_level_values('firm_name'), X_test.index.get_level_values('url'), y_predicted):\n",
    "        # print (fn + ' with url ' + u + ' has predicted value ' + str(predicted_value))\n",
    "        about_writer.writerow([clean_firm_name(fn), u, predicted_value])\n",
    "    # and the labeled ones too...\n",
    "    for fn, u, labeled_value in zip(X.index.get_level_values('firm_name'), X.index.get_level_values('url'), y):\n",
    "        # print (fn + ' with url ' + u + ' has predicted value ' + str(labeled_value))\n",
    "        about_writer.writerow([clean_firm_name(fn), u, labeled_value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from mongodb\n",
    "* Now that we know which pages are about pages, extract from mongodb and output for topic modeling\n",
    "* For now, construct paragraphs from different pages by ordering urls by their length.  In the future, might want to contruct paragraphs in their 'natural' sequential order as they would appear on a home page or landing page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine both labeled and predicted frames\n",
    "print (X_test.shape)\n",
    "print(X.shape)\n",
    "\n",
    "combined = X_test.append(X)\n",
    "print (combined.shape)\n",
    "print (len(y_predicted))\n",
    "print (len(y))\n",
    "abouts = pd.DataFrame(index=combined.index)\n",
    "\n",
    "abouts['is_about'] = list(y_predicted) + list(y)\n",
    "abouts = abouts.reset_index()\n",
    "abouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather unique firm_names from mongodb\n",
    "firm_names = set(abouts['firm_name'])\n",
    "print (len(firm_names))\n",
    "pp = pprint.PrettyPrinter()\n",
    "pp.pprint(firm_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ordered_about_urls (firm_name):\n",
    "    urls = list (abouts.loc[(abouts['firm_name'] == firm_name) & (abouts['is_about'] == 1), 'url'])\n",
    "    urls.sort(key = len)\n",
    "    # print ('Original urls')\n",
    "    # pp.pprint(urls)\n",
    "\n",
    "    index = {}\n",
    "    for url in urls:\n",
    "        path_fragments = len(url.split('/'))\n",
    "        added = False\n",
    "        for i in range(1, path_fragments):\n",
    "            key_phrase = url.rsplit('/', maxsplit=i)[0]\n",
    "            if key_phrase in urls or (key_phrase + '/') in urls: \n",
    "                od = index.setdefault(key_phrase, OrderedDict())\n",
    "                od[url] = 1\n",
    "                added = True\n",
    "                continue\n",
    "        if not added:\n",
    "            od = index.setdefault(url, OrderedDict())\n",
    "            od[url] = 1\n",
    " \n",
    "    # pp.pprint (index)\n",
    "    \n",
    "    return_urls = [] \n",
    "    seen = set ()\n",
    "    for key in index.keys():\n",
    "        tree_urls = index[key]\n",
    "        for fu in tree_urls:\n",
    "            if fu not in seen:\n",
    "                return_urls.appju8j nend(fu)\n",
    "                seen.add(fu)\n",
    "    \n",
    "    # finally remove home page if it exists and if there are other pages to draw on\n",
    "    if not return_urls: \n",
    "        return None\n",
    "    else: \n",
    "        first_page = return_urls[0]\n",
    "        first_page_path = get_page_path_text (first_page)\n",
    "        if first_page_path == ' ' or first_page_path == '':\n",
    "            print (first_page_path + 'empty')\n",
    "            return_urls.pop(0)\n",
    "        return return_urls\n",
    "\n",
    "test_urls = get_ordered_about_urls ('Previvo Genetics')\n",
    "print ('Ordered urls')\n",
    "pp.pprint (test_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove html content\n",
    "def is_javascript (x):\n",
    "    match_string = r\"(CDATA|return\\s+true|return\\s+false|getelementbyid|function|\\w+\\(.*?\\);|\\w{2,}[\\\\.|:]+\\w{2,}|header|hover|'\\w+':\\s+'\\w+|\\\\|{|}|\\r|\\n|\\/\\/')\"\n",
    "    # capture CDATA; function declarations; function calls; word sequences separated by a period (e.g., denoting paths)\n",
    "    regex = re.findall(match_string, x) \n",
    "    # check to see if the regex finds some percentage of the words look like javascript patterns\n",
    "    if (len(regex) / float(len(x.split())) > .10):\n",
    "        return True \n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def clean_page_content (text_list):\n",
    "    # remove whatever we think is html\n",
    "    removed_html = filter(lambda x: not( bool(BeautifulSoup(x, \"html.parser\").find()) ), text_list)\n",
    "    # remove content that looks like javascript \n",
    "    removed_js = filter(lambda x: not (is_javascript(x)), removed_html)\n",
    "    # add other checks here as needed\n",
    "\n",
    "    return removed_js\n",
    "    \n",
    "\n",
    "# iterate through firm urls and return concatenated string\n",
    "def get_content (urls): \n",
    "    running_text = ''\n",
    "    for url in urls:\n",
    "        print ('\\tWorking on ' + url)\n",
    "        result = col.find_one( {\"url\": url} )\n",
    "        if result:\n",
    "            clnd_text = clean_page_content(result['full_text'])\n",
    "            clnd_text = '\\n'.join(clnd_text)\n",
    "            boilerpipe = None\n",
    "            \n",
    "            if 'body' in result:\n",
    "                extractor = Extractor(extractor='DefaultExtractor', html = result['body'][0])\n",
    "                lines = extractor.getText().replace(u'\\xa0', u' ').split('\\n')\n",
    "                filtered = filter(lambda x: not re.match(r'^\\s*$', x), lines)\n",
    "                boilerpipe = '\\n'.join(filtered)\n",
    "\n",
    "            # TODO fix to split().  Counting characters currently \n",
    "            if boilerpipe and (len(boilerpipe) > .5 * len(clnd_text)):\n",
    "                print ('\\t\\tUsing boilerplate')\n",
    "                running_text += boilerpipe\n",
    "            else:\n",
    "                print ('\\t\\tUsing clnd_text')\n",
    "                running_text += clnd_text\n",
    "        else:\n",
    "            print ('Cannot find url: ' + url)\n",
    "\n",
    "    return running_text\n",
    "\n",
    "def clean_text (firm_name, text):\n",
    "    strip_regex = re.compile(r\"(\" + \"\\s|\".join(firm_name.split()) + \"\\s)\", re.IGNORECASE)\n",
    "    clnd_text = strip_regex.sub ('', text)\n",
    "    \n",
    "    more_regex = re.compile(r\"([A-Z]\\.?){1,} \")\n",
    "    clnd_text = more_regex.sub ('', clnd_text)\n",
    "    \n",
    "    return ' '.join(clnd_text.split(' '))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex test \n",
    "regex = re.findall(r\"(CDATA|return\\s+true|return\\s+false|getelementbyid|function|\\w+\\(.*?\\);|\\w{2,}[\\\\.|:]+\\w{2,}|'\\w+':\\s+'\\w+|\\\\|{|}|\\r|\\n|\\/\\/')\", \n",
    "                   \"CDATA function contact-us getelementbyid javascript.function linker:autoLink www.littlekidsinc.com fxnCall(param.param); email@dextr.us 'type': 'image' return true return false rev7bynlh\\\\u00252bvcgrjg\\\\ {height}\") # last part is words sequences separated by punct\n",
    "print (regex)\n",
    "\n",
    "print (clean_firm_name('Ford Motor Co. Corporation'))\n",
    "print (clean_text (clean_firm_name('Ford Motor Co.'), 'Ford is a motor company.  It has been building vehicles for over a century. H.W.F_ Ford was a nice guy.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_site_text = get_content (test_urls)\n",
    "print (test_site_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run process_firm and write to file\n",
    "pp = pprint.PrettyPrinter()\n",
    "for firm_name in firm_names: \n",
    "    print (\"Working on \" + firm_name)\n",
    "    about_urls = get_ordered_about_urls(firm_name)\n",
    "    if not about_urls:\n",
    "        print (\"\\tCouldn't find any urls for firm!\")\n",
    "        continue\n",
    "        \n",
    "    about_text = get_content (about_urls)\n",
    "    \n",
    "    if about_text: \n",
    "        firm_clnd = clean_firm_name(firm_name) # standard cleaning code throughout project\n",
    "        about_clnd = clean_text (firm_name, about_text)\n",
    "        file = re.sub('\\/', '|', firm_clnd) + '.txt'\n",
    "        with io.open(DATA_DIR + file,'w',encoding='utf8') as f:\n",
    "            f.write (about_clnd)\n",
    "    else:\n",
    "        print (\"\\tCouldn't find any text for firm!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
