nohup: ignoring input
Checking url for King Electric Vehicles Inc.
	Trying http://https://king-electric.com/
	Trying http://www.https://king-electric.com/
	Trying https://https://king-electric.com/
	Trying https://www.https://king-electric.com/
Checking url for NanoGram Corporation
	Trying http://nanogram.com
Checking url for Pharmatrophix
	Trying http://pharmatrophix.com/
Checking url for Sinton Consulting
	Trying http://sintoninstruments.com/
Checking url for Sun Synchrony
	Trying http://sunsynchrony.com/
Checking url for True-Safe Technologies
	Trying http://true-safe.com/
	Trying http://www.true-safe.com/
	Trying https://true-safe.com/
	Trying https://www.true-safe.com/
Checking url for Wave Energy Conversion Corporation of America
	Trying http://weccamerica.com/
Checking url for Crystal Solar Incorporated
	Trying http://xtalsolar.com/
Checking url for Building Envelope Innovations
	Trying http://wet-flash.com/
	Trying http://www.wet-flash.com/
	Trying https://wet-flash.com/
	Trying https://www.wet-flash.com/
Checking url for ADMA Products
	Trying http://admaproducts.com/#!
Checking url for Cooper Technologies Company
	Trying http://coopertechnologies.net/
Checking url for Enginuity Worldwide
	Trying http://enginuityww.webstarts.com/
Checking url for LUCERA LABS
	Trying http://luceralabs.com/
Checking url for Morpho
	Trying http://morpho.com/en
	Trying http://www.morpho.com/en
	Trying https://morpho.com/en
	Trying https://www.morpho.com/en
Checking url for Suganit Systems
	Trying http://suganit.com/
Checking url for NOVA PANGAEA TECHNOLOGIES LIMITED
	Trying http://novapangaea.com/
Checking url for WCA Group LLC
	Trying http://wca-group.com/
Checking url for Narsys
	Trying http://http://www.narsystech.com/
	Trying http://www.http://www.narsystech.com/
	Trying https://http://www.narsystech.com/
	Trying https://www.http://www.narsystech.com/
Checking url for Pardev
	Trying http://pardev.com/
Checking url for Lumenco
	Trying http://en.lumenco.ca/
	Trying http://www.en.lumenco.ca/
	Trying https://en.lumenco.ca/
	Trying https://www.en.lumenco.ca/
Checking url for Teal Sales Incorporated
	Trying http://tsi-inc.net
	Trying http://www.tsi-inc.net
	Trying https://tsi-inc.net
	Trying https://www.tsi-inc.net
Checking url for Polysar Corporation
	Trying http://polystar.com/
	Trying http://www.polystar.com/
	Trying https://polystar.com/
	Trying https://www.polystar.com/
Checking url for The Iams Company
	Trying http://iams.com/
Checking url for Singular Bio
	Trying http://singularbio.com/
Checking url for TECHNOLOGY RESEARCH
	Trying http://trci.net/
	Trying http://www.trci.net/
	Trying https://trci.net/
	Trying https://www.trci.net/
Checking url for GIGA SOLAR FPC
	Trying http://gigasolarpv.com/
Checking url for Tessera Advanced Technologies
	Trying http://tessera.com/
Checking url for Aurora Alage
	Trying http://aurorabiofuels.com/
Checking url for INENTEC INC.
	Trying http://inentec.com/
Checking url for PNM
	Trying http://pnm.com/
Checking url for Starlight Energy Holdings LLC
	Trying http://starlightenergy.us/
Checking url for Tesla Nanocoatings
	Trying http://teslanano.com/
	Trying http://www.teslanano.com/
	Trying https://teslanano.com/
	Trying https://www.teslanano.com/
Checking url for ESCAPE THERAPEUTICS
	Trying http://escapetherapeutics.com/
Checking url for Synaptic Research
	Trying http://research.synaptic.co.uk/
Checking url for Cima NanoTech Israel Ltd.
	Trying http://cimananotech.com/
Checking url for Tanaka Kikinzoku Kogyo K.K.
	Trying http://gold.tanaka.co.jp/english/
Checking url for Pyrexar Medical Inc.
	Trying http://pyrexar.com/
Checking url for Sirnaomics
	Trying http://new.sirnaomics.com/
Checking url for Bruin Biometrics
	Trying http://bruinbiometrics.com/us/
Checking url for LATITUDE PHARMACEUTICALS INC.
	Trying http://latitudepharma.com/
Checking url for Swagelok Company
	Trying http://swagelok.com/
Checking url for PELLION TECHNOLOGIES
	Trying http://pelliontech.com/
Checking url for Traex Corporation
	Trying http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm
Checking url for Wentworth Laboratories
	Trying http://wentworthlabs.com/
Checking url for Dynamic Solutions Worldwide
	Trying http://dynatrap.com/
Checking url for POLAR LIGHT TECHNOLOGIES AB
	Trying http://polar-light-technologies.com/
Checking url for Yeda Research and Development Co.
	Trying http://yedarnd.com/Overview
	Trying http://www.yedarnd.com/Overview
	Trying https://yedarnd.com/Overview
	Trying https://www.yedarnd.com/Overview
Checking url for Diamond Innovations
	Trying http://hyperionmt.com/
Checking url for Xyleco
	Trying http://xyleco.com/
Checking url for BRIGHTLEAF TECHNOLOGIES INC.
	Trying http://brightleafpower.com/
Checking url for Bisco
	Trying http://biscoind.com/
Checking url for Angstron Materials
	Trying http://theglobalgraphenegroup.com/
Checking url for AROG PHARMACEUTICALS
	Trying http://arogpharmaceuticals.com/
Checking url for Avalanche Technology
	Trying http://avalanche-technology.com/
Checking url for QuantumScape Corporation
	Trying http://quantumscape.com/
Checking url for Biological Dynamics
	Trying http://biologicaldynamics.com/
Checking url for Schlumberger Technology Corporation
	Trying http://slb.com/
Checking url for Galemed Corporation
	Trying http://galemed.com/manager/eService/RC08-all_0225.pdf
Checking url for Opel Solar
	Trying http://opelinc.com/
Checking url for K.S. INTERNATIONAL CO.
	Trying http://ksandco.com/
Checking url for Locus Energy
	Trying http://locusenergy.com/
Checking url for Ceramatec
	Trying http://ceramtec.com/
Checking url for Sienna Labs
	Trying http://siennabio.com/
Checking url for SOLAZYME
	Trying http://solazyme.com/
Checking url for Chipmos Technologies Inc.
	Trying http://chipmos.com/index-en.aspx
	Trying http://www.chipmos.com/index-en.aspx
	Trying https://chipmos.com/index-en.aspx
	Trying https://www.chipmos.com/index-en.aspx
Checking url for Ethox Chemicals
	Trying http://blog.ethox.com/
Checking url for TheGreenCell
	Trying http://thegreencell.com/
Checking url for Shimadzu Corporation
	Trying http://shimadzu.com/
Checking url for PLEX LLC
	Trying http://plex.tv/
Checking url for Intellectual Keystone Technology LLC
	Trying http://ikt.us.com/
Checking url for Ostendo Technologies
	Trying http://ostendo.com/
Checking url for Kinestral Technologies
	Trying http://kinestral.com/
Checking url for Nestec S. A.
	Trying http://nestle.com/
Checking url for Cook Biotech Incorporated
	Trying http://cookbiotech.com/
	Trying http://www.cookbiotech.com/
	Trying https://cookbiotech.com/
	Trying https://www.cookbiotech.com/
Checking url for Astex Pharmaceuticals
	Trying http://astx.com/
Checking url for Kajima Corporation
	Trying http://kajimausa.com/
	Trying http://www.kajimausa.com/
	Trying https://kajimausa.com/
	Trying https://www.kajimausa.com/
Checking url for Sysmex Corporation
	Trying http://sysmex.com/Pages/default.aspx
	Trying http://www.sysmex.com/Pages/default.aspx
	Trying https://sysmex.com/Pages/default.aspx
	Trying https://www.sysmex.com/Pages/default.aspx
Checking url for GlassPoint Solar
	Trying http://glasspoint.com/
Checking url for Texas Research International
	Trying http://tri-intl.com/
Checking url for Ajinomoto Althea
	Trying http://ajibio-pharma.com/
	Trying http://www.ajibio-pharma.com/
	Trying https://ajibio-pharma.com/
	Trying https://www.ajibio-pharma.com/
Checking url for ASM IP Holdings LLC
	Trying http://asm.com/
Checking url for Yageo Corporation
	Trying http://yageo.com/portal/product/product.jsp
	Trying http://www.yageo.com/portal/product/product.jsp
Checking url for Applied Nanostructures
	Trying http://appnano.com/
Checking url for Inventus Holdings
	Trying http://inventusholdings.com/
Checking url for Harris Corporation
	Trying http://harris.com/
Checking url for Electronic Warfare Associates
	Trying http://ewa.com/
Checking url for mVerify Corporation
	Trying http://numverify.com/
Checking url for ACell
	Trying http://acell.com/
Checking url for Saint-Gobain Adfors Canada
	Trying http://adfors.com/us/
Checking url for CARBO Ceramics Inc.
	Trying http://carboceramics.com/about/about-carbo
Checking url for Zygo Corporation
	Trying http://zygo.com/
Checking url for KOLON INDUSTRIES
	Trying http://kolonindustries.com/eng/main.asp
Checking url for Relypsa
	Trying http://relypsa.com/
Checking url for Toray Plastics (America)
	Trying http://toraytpa.com/
Checking url for nLIGHT
	Trying http://nlight.net/
Checking url for FUJIFILM Dimatix
	Trying http://fujifilmusa.com/products/industrial_inkjet_printheads/index.html
[{'domain': 'nanogram.com',
  'firm_name': 'NanoGram Corporation',
  'url': 'http://nanogram.com/'},
 {'domain': 'pharmatrophix.com',
  'firm_name': 'Pharmatrophix',
  'url': 'http://pharmatrophix.com/'},
 {'domain': 'www.sintoninstruments.com',
  'firm_name': 'Sinton Consulting',
  'url': 'https://www.sintoninstruments.com/'},
 {'domain': 'sunsynchrony.com',
  'firm_name': 'Sun Synchrony',
  'url': 'http://sunsynchrony.com/'},
 {'domain': 'weccamerica.com',
  'firm_name': 'Wave Energy Conversion Corporation of America',
  'url': 'https://weccamerica.com/'},
 {'domain': 'xtalsolar.com',
  'firm_name': 'Crystal Solar Incorporated',
  'url': 'http://xtalsolar.com/'},
 {'domain': 'www.admaproducts.com',
  'firm_name': 'ADMA Products',
  'url': 'https://www.admaproducts.com/#!'},
 {'domain': 'www.coopertechnologies.net',
  'firm_name': 'Cooper Technologies Company',
  'url': 'https://www.coopertechnologies.net/'},
 {'domain': 'enginuityww.webstarts.com',
  'firm_name': 'Enginuity Worldwide',
  'url': 'http://enginuityww.webstarts.com/'},
 {'domain': 'www.luceralabs.com',
  'firm_name': 'LUCERA LABS',
  'url': 'https://www.luceralabs.com/'},
 {'domain': 'suganit.com',
  'firm_name': 'Suganit Systems',
  'url': 'http://suganit.com/'},
 {'domain': 'www.novapangaea.com',
  'firm_name': 'NOVA PANGAEA TECHNOLOGIES LIMITED',
  'url': 'https://www.novapangaea.com/'},
 {'domain': 'www.wca-group.com',
  'firm_name': 'WCA Group LLC',
  'url': 'https://www.wca-group.com/'},
 {'domain': 'pardev.com', 'firm_name': 'Pardev', 'url': 'http://pardev.com/'},
 {'domain': 'iams.com',
  'firm_name': 'The Iams Company',
  'url': 'http://iams.com/'},
 {'domain': 'www.singularbio.com',
  'firm_name': 'Singular Bio',
  'url': 'https://www.singularbio.com/'},
 {'domain': 'www.gigasolarpv.com',
  'firm_name': 'GIGA SOLAR FPC',
  'url': 'https://www.gigasolarpv.com/'},
 {'domain': 'www.tessera.com',
  'firm_name': 'Tessera Advanced Technologies',
  'url': 'https://www.tessera.com/'},
 {'domain': 'www.aurorabiofuels.com',
  'firm_name': 'Aurora Alage',
  'url': 'http://www.aurorabiofuels.com/'},
 {'domain': 'inentec.com',
  'firm_name': 'INENTEC INC.',
  'url': 'http://inentec.com/'},
 {'domain': 'WWW.PNM.COM', 'firm_name': 'PNM', 'url': 'https://WWW.PNM.COM/'},
 {'domain': 'www.starlightenergy.us',
  'firm_name': 'Starlight Energy Holdings LLC',
  'url': 'http://www.starlightenergy.us/'},
 {'domain': 'escapetherapeutics.com',
  'firm_name': 'ESCAPE THERAPEUTICS',
  'url': 'http://escapetherapeutics.com/'},
 {'domain': 'research.synaptic.co.uk',
  'firm_name': 'Synaptic Research',
  'url': 'https://research.synaptic.co.uk/SynapticResearch/login.asp?t=16685&page=/Default.asp?'},
 {'domain': 'cimananotech.com',
  'firm_name': 'Cima NanoTech Israel Ltd.',
  'url': 'http://cimananotech.com/en/closure/'},
 {'domain': 'gold.tanaka.co.jp',
  'firm_name': 'Tanaka Kikinzoku Kogyo K.K.',
  'url': 'https://gold.tanaka.co.jp/english/'},
 {'domain': 'pyrexar.com',
  'firm_name': 'Pyrexar Medical Inc.',
  'url': 'https://pyrexar.com/'},
 {'domain': 'new.sirnaomics.com',
  'firm_name': 'Sirnaomics',
  'url': 'http://new.sirnaomics.com/'},
 {'domain': 'bruinbiometrics.com',
  'firm_name': 'Bruin Biometrics',
  'url': 'http://bruinbiometrics.com/us/'},
 {'domain': 'latitudepharma.com',
  'firm_name': 'LATITUDE PHARMACEUTICALS INC.',
  'url': 'http://latitudepharma.com/'},
 {'domain': 'www.swagelok.com:443',
  'firm_name': 'Swagelok Company',
  'url': 'https://www.swagelok.com:443/en'},
 {'domain': 'www.pelliontech.com',
  'firm_name': 'PELLION TECHNOLOGIES',
  'url': 'http://www.pelliontech.com/'},
 {'domain': 'vollrath.com',
  'firm_name': 'Traex Corporation',
  'url': 'http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm'},
 {'domain': 'www.wentworthlabs.com',
  'firm_name': 'Wentworth Laboratories',
  'url': 'https://www.wentworthlabs.com/'},
 {'domain': 'dynatrap.com',
  'firm_name': 'Dynamic Solutions Worldwide',
  'url': 'https://dynatrap.com/'},
 {'domain': 'www.polar-light-technologies.com',
  'firm_name': 'POLAR LIGHT TECHNOLOGIES AB',
  'url': 'http://www.polar-light-technologies.com/'},
 {'domain': 'www.hyperionmt.com',
  'firm_name': 'Diamond Innovations',
  'url': 'https://www.hyperionmt.com/'},
 {'domain': 'www.xyleco.com',
  'firm_name': 'Xyleco',
  'url': 'https://www.xyleco.com/'},
 {'domain': 'www.brightleafpower.com',
  'firm_name': 'BRIGHTLEAF TECHNOLOGIES INC.',
  'url': 'http://www.brightleafpower.com/'},
 {'domain': 'www.biscoind.com',
  'firm_name': 'Bisco',
  'url': 'https://www.biscoind.com/'},
 {'domain': 'www.theglobalgraphenegroup.com',
  'firm_name': 'Angstron Materials',
  'url': 'https://www.theglobalgraphenegroup.com/'},
 {'domain': 'www.arogpharmaceuticals.com',
  'firm_name': 'AROG PHARMACEUTICALS',
  'url': 'https://www.arogpharmaceuticals.com/'},
 {'domain': 'www.avalanche-technology.com',
  'firm_name': 'Avalanche Technology',
  'url': 'http://www.avalanche-technology.com/'},
 {'domain': 'www.quantumscape.com',
  'firm_name': 'QuantumScape Corporation',
  'url': 'http://www.quantumscape.com/'},
 {'domain': 'biologicaldynamics.com',
  'firm_name': 'Biological Dynamics',
  'url': 'https://biologicaldynamics.com/'},
 {'domain': 'www.slb.com',
  'firm_name': 'Schlumberger Technology Corporation',
  'url': 'https://www.slb.com/'},
 {'domain': 'galemed.com',
  'firm_name': 'Galemed Corporation',
  'url': 'http://galemed.com/manager/eService/RC08-all_0225.pdf'},
 {'domain': 'opelinc.com',
  'firm_name': 'Opel Solar',
  'url': 'http://opelinc.com/'},
 {'domain': 'ksandco.com',
  'firm_name': 'K.S. INTERNATIONAL CO.',
  'url': 'http://ksandco.com/'},
 {'domain': 'locusenergy.com:443',
  'firm_name': 'Locus Energy',
  'url': 'https://locusenergy.com:443/'},
 {'domain': 'www.ceramtec.com',
  'firm_name': 'Ceramatec',
  'url': 'https://www.ceramtec.com/'},
 {'domain': 'siennabio.com',
  'firm_name': 'Sienna Labs',
  'url': 'https://siennabio.com/'},
 {'domain': 'solazyme.com',
  'firm_name': 'SOLAZYME',
  'url': 'http://solazyme.com/'},
 {'domain': 'blog.ethox.com',
  'firm_name': 'Ethox Chemicals',
  'url': 'http://blog.ethox.com/'},
 {'domain': 'thegreencell.com',
  'firm_name': 'TheGreenCell',
  'url': 'http://thegreencell.com/'},
 {'domain': 'www.shimadzu.com',
  'firm_name': 'Shimadzu Corporation',
  'url': 'https://www.shimadzu.com/'},
 {'domain': 'www.plex.tv',
  'firm_name': 'PLEX LLC',
  'url': 'https://www.plex.tv/'},
 {'domain': 'ikt.us.com',
  'firm_name': 'Intellectual Keystone Technology LLC',
  'url': 'http://ikt.us.com/'},
 {'domain': 'www.ostendo.com',
  'firm_name': 'Ostendo Technologies',
  'url': 'https://www.ostendo.com/'},
 {'domain': 'www.kinestral.com',
  'firm_name': 'Kinestral Technologies',
  'url': 'https://www.kinestral.com/'},
 {'domain': 'www.nestle.com',
  'firm_name': 'Nestec S. A.',
  'url': 'http://www.nestle.com/'},
 {'domain': 'astx.com',
  'firm_name': 'Astex Pharmaceuticals',
  'url': 'https://astx.com/'},
 {'domain': 'glasspoint.com',
  'firm_name': 'GlassPoint Solar',
  'url': 'https://glasspoint.com/'},
 {'domain': 'tri-intl.com',
  'firm_name': 'Texas Research International',
  'url': 'https://tri-intl.com/'},
 {'domain': 'www.asm.com',
  'firm_name': 'ASM IP Holdings LLC',
  'url': 'http://www.asm.com/'},
 {'domain': 'www.yageo.com',
  'firm_name': 'Yageo Corporation',
  'url': 'http://www.yageo.com/portal/product/product.jsp'},
 {'domain': 'www.appnano.com',
  'firm_name': 'Applied Nanostructures',
  'url': 'http://www.appnano.com/'},
 {'domain': 'inventusholdings.com',
  'firm_name': 'Inventus Holdings',
  'url': 'http://inventusholdings.com/'},
 {'domain': 'www.harris.com',
  'firm_name': 'Harris Corporation',
  'url': 'https://www.harris.com//'},
 {'domain': 'ewa.com',
  'firm_name': 'Electronic Warfare Associates',
  'url': 'http://ewa.com/'},
 {'domain': 'numverify.com',
  2018-11-12 04:43:12 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: FirmDB)
2018-11-12 04:43:12 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.6 (default, Sep 12 2018, 18:26:19) - [GCC 8.0.1 20180414 (experimental) [trunk revision 259383]], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-1023-aws-x86_64-with-Ubuntu-18.04-bionic
2018-11-12 04:43:12 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'FirmDB', 'CONCURRENT_REQUESTS_PER_DOMAIN': 4, 'DEPTH_LIMIT': '1', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'FirmDB.spiders', 'ROBOTSTXT_OBEY': 'False', 'SPIDER_MODULES': ['FirmDB.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2049.0 Safari/537.36'}
2018-11-12 04:43:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-11-12 04:43:12 [py.warnings] WARNING: /home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2018-11-12 04:43:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'random_useragent.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-11-12 04:43:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-11-12 04:43:12 [scrapy.middleware] INFO: Enabled item pipelines:
['FirmDB.pipelines.FirmDBPipeline']
2018-11-12 04:43:12 [scrapy.core.engine] INFO: Spider opened
2018-11-12 04:43:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 04:48:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.admaproducts.com/> (referer: https://www.admaproducts.com/?_escaped_fragment_=)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 58, in parse_links
    browser.get(response.url)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: Timeout loading page after 300000ms

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 04:49:09 [scrapy.extensions.logstats] INFO: Crawled 37 pages (at 37 pages/min), scraped 11 items (at 11 items/min)
/bin/sh: 1: kill: No such process

2018-11-12 04:49:13 [scrapy.extensions.logstats] INFO: Crawled 37 pages (at 0 pages/min), scraped 12 items (at 1 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 04:50:22 [scrapy.extensions.logstats] INFO: Crawled 55 pages (at 18 pages/min), scraped 24 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 04:51:14 [scrapy.extensions.logstats] INFO: Crawled 60 pages (at 5 pages/min), scraped 33 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 04:51:28 [scrapy.core.scraper] ERROR: Spider error processing <GET http://sunsynchrony.com/about/index.html> (referer: http://sunsynchrony.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 58, in parse_links
    browser.get(response.url)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: Reached error page: about:neterror?e=dnsNotFound&u=http%3A//sunsynchrony.com/about/index.html&c=UTF-8&f=regular&d=We%20can%E2%80%99t%20connect%20to%20the%20server%20at%20sunsynchrony.com.

2018-11-12 04:51:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <406 http://sunsynchrony.com/index.html>: HTTP status code is not handled or not allowed
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 04:52:23 [scrapy.extensions.logstats] INFO: Crawled 72 pages (at 12 pages/min), scraped 41 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 04:53:18 [scrapy.extensions.logstats] INFO: Crawled 78 pages (at 6 pages/min), scraped 49 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

'firm_name': 'mVerify Corporation',
  'url': 'https://numverify.com/'},
 {'domain': 'acell.com', 'firm_name': 'ACell', 'url': 'https://acell.com/'},
 {'domain': 'www.adfors.com',
  'firm_name': 'Saint-Gobain Adfors Canada',
  'url': 'http://www.adfors.com/us/'},
 {'domain': 'carboceramics.com',
  'firm_name': 'CARBO Ceramics Inc.',
  'url': 'https://carboceramics.com/about/about-carbo'},
 {'domain': 'www.zygo.com',
  'firm_name': 'Zygo Corporation',
  'url': 'https://www.zygo.com/'},
 {'domain': 'kolonindustries.com',
  'firm_name': 'KOLON INDUSTRIES',
  'url': 'http://kolonindustries.com/eng/main.asp'},
 {'domain': 'www.relypsa.com',
  'firm_name': 'Relypsa',
  'url': 'https://www.relypsa.com/'},
 {'domain': 'toraytpa.com',
  'firm_name': 'Toray Plastics (America)',
  'url': 'http://toraytpa.com/'},
 {'domain': 'nlight.net', 'firm_name': 'nLIGHT', 'url': 'http://nlight.net/'},
 {'domain': 'www.fujifilmusa.com',
  'firm_name': 'FUJIFILM Dimatix',
  'url': 'https://www.fujifilmusa.com/products/industrial_inkjet_printheads/index.html'}]
Missing Teal Sales Incorporated, Tesla Nanocoatings, Chipmos Technologies Inc., King Electric Vehicles Inc., Building Envelope Innovations, True-Safe Technologies, Cook Biotech Incorporated, Kajima Corporation, Lumenco, Yeda Research and Development Co., Morpho, Ajinomoto Althea, Polysar Corporation, TECHNOLOGY RESEARCH, Narsys, Sysmex Corporation in the fixed urls list
Pages to scrape: ['http://nanogram.com/', 'http://pharmatrophix.com/', 'https://www.sintoninstruments.com/', 'http://sunsynchrony.com/', 'https://weccamerica.com/', 'http://xtalsolar.com/', 'https://www.admaproducts.com/#!', 'https://www.coopertechnologies.net/', 'http://enginuityww.webstarts.com/', 'https://www.luceralabs.com/', 'http://suganit.com/', 'https://www.novapangaea.com/', 'https://www.wca-group.com/', 'http://pardev.com/', 'http://iams.com/', 'https://www.singularbio.com/', 'https://www.gigasolarpv.com/', 'https://www.tessera.com/', 'http://www.aurorabiofuels.com/', 'http://inentec.com/', 'https://WWW.PNM.COM/', 'http://www.starlightenergy.us/', 'http://escapetherapeutics.com/', 'https://research.synaptic.co.uk/SynapticResearch/login.asp?t=16685&page=/Default.asp?', 'http://cimananotech.com/en/closure/', 'https://gold.tanaka.co.jp/english/', 'https://pyrexar.com/', 'http://new.sirnaomics.com/', 'http://bruinbiometrics.com/us/', 'http://latitudepharma.com/', 'https://www.swagelok.com:443/en', 'http://www.pelliontech.com/', 'http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm', 'https://www.wentworthlabs.com/', 'https://dynatrap.com/', 'http://www.polar-light-technologies.com/', 'https://www.hyperionmt.com/', 'https://www.xyleco.com/', 'http://www.brightleafpower.com/', 'https://www.biscoind.com/', 'https://www.theglobalgraphenegroup.com/', 'https://www.arogpharmaceuticals.com/', 'http://www.avalanche-technology.com/', 'http://www.quantumscape.com/', 'https://biologicaldynamics.com/', 'https://www.slb.com/', 'http://galemed.com/manager/eService/RC08-all_0225.pdf', 'http://opelinc.com/', 'http://ksandco.com/', 'https://locusenergy.com:443/', 'https://www.ceramtec.com/', 'https://siennabio.com/', 'http://solazyme.com/', 'http://blog.ethox.com/', 'http://thegreencell.com/', 'https://www.shimadzu.com/', 'https://www.plex.tv/', 'http://ikt.us.com/', 'https://www.ostendo.com/', 'https://www.kinestral.com/', 'http://www.nestle.com/', 'https://astx.com/', 'https://glasspoint.com/', 'https://tri-intl.com/', 'http://www.asm.com/', 'http://www.yageo.com/portal/product/product.jsp', 'http://www.appnano.com/', 'http://inventusholdings.com/', 'https://www.harris.com//', 'http://ewa.com/', 'https://numverify.com/', 'https://acell.com/', 'http://www.adfors.com/us/', 'https://carboceramics.com/about/about-carbo', 'https://www.zygo.com/', 'http://kolonindustries.com/eng/main.asp', 'https://www.relypsa.com/', 'http://toraytpa.com/', 'http://nlight.net/', 'https://www.fujifilmusa.com/products/industrial_inkjet_printheads/index.html']
Crawling page: https://www.singularbio.com/our-technology       valid
Crawling page: http://xtalsolar.com/       valid
Crawling page: http://xtalsolar.com/contact.html       valid
Crawling page: https://www.luceralabs.com/team/       valid
Crawling page: http://xtalsolar.com/careers.html       valid
Crawling page: http://xtalsolar.com/news.html       valid
Crawling page: https://www.sintoninstruments.com/category/products/       valid
Crawling page: https://www.coopertechnologies.net/%20privacy-policy/       valid
Crawling page: https://www.admaproducts.com/       Crawling page: https://www.tessera.com       valid
Crawling page: https://www.luceralabs.com/       valid
Crawling page: https://www.sintoninstruments.com/fmt-500-latest-module-test-technology/       valid
Crawling page: https://www.sintoninstruments.com/category/events/       valid
Crawling page: https://research.synaptic.co.uk/SynapticResearch/CookiePolicy.html       valid
Crawling page: http://www.starlightenergy.us/contact-us-1/       valid
Crawling page: http://www.starlightenergy.us/       valid
Crawling page: https://www.wca-group.com/sitemap/       valid
Crawling page: http://www.starlightenergy.us/sitemap/       valid
Crawling page: https://www.sintoninstruments.com/contact-sinton-instruments/       valid
Crawling page: https://www.wca-group.com/contacts/       valid
Crawling page: https://www.wca-group.com/mining/bravo-mine/       valid
Crawling page: https://www.wca-group.com/investor/       valid
Crawling page: https://www.wca-group.com/mining/alpha-mine/       valid
Crawling page: https://weccamerica.com       valid
Crawling page: https://weccamerica.com/about/       valid
Crawling page: https://www.wca-group.com/mining/       valid
Crawling page: https://www.wca-group.com/products/metal-scarp/       valid
Crawling page: https://www.wca-group.com/products/base-metal/       valid
Crawling page: https://www.wca-group.com/products/       valid
Crawling page: https://www.sintoninstruments.com/about-sinton-instruments/       valid
Crawling page: https://www.wca-group.com/about-us/       valid
Crawling page: https://www.sintoninstruments.com/products/spare-parts/       valid
Crawling page: https://www.sintoninstruments.com/products/fct-750/       valid
Crawling page: https://www.sintoninstruments.com/products/il800/       valid
Crawling page: https://www.sintoninstruments.com/products/bct400bls-i/       valid
Crawling page: https://www.sintoninstruments.com/products/       valid
Crawling page: http://sunsynchrony.com/about/index.html       Crawling page: https://www.wca-group.com/       valid
Crawling page: https://www.coopertechnologies.net/get-support/       valid
Crawling page: https://www.coopertechnologies.net/emergency/       valid
Crawling page: https://www.coopertechnologies.net/contact-us/       valid
Crawling page: https://www.coopertechnologies.net/it-support-for-government-agencies/       valid
Crawling page: https://www.coopertechnologies.net/category/blog/       valid
Crawling page: https://www.coopertechnologies.net/right-it-partner/       valid
Crawling page: https://www.coopertechnologies.net/voip-services/       valid
Crawling page: https://www.coopertechnologies.net/agricultural-companies/       valid
Crawling page: https://www.coopertechnologies.net/it-services-for-education/       valid
Crawling page: https://www.coopertechnologies.net/cyber-security/       valid
Crawling page: http://xtalsolar.com/company.html       valid
Crawling page: http://xtalsolar.com/technology.html       valid
Crawling page: https://www.sintoninstruments.com/products/wct-120/       valid
Crawling page: https://www.coopertechnologies.net/get-started/       valid
Crawling page: https://www.coopertechnologies.net/careers/       valid
Crawling page: https://www.coopertechnologies.net/managed-it-services/       valid
Crawling page: https://www.coopertechnologies.net/our-team/       valid
Crawling page: https://www.coopertechnologies.net/cloud-services/       valid
Crawling page: https://www.coopertechnologies.net       valid
Crawling page: https://www.sintoninstruments.com/products/wct-120ts/       valid
Crawling page: https://www.sintoninstruments.com/products/fct-450//bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 04:54:16 [scrapy.extensions.logstats] INFO: Crawled 91 pages (at 13 pages/min), scraped 58 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 04:54:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.coopertechnologies.net/it-support-for-local-businesse/> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 04:55:03 [scrapy.core.scraper] ERROR: Error downloading <GET http://coopertech.wpengine.com/expert-it-services/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 04:55:16 [scrapy.extensions.logstats] INFO: Crawled 97 pages (at 6 pages/min), scraped 67 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 04:55:24 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.weccamerica.com/media>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:55:24 [scrapy.core.scraper] ERROR: Error downloading <GET https://weccamerica.com/technology/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:55:24 [scrapy.core.scraper] ERROR: Error downloading <GET https://weccamerica.com/technology#1>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:55:24 [scrapy.core.scraper] ERROR: Error downloading <GET https://weccamerica.com/potential/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:55:24 [scrapy.core.scraper] ERROR: Error downloading <GET https://weccamerica.com/tel:3013650250>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
/bin/sh: 1: kill: No such process

2018-11-12 04:55:29 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.sintoninstruments.com/see-eupvsec-2018/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 04:55:29 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.sintoninstruments.com/fct-750-cell-tester/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
/bin/sh: 1: kill: No such process

2018-11-12 04:55:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.coopertechnologies.net/IT-management-services/> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 04:55:43 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.gigasolarpv.com/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:55:43 [scrapy.core.scraper] ERROR: Error downloading <GET http://sunsynchrony.com/products/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 04:55:43 [scrapy.core.scraper] ERROR: Error downloading <GET http://sunsynchrony.com/technology/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 04:55:43 [scrapy.core.scraper] ERROR: Error downloading <GET http://sunsynchrony.com/contact/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 04:55:43 [scrapy.core.scraper] ERROR: Error downloading <GET http://sunsynchrony.com/press/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
/bin/sh: 1: kill: No such process

2018-11-12 04:55:47 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.sintoninstruments.com/products/fmt-500/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:55:47 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.sintoninstruments.com/comments-cost-impact-100-renewable-energy-portfolio-standard-state-colorado/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:55:47 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.sintoninstruments.com/products/wct-120pl/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:55:47 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.coopertechnologies.net/technology-support/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:55:47 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.coopertechnologies.net/computer-support/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:55:47 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.novapangaea.com/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 04:56:12 [scrapy.extensions.logstats] INFO: Crawled 129 pages (at 32 pages/min), scraped 76 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 04:56:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/store?register=1> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 8610-8611: character maps to <undefined>
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 04:56:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Foodservice> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 04:57:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/customers/dealers/terms-conditions-sale> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 04:57:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Privacy-Policy.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 04:57:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Parts-Support/Service-Agencies.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 04:57:40 [scrapy.extensions.logstats] INFO: Crawled 143 pages (at 14 pages/min), scraped 86 items (at 10 items/min)
2018-11-12 04:57:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Terms-of-Use.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

2018-11-12 04:58:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Contact-Us/Vollrath-Email-Signup> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 04:58:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Contact-Us/Website-Feedback> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 04:58:12 [scrapy.extensions.logstats] INFO: Crawled 143 pages (at 0 pages/min), scraped 87 items (at 1 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 04:59:16 [scrapy.extensions.logstats] INFO: Crawled 154 pages (at 11 pages/min), scraped 95 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:00:12 [scrapy.extensions.logstats] INFO: Crawled 163 pages (at 9 pages/min), scraped 104 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:01:12 [scrapy.extensions.logstats] INFO: Crawled 180 pages (at 17 pages/min), scraped 116 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:02:23 [scrapy.extensions.logstats] INFO: Crawled 192 pages (at 12 pages/min), scraped 131 items (at 15 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:03:19 [scrapy.extensions.logstats] INFO: Crawled 202 pages (at 10 pages/min), scraped 143 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

       valid
Crawling page: https://www.coopertechnologies.net/testimonials/j-westerberg-core-strategies-physical-therapy/       valid
Crawling page: https://www.sintoninstruments.com/faqs/       valid
Crawling page: https://www.coopertechnologies.net/testimonials/b-bournebowman-williams/       valid
Crawling page: https://www.coopertechnologies.net/it-support-for-local-businesse/       Crawling page: https://www.coopertechnologies.net/testimonials/r-larkinlarkin-precision-machining/       valid
Crawling page: https://www.sintoninstruments.com/publications/       valid
Crawling page: https://www.coopertechnologies.net/sitemap.xml       valid
Crawling page: https://www.coopertechnologies.net/santa-cruz-it-support/       valid
Crawling page: https://www.sintoninstruments.com/request-a-quote/       valid
Crawling page: https://www.sintoninstruments.com/category/public-policy-analysis/       valid
Crawling page: https://research.synaptic.co.uk/SynapticResearch/login.asp?t=8030&page=/SynapticResearch/index.asp?       valid
Crawling page: https://www.sintoninstruments.com/author/adrienne/       valid
Crawling page: https://research.synaptic.co.uk/SynapticResearch/login.asp?t=16685&page=/Default.asp?       valid
Crawling page: https://www.sintoninstruments.com/?lang=zh-hans       valid
Crawling page: https://www.sintoninstruments.com/news/       valid
Crawling page: https://www.coopertechnologies.net/IT-management-services/       Crawling page: https://www.sintoninstruments.com/products/suns-voc/       valid
Crawling page: https://www.sintoninstruments.com/       valid
Crawling page: https://dynatrap.com/flylight-insect-trap-series/       valid
Crawling page: https://www.xyleco.com/privacy-policy/       valid
Crawling page: https://www.arogpharmaceuticals.com/gist       valid
Crawling page: http://latitudepharma.com/news-press-releases-on-latitude/       valid
Crawling page: https://www.xyleco.com/terms-and-conditions/       valid
Crawling page: http://latitudepharma.com/contact/       valid
Crawling page: http://latitudepharma.com/employment-opportunities/       valid
Crawling page: https://www.biscoind.com/store?register=1       Crawling page: https://www.xyleco.com/board/       valid
Crawling page: https://www.arogpharmaceuticals.com/aml       valid
Crawling page: https://www.slb.com/alumni.aspx       valid
Crawling page: https://vollrath.com/Foodservice       Crawling page: https://www.hyperionmt.com/en/offerings/?section=offeringSection3       valid
Crawling page: https://www.slb.com/resources/other_resources/product_sheets.aspx       valid
Crawling page: https://www.hyperionmt.com/en/offerings/?section=offeringSection2       valid
Crawling page: https://vollrath.com/customers/dealers/terms-conditions-sale       Crawling page: https://vollrath.com/Vollrath/Privacy-Policy.htm       Crawling page: https://vollrath.com/Vollrath/Parts-Support/Service-Agencies.htm       Crawling page: https://vollrath.com/Vollrath/Terms-of-Use.htm       Crawling page: https://www.hyperionmt.com/en/offerings/?section=offeringSection1       valid
Crawling page: https://vollrath.com/Contact-Us/Vollrath-Email-Signup       Crawling page: https://vollrath.com/Contact-Us/Website-Feedback       Crawling page: https://www.hyperionmt.com/en/offerings/engineered-solutions/can-tooling/       valid
Crawling page: https://www.hyperionmt.com/en/offerings/toolmaker-solutions/saw-tips/       valid
Crawling page: https://www.hyperionmt.com/en/offerings/toolmaker-solutions/Blanks-for-Oil-Gas/       valid
Crawling page: https://www.hyperionmt.com/en/offerings/toolmaker-solutions/pdc-cutters/       valid
Crawling page: https://www.slb.com/~/link.aspx?_id=AA149731ABC349F3B5F975B732F29DE3&_z=z       valid
Crawling page: https://www.slb.com/resources/help.aspx       valid
Crawling page: https://www.hyperionmt.com/en/offerings/toolmaker-solutions/pcbn/       valid
Crawling page: https://www.hyperionmt.com/en/recycling/       valid
Crawling page: https://www.slb.com/about/tos.aspx       valid
Crawling page: https://www.hyperionmt.com/en/contact-us/       valid
Crawling page: https://www.hyperionmt.com/en/news/       valid
Crawling page: https://www.slb.com/resources/VideoListingPage.aspx       valid
Crawling page: https://www.hyperionmt.com/en/solutions/       valid
Crawling page: https://www.slb.com/registrations/registration       valid
Crawling page: https://www.slb.com/resources/rss.aspx       valid
Crawling page: https://www.slb.com/contact_us/geographical.aspx       valid
Crawling page: https://www.slb.com/news/press_releases/2018/2018_1012_schlumberger_tgs_pr.aspx       valid
Crawling page: https://www.slb.com/news/press_releases/2018/2018_10_19_q3_earnings.aspx       valid
Crawling page: https://www.slb.com/news/press_releases/2018/2018_1025_hexacom_the_oilfield_marketplace_pr.aspx       valid
Crawling page: https://www.slb.com/resources/other_resources.aspx       valid
Crawling page: https://www.slb.com/resources/technical_papers.aspx       valid
Crawling page: https://www.slb.com/resources/supply.aspx       valid
Crawling page: https://www.slb.com/resources/publications/roc.aspx       valid
Crawling page: https://www.slb.com/resources/publications/books.aspx       valid
Crawling page: https://www.slb.com/resources/publications/mearr.aspx       valid
Crawling page: https://www.slb.com/resources/publications.aspx       valid
Crawling page: https://www.slb.com/resources/oilfield_review/or_en_intro_article.aspx       valid
Crawling page: https://www.slb.com/resources/oilfield_review/en.aspx       valid
Crawling page: https://www.slb.com/resources/other_resources/brochures.aspx       valid
Crawling page: https://www.slb.com/resources/oilfield_review.aspx       valid
Crawling page: https://www.slb.com/news/inside_news/2018/2018-10-12-faculty-for-the-future-forum.aspx       valid
Crawling page: https://www.slb.com/news/inside_news/2018/2018-10-17-world-oil-awards.aspx       valid
Crawling page: https://www.slb.com/news/inside_news/2018/2018-11-09-women-in-energy.aspx       valid
Crawling page: https://www.slb.com/resources/next.aspx       valid
Crawling page: https://www.slb.com/resources/publications/industry_articles.aspx       valid
Crawling page: https://www.slb.com/resources/software/welleye_borehole_data_viewer.aspx       valid
Crawling page: https://www.slb.com/resources/software/well_sync.aspx       valid
Crawling page: https://www.slb.com/resources/software/pdsview.aspx       valid
Crawling page: https://www.slb.com/resources/software/ihandbook.aspx       valid
Crawling page: https://www.slb.com/resources/software.aspx       valid
Crawling page: https://www.slb.com/resources/events.aspx       valid
Crawling page: https://www.slb.com/resources/email_subscriptions.aspx       valid
Crawling page: https://www.slb.com/resources/webinars/wg_seismic_webinars.aspx       valid
Crawling page: https://www.slb.com/resources/webinars/miswaco-webinars.aspx       valid
Crawling page: https://www.slb.com/resources/webinars.aspx       valid
Crawling page: https://www.slb.com/resources/videolistingpage.aspx       valid
Crawling page: https://www.slb.com/resources/tech_reports.aspx       valid
Crawling page: https://www.slb.com/resources/msds-sheets.aspx       valid
Crawling page: https://www.slb.com/alumni/network.aspx       valid
Crawling page: https://www.slb.com/resources/case_studies.aspx       valid
Crawling page: https://www.slb.com/resources.aspx       valid
Crawling page: https://www.slb.com/resources/mobile_applications.aspx       valid
Crawling page: https://www.slb.com/hse/hse_policy.aspx       valid
Crawling page: https://www.slb.com/alumni/benefits/medical_plan.aspx       valid
Crawling page: https://www.slb.com/alumni/benefits.aspx       valid
Crawling page: https://www.slb.com/alumni/benefits/human_resources.aspx       valid
Crawling page: https://www.slb.com/hse.aspx       valid
Crawling page: https://www.slb.com/hse/hse_mngt_sys.aspx       valid
Crawling page: https://www.slb.com/news/inside_news.aspx       valid
Crawling page: https://www.slb.com/news.aspx       valid
Crawling page: https://www.slb.com/news/presentations.aspx       valid
Crawling page: https://www.slb.com/news/press_releases.aspx/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:04:21 [scrapy.extensions.logstats] INFO: Crawled 216 pages (at 14 pages/min), scraped 155 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:05:20 [scrapy.extensions.logstats] INFO: Crawled 227 pages (at 11 pages/min), scraped 167 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:05:46 [scrapy.core.scraper] ERROR: Spider error processing <GET http://media.corporate-ir.net/media_files/IROL/97/97513/global_stewardship/index.html> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 05:06:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Contact-Us/Contact-Lookup> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: Failed to decode response from marionette

2018-11-12 05:06:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Customers/Dealers/Customer-Self-Service.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:06:15 [scrapy.extensions.logstats] INFO: Crawled 244 pages (at 17 pages/min), scraped 172 items (at 5 items/min)
2018-11-12 05:06:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Parts-Support/Product-Registration.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:06:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Parts-Support/FAQ-and-Reference.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
2018-11-12 05:07:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Parts-Support/Warranty-Information.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: Illegal number: 
2018-11-12 05:07:24 [root] ERROR: Unable to find match for url: https://www.software.slb.com/clp/upstream-webinar-series
/bin/sh: 1: kill: Illegal number: 
2018-11-12 05:07:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Parts-Support/TSR-and-CSR-Contact.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: Illegal number: 
2018-11-12 05:07:42 [scrapy.extensions.logstats] INFO: Crawled 252 pages (at 8 pages/min), scraped 180 items (at 8 items/min)
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
2018-11-12 05:08:38 [scrapy.extensions.logstats] INFO: Crawled 252 pages (at 0 pages/min), scraped 187 items (at 7 items/min)
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
2018-11-12 05:09:41 [scrapy.extensions.logstats] INFO: Crawled 267 pages (at 15 pages/min), scraped 195 items (at 8 items/min)
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
2018-11-12 05:10:20 [scrapy.extensions.logstats] INFO: Crawled 271 pages (at 4 pages/min), scraped 203 items (at 8 items/min)
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:11:25 [scrapy.extensions.logstats] INFO: Crawled 281 pages (at 10 pages/min), scraped 216 items (at 13 items/min)
/bin/sh: 1: kill: No such process

2018-11-12 05:11:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Parts-Support/Parts-Search.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:11:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Resource-Library.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:11:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/calprop65> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:12:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Resource-Library/Catalogs.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:12:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Resource-Library/Price-Lists.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:12:15 [scrapy.extensions.logstats] INFO: Crawled 292 pages (at 11 pages/min), scraped 217 items (at 1 items/min)
2018-11-12 05:12:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Resource-Library/New-Product-Brochure.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:12:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/vollrathUniversity/Product-Training.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:12:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/vollrathUniversity/Culinary-Support/Guides-Reference.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:12:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/VollrathUniversity/About.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:12:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/warewashingconfigurator> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:13:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Warewashing-Handling-and-Dispensers/How-To-Order-Traex-Imprinted-Racks.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:13:13 [scrapy.core.scraper] ERROR: Spider error processing <GET http://vollrath.com/inductiondrywell/> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:13:13 [scrapy.extensions.logstats] INFO: Crawled 296 pages (at 4 pages/min), scraped 217 items (at 0 items/min)
2018-11-12 05:13:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Warewashing-Handling-and-Dispensers/How-To-Order-Traex-Color-Racks.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:13:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Contact-Us/Locations> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:13:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://vollrath.com/induction/> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:13:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Contact-Us/Contact-Sales> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:13:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Fabricator-Components-Products.htm?FB_Values=F2_C146_A568!F2_C146_A517!&F2_ajaxEnabled=1&F2_DocID=186&F2_keywordFilter=&&&> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:14:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Products/Warewashing-Handling-and-Dispensers/Signature-Dishrack-Component-Parts-Guide.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:14:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Cookware-Bakeware-Products.htm?FB_Values=F2_C84_A5!&F2_ajaxEnabled=1&F2_DocID=189&F2_keywordFilter=&&&> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:14:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Warming-Equipment-Products.htm?FB_Values=F2_C129_A100!&F2_ajaxEnabled=1&F2_DocID=185&F2_keywordFilter=&&&> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:14:22 [scrapy.extensions.logstats] INFO: Crawled 307 pages (at 11 pages/min), scraped 217 items (at 0 items/min)
2018-11-12 05:14:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Induction-Products> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

       valid
Crawling page: https://www.slb.com/about/history/2010s.aspx       valid
Crawling page: https://www.slb.com/about/history/2000s.aspx       valid
Crawling page: https://www.slb.com/about/history/1990s.aspx       valid
Crawling page: https://www.slb.com/about/history/1980s.aspx       valid
Crawling page: https://www.slb.com/about/history/1970s.aspx       valid
Crawling page: https://www.slb.com/about/history/1960s.aspx       valid
Crawling page: https://www.slb.com/about/history/1940s.aspx       valid
Crawling page: https://www.slb.com/about/history/1930s.aspx       valid
Crawling page: https://www.slb.com/about/history/1950s.aspx       valid
Crawling page: https://www.slb.com/about/history/1920s.aspx       valid
Crawling page: https://www.slb.com/about/history.aspx       valid
Crawling page: https://www.slb.com/about/rd/technology.aspx       valid
Crawling page: https://www.slb.com/about/rd/research.aspx       valid
Crawling page: https://www.slb.com/about/rd.aspx       valid
Crawling page: https://www.slb.com/about/foundation/board.aspx       valid
Crawling page: https://www.slb.com/about/foundation/about_foundation.aspx       valid
Crawling page: https://www.slb.com/about/foundation.aspx       valid
Crawling page: https://www.slb.com/about/codeofconduct.aspx       valid
Crawling page: https://www.slb.com/about/accountability/political_contributions.aspx       valid
Crawling page: https://www.slb.com/about/accountability/conflict_minerals.aspx       valid
Crawling page: https://www.slb.com/about/accountability/human-rights.aspx       valid
Crawling page: https://www.slb.com/about/accountability.aspx       valid
Crawling page: https://www.slb.com/about/guiding_principles/diversity.aspx       valid
Crawling page: http://media.corporate-ir.net/media_files/IROL/97/97513/global_stewardship/index.html       Crawling page: https://vollrath.com/Contact-Us/Contact-Lookup       Crawling page: https://vollrath.com/Vollrath/Customers/Dealers/Customer-Self-Service.htm       Crawling page: https://vollrath.com/Vollrath/Parts-Support/Product-Registration.htm       Crawling page: https://vollrath.com/Vollrath/Parts-Support/FAQ-and-Reference.htm       Crawling page: https://www.theglobalgraphenegroup.com/at1500       valid
Crawling page: https://www.theglobalgraphenegroup.com/composites       valid
Crawling page: https://www.theglobalgraphenegroup.com/private-policy       valid
Crawling page: https://www.theglobalgraphenegroup.com/powders       valid
Crawling page: https://www.slb.com/about/guiding_principles/corpgovernance.aspx       valid
Crawling page: https://vollrath.com/Vollrath/Parts-Support/Warranty-Information.htm       Crawling page: https://www.software.slb.com/clp/upstream-webinar-series       valid
Crawling page: https://www.slb.com/about/guiding_principles.aspx       valid
Crawling page: https://vollrath.com/Vollrath/Parts-Support/TSR-and-CSR-Contact.htm       Crawling page: https://www.slb.com/about/execmanagement.aspx       valid
Crawling page: https://www.theglobalgraphenegroup.com/gca       valid
Crawling page: https://www.theglobalgraphenegroup.com/careers       valid
Crawling page: https://www.theglobalgraphenegroup.com/contact       valid
Crawling page: https://www.slb.com/about/who/backgrounder.aspx       valid
Crawling page: https://www.slb.com/about/who.aspx       valid
Crawling page: https://www.slb.com/about.aspx       valid
Crawling page: https://www.slb.com/services/links.aspx       valid
Crawling page: https://www.theglobalgraphenegroup.com/news       valid
Crawling page: https://www.slb.com/services/additional/adil-oil-and-gas-consulting.aspx       valid
Crawling page: https://www.slb.com/services/additional/3q-flow-measurement-technical-services.aspx       valid
Crawling page: https://www.slb.com/services/additional/training.aspx       valid
Crawling page: https://www.slb.com/services/additional/sensa-fiber-optic-monitoring.aspx       valid
Crawling page: https://www.slb.com/services/additional/water.aspx       valid
Crawling page: https://www.theglobalgraphenegroup.com/about       valid
Crawling page: https://www.theglobalgraphenegroup.com/patents       valid
Crawling page: https://www.slb.com/services/additional/integrated-oilfield-projects.aspx       valid
Crawling page: https://www.slb.com/services/additional/carbon.aspx       valid
Crawling page: https://www.slb.com/services/additional/spm.aspx       valid
Crawling page: https://www.slb.com/services/additional/geothermal.aspx       valid
Crawling page: https://www.slb.com/services/additional.aspx       valid
Crawling page: https://www.slb.com/services/technical_challenges/carbonates.aspx       valid
Crawling page: https://www.slb.com/services/technical_challenges/high_pressure_high_temperature.aspx       valid
Crawling page: https://www.slb.com/services/technical_challenges/geomechanics.aspx       valid
Crawling page: https://www.slb.com/services/technical_challenges/exploration.aspx       valid
Crawling page: https://www.slb.com/services/technical_challenges/enhanced_oil_recovery.aspx       valid
Crawling page: https://www.slb.com/services/technical_challenges/heavy_oil.aspx       valid
Crawling page: https://www.slb.com/services/technical_challenges/unconventional_resources.aspx       valid
Crawling page: https://www.slb.com/services/technical_challenges/geothermal.aspx       valid
Crawling page: https://www.slb.com/services/technical_challenges/deepwater_operations.aspx       valid
Crawling page: https://www.slb.com/services/technical_challenges/flow_assurance.aspx       valid
Crawling page: https://www.slb.com/services/technical_challenges.aspx       valid
Crawling page: https://www.slb.com/services/well_intervention/well_abandonment.aspx       valid
Crawling page: https://www.slb.com/services/well_intervention/sidetracking-services.aspx       valid
Crawling page: https://www.slb.com/services/well_intervention/fishing_services.aspx       valid
Crawling page: https://www.slb.com/services/well_intervention/subsea_landing_string.aspx       valid
Crawling page: https://www.slb.com/services/well_intervention/expandable-steel-patches.aspx       valid
Crawling page: https://www.slb.com/services/well_intervention/casing-reconnect.aspx       valid
Crawling page: https://vollrath.com/Vollrath/Parts-Support/Parts-Search.htm       Crawling page: https://vollrath.com/Vollrath/Resource-Library.htm       Crawling page: https://vollrath.com/calprop65       Crawling page: https://vollrath.com/Vollrath/Resource-Library/Catalogs.htm       Crawling page: https://vollrath.com/Vollrath/Resource-Library/Price-Lists.htm       Crawling page: https://vollrath.com/Vollrath/Resource-Library/New-Product-Brochure.htm       Crawling page: https://vollrath.com/vollrathUniversity/Product-Training.htm       Crawling page: https://vollrath.com/vollrathUniversity/Culinary-Support/Guides-Reference.htm       Crawling page: https://vollrath.com/VollrathUniversity/About.htm       Crawling page: https://vollrath.com/warewashingconfigurator       Crawling page: https://vollrath.com/Warewashing-Handling-and-Dispensers/How-To-Order-Traex-Imprinted-Racks.htm       Crawling page: http://vollrath.com/inductiondrywell/       Crawling page: https://vollrath.com/Warewashing-Handling-and-Dispensers/How-To-Order-Traex-Color-Racks.htm       Crawling page: https://vollrath.com/Contact-Us/Locations       Crawling page: http://vollrath.com/induction/       Crawling page: https://vollrath.com/Contact-Us/Contact-Sales       Crawling page: https://vollrath.com/Fabricator-Components-Products.htm?FB_Values=F2_C146_A568!F2_C146_A517!&F2_ajaxEnabled=1&F2_DocID=186&F2_keywordFilter=&&&       Crawling page: https://vollrath.com/Vollrath/Products/Warewashing-Handling-and-Dispensers/Signature-Dishrack-Component-Parts-Guide.htm       Crawling page: https://vollrath.com/Cookware-Bakeware-Products.htm?FB_Values=F2_C84_A5!&F2_ajaxEnabled=1&F2_DocID=189&F2_keywordFilter=&&&       Crawling page: https://vollrath.com/Warming-Equipment-Products.htm?FB_Values=F2_C129_A100!&F2_ajaxEnabled=1&F2_DocID=185&F2_keywordFilter=&&&       Crawling page: https://vollrath.com/Induction-Products       Crawling page: https://vollrath.com/Cooking-Equipment-Products.htm?FB_Values=F2_C146_A49!&F2_ajaxEnabled=1&F2_DocID=184&F2_keywordFilter=&&&2018-11-12 05:14:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Cooking-Equipment-Products.htm?FB_Values=F2_C146_A49!&F2_ajaxEnabled=1&F2_DocID=184&F2_keywordFilter=&&&> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:14:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Contact-Us> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:15:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Fabricator-Components/Fabricator-Components-Guides-and-Training.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:15:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Fabricator-Components-Products.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:15:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Serving-System-Products.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:15:20 [scrapy.extensions.logstats] INFO: Crawled 313 pages (at 6 pages/min), scraped 217 items (at 0 items/min)
2018-11-12 05:15:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Fabricator-Components> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

2018-11-12 05:15:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Mobile-Food-Serving-Equipment/Serving-Systems-Guides-and-Training.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:16:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Products/Serving-Systems--Components.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:16:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath-Serving-Systems> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:16:13 [scrapy.extensions.logstats] INFO: Crawled 313 pages (at 0 pages/min), scraped 218 items (at 1 items/min)
2018-11-12 05:16:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath-Frozen-Treat-Equipment-Products.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:17:26 [scrapy.extensions.logstats] INFO: Crawled 330 pages (at 17 pages/min), scraped 231 items (at 13 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:18:28 [scrapy.extensions.logstats] INFO: Crawled 342 pages (at 12 pages/min), scraped 244 items (at 13 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:18:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Warming-Equipment-Products.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:19:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Merchandising-Display-Solutions> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:19:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Frozen-Treat-Equipment> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:19:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Merchandising-Display-Solutions/Merchandising-Display-Solutions-Guides-and-Training.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:19:20 [scrapy.extensions.logstats] INFO: Crawled 351 pages (at 9 pages/min), scraped 247 items (at 3 items/min)
2018-11-12 05:19:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Warming-Equipment> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:19:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Cooking-Equipment-Products.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:19:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Warming-Equipment/Warming-Equipment-Guides-and-Training.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:19:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Cooking-Equipment/Cooking-Equipment-Guides-and-Training.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:20:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Cooking-Equipment> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:20:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Products/Food-Prep-Equipment/Food-Prep-Equipment-Guides-Training.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:20:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Food-Prep-Equipment-Products.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:20:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Food-Prep-Equipment> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:20:31 [scrapy.extensions.logstats] INFO: Crawled 355 pages (at 4 pages/min), scraped 247 items (at 0 items/min)
2018-11-12 05:20:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Products/Countertop-Equipment.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:20:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Warewashing-Handling/Warewashing-Handling-Guides-and-Training.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:20:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Warewashing-Handling-Products.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:21:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Buffet-and-Tabletop-Service/Buffet-and-Tabletop-Service-Guides-and-Training.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:21:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Dispensing-Solutions/Dispensing-Solutions-Guides-and-Training.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:21:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Dispensing-Solutions> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:21:23 [scrapy.extensions.logstats] INFO: Crawled 367 pages (at 12 pages/min), scraped 247 items (at 0 items/min)
2018-11-12 05:21:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Buffet-and-Tabletop-Services-Products.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:21:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Warewashing-Handling> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:21:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Kitchen-Essentials> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:21:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Kitchen-Essentials/Kitchen-Essentials-Guides-and-Training.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:22:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Kitchen-Essentials-Products.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:22:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Buffet-Tabletop-Service> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:22:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Cookware-Bakeware-Products.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:22:20 [scrapy.extensions.logstats] INFO: Crawled 372 pages (at 5 pages/min), scraped 247 items (at 0 items/min)
2018-11-12 05:22:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Cookware-Bakeware/Cookware-Bakeware-Guides-and-Training.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:22:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Cookware-Bakeware> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:22:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Steam-Table-Pans-Products.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:22:51 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wheretobuy.vollrath.com> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:23:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Steam-Table-Pans/Steam-Table-Pans-Guides-and-Training.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:23:11 [scrapy.core.scraper] ERROR: Spider error processing <GET http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:23:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Steam-Table-Pans> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:23:18 [scrapy.extensions.logstats] INFO: Crawled 386 pages (at 14 pages/min), scraped 247 items (at 0 items/min)
2018-11-12 05:23:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Press-Room.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:23:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Products/Smallwares> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:23:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Events-Calendar.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:23:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Suppliers> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:24:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Customers.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

2018-11-12 05:24:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Customers/Dealers.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

2018-11-12 05:24:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Customers/Chains.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Customers/Consultants.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:24:55 [scrapy.extensions.logstats] INFO: Crawled 392 pages (at 6 pages/min), scraped 251 items (at 4 items/min)
2018-11-12 05:25:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/OffNav/Wish-List/Your-Wish-List.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:25:28 [scrapy.extensions.logstats] INFO: Crawled 392 pages (at 0 pages/min), scraped 255 items (at 4 items/min)
2018-11-12 05:25:35 [scrapy.core.scraper] ERROR: Spider error processing <GET http://vollrathcompany.com/careers> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

2018-11-12 05:25:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://pyrexar.com/?Itemid=185>: HTTP status code is not handled or not allowed
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:26:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/privacy-policy> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:26:28 [scrapy.extensions.logstats] INFO: Crawled 405 pages (at 13 pages/min), scraped 262 items (at 7 items/min)
/bin/sh: 1: kill: No such process

       Crawling page: https://vollrath.com/Contact-Us       Crawling page: https://vollrath.com/Fabricator-Components/Fabricator-Components-Guides-and-Training.htm       Crawling page: https://vollrath.com/Fabricator-Components-Products.htm       Crawling page: https://vollrath.com/Serving-System-Products.htm       Crawling page: https://vollrath.com/Fabricator-Components       Crawling page: https://www.theglobalgraphenegroup.com       valid
Crawling page: https://vollrath.com/Mobile-Food-Serving-Equipment/Serving-Systems-Guides-and-Training.htm       Crawling page: https://vollrath.com/Vollrath/Products/Serving-Systems--Components.htm       Crawling page: https://vollrath.com/Vollrath-Serving-Systems       Crawling page: https://vollrath.com/Vollrath-Frozen-Treat-Equipment-Products.htm       Crawling page: https://www.slb.com/services/processing-separation.aspx       valid
Crawling page: https://www.slb.com/services/production/monitoring.aspx       valid
Crawling page: https://www.slb.com/services/production/corrosion_evaluation.aspx       valid
Crawling page: https://www.slb.com/services/production/production_optimization.aspx       valid
Crawling page: https://www.slb.com/services/production/production-technologies-chemicals-treatments.aspx       valid
Crawling page: https://www.slb.com/services/production/safety-systems.aspx       valid
Crawling page: https://www.slb.com/services/production/production_testing.aspx       valid
Crawling page: https://www.slb.com/services/production/dryaccess-deepwater-production-systems.aspx       valid
Crawling page: https://www.slb.com/services/production/cameron-life-of-field-services.aspx       valid
Crawling page: https://www.slb.com/services/production/production_logging.aspx       valid
Crawling page: https://www.slb.com/services/production/artificial_lift.aspx       valid
Crawling page: https://www.slb.com/services/production.aspx       valid
Crawling page: https://www.slb.com/services/subsea/subsea_lift_rd.aspx       valid
Crawling page: https://www.slb.com/services/subsea/subsea_landing_string.aspx       valid
Crawling page: https://www.slb.com/services/subsea.aspx       valid
Crawling page: https://www.slb.com/services/completions/subsea_landing_string.aspx       valid
Crawling page: https://www.slb.com/services/completions/stimulation.aspx       valid
Crawling page: https://www.slb.com/services/completions/accessories.aspx       valid
Crawling page: https://www.slb.com/services/completions/sand_control.aspx       valid
Crawling page: https://www.slb.com/services/completions/perforating.aspx       valid
Crawling page: https://www.slb.com/services/completions/completions.aspx       valid
Crawling page: https://www.slb.com/services/completions/safety_valves.aspx       valid
Crawling page: https://www.slb.com/services/completions/fiv.aspx       valid
Crawling page: https://www.slb.com/services/completions/liner_hangers.aspx       valid
Crawling page: https://www.slb.com/services/completions/packers.aspx       valid
Crawling page: https://www.slb.com/services/completions/intelligent.aspx       valid
Crawling page: https://www.slb.com/services/completions/wellwatcher.aspx       valid
Crawling page: https://www.slb.com/services/completions/multilaterals.aspx       valid
Crawling page: https://www.slb.com/services/completions/multistage_stimulation_systems.aspx       valid
Crawling page: https://vollrath.com/Warming-Equipment-Products.htm       Crawling page: https://vollrath.com/Merchandising-Display-Solutions       Crawling page: https://vollrath.com/Frozen-Treat-Equipment       Crawling page: https://vollrath.com/Merchandising-Display-Solutions/Merchandising-Display-Solutions-Guides-and-Training.htm       Crawling page: https://vollrath.com/Warming-Equipment       Crawling page: https://vollrath.com/Cooking-Equipment-Products.htm       Crawling page: https://vollrath.com/Warming-Equipment/Warming-Equipment-Guides-and-Training.htm       Crawling page: https://vollrath.com/Cooking-Equipment/Cooking-Equipment-Guides-and-Training.htm       Crawling page: https://vollrath.com/Cooking-Equipment       Crawling page: https://vollrath.com/Vollrath/Products/Food-Prep-Equipment/Food-Prep-Equipment-Guides-Training.htm       Crawling page: https://vollrath.com/Food-Prep-Equipment-Products.htm       Crawling page: https://vollrath.com/Food-Prep-Equipment       Crawling page: https://vollrath.com/Vollrath/Products/Countertop-Equipment.htm       Crawling page: https://vollrath.com/Warewashing-Handling/Warewashing-Handling-Guides-and-Training.htm       Crawling page: https://vollrath.com/Warewashing-Handling-Products.htm       Crawling page: https://vollrath.com/Buffet-and-Tabletop-Service/Buffet-and-Tabletop-Service-Guides-and-Training.htm       Crawling page: https://vollrath.com/Dispensing-Solutions/Dispensing-Solutions-Guides-and-Training.htm       Crawling page: https://vollrath.com/Dispensing-Solutions       Crawling page: https://vollrath.com/Buffet-and-Tabletop-Services-Products.htm       Crawling page: https://vollrath.com/Warewashing-Handling       Crawling page: https://vollrath.com/Kitchen-Essentials       Crawling page: https://vollrath.com/Kitchen-Essentials/Kitchen-Essentials-Guides-and-Training.htm       Crawling page: https://vollrath.com/Kitchen-Essentials-Products.htm       Crawling page: https://vollrath.com/Buffet-Tabletop-Service       Crawling page: https://vollrath.com/Cookware-Bakeware-Products.htm       Crawling page: https://vollrath.com/Cookware-Bakeware/Cookware-Bakeware-Guides-and-Training.htm       Crawling page: https://vollrath.com/Cookware-Bakeware       Crawling page: https://vollrath.com/Steam-Table-Pans-Products.htm       Crawling page: http://wheretobuy.vollrath.com       Crawling page: https://vollrath.com/Steam-Table-Pans/Steam-Table-Pans-Guides-and-Training.htm       Crawling page: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm       Crawling page: https://vollrath.com/Steam-Table-Pans       Crawling page: https://vollrath.com/Vollrath/Press-Room.htm       Crawling page: https://vollrath.com/Vollrath/Products/Smallwares       Crawling page: https://vollrath.com/Vollrath/Events-Calendar.htm       Crawling page: https://vollrath.com/Suppliers       Crawling page: https://vollrath.com/Vollrath/Customers.htm       Crawling page: http://www.avalanche-technology.com/2018/10/21/how-ai-and-ml-are-driving-memory-architecture-changes/       valid
Crawling page: https://vollrath.com/Vollrath/Customers/Dealers.htm       Crawling page: http://www.avalanche-technology.com/contact/evaluation-board-or-kit-request/       valid
Crawling page: https://vollrath.com/Vollrath/Customers/Chains.htm       Crawling page: https://vollrath.com/Vollrath/Customers/Consultants.htm       Crawling page: http://www.avalanche-technology.com/contact/sample-request/       valid
Crawling page: http://www.avalanche-technology.com/contact/product-inquiry/       valid
Crawling page: https://vollrath.com/Vollrath/OffNav/Wish-List/Your-Wish-List.htm       Crawling page: http://www.avalanche-technology.com/contact/general-inquiry/       valid
Crawling page: http://www.avalanche-technology.com/contact/       valid
Crawling page: http://www.avalanche-technology.com/company/careers/       valid
Crawling page: http://www.avalanche-technology.com/company/news-insights/       valid
Crawling page: http://vollrathcompany.com/careers       Crawling page: http://www.brightleafpower.com/2013/04/18/what-does-warren-buffet-see-in-solar-article/       valid
Crawling page: http://www.brightleafpower.com/2013/06/18/watch-newspaper-article/       valid
Crawling page: http://www.brightleafpower.com/2013/02/25/off-the-grid-mansions/       valid
Crawling page: http://www.avalanche-technology.com/applications/iot/       valid
Crawling page: http://www.avalanche-technology.com/applications/       valid
Crawling page: http://www.avalanche-technology.com/applications/consumer/       valid
Crawling page: http://www.avalanche-technology.com/applications/industrial/       valid
Crawling page: https://pyrexar.com/privacy-policy       Crawling page: http://www.avalanche-technology.com/technology/mram-technology/       valid
Crawling page:/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:27:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/new-study-supporting-focus-and-steering> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:27:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.pyrexar.com/clinical/clinical-trials> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:27:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/hyperthermia-proton-beam> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:27:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/2018-website-update> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:27:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/astro-2018-in-review> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:27:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.pyrexar.com/hyperthermia/bsd-2000-3d-mr> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:27:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/sarcoma-patients-living-longer-with-hyperthermia> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:27:42 [scrapy.extensions.logstats] INFO: Crawled 412 pages (at 7 pages/min), scraped 266 items (at 4 items/min)
2018-11-12 05:27:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/about-us> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:27:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/hyperthermia-blog> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:28:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/about-us/our-team> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:28:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/about-us/featured-event> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:28:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/knowledgebase/treatment-centers> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:28:20 [scrapy.extensions.logstats] INFO: Crawled 418 pages (at 6 pages/min), scraped 266 items (at 0 items/min)
2018-11-12 05:28:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/knowledgebase/organizations> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:28:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/knowledgebase/pyrexar-video> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:29:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/clinical/clinical-trials> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:29:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/knowledgebase/hyperthermia-faq> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:29:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/clinical/by-indication> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:29:21 [scrapy.extensions.logstats] INFO: Crawled 431 pages (at 13 pages/min), scraped 269 items (at 3 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:30:22 [scrapy.extensions.logstats] INFO: Crawled 448 pages (at 17 pages/min), scraped 278 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:31:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/hyperthermia/bsd-2000> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:31:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/hyperthermia/bsd-500> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:31:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/hyperthermia> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:32:11 [scrapy.extensions.logstats] INFO: Crawled 454 pages (at 6 pages/min), scraped 289 items (at 11 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:32:36 [scrapy.extensions.logstats] INFO: Crawled 454 pages (at 0 pages/min), scraped 293 items (at 4 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:33:17 [scrapy.extensions.logstats] INFO: Crawled 462 pages (at 8 pages/min), scraped 301 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:34:12 [scrapy.extensions.logstats] INFO: Crawled 476 pages (at 14 pages/min), scraped 313 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:35:15 [scrapy.extensions.logstats] INFO: Crawled 486 pages (at 10 pages/min), scraped 327 items (at 14 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:36:16 [scrapy.extensions.logstats] INFO: Crawled 509 pages (at 23 pages/min), scraped 340 items (at 13 items/min)
/bin/sh: 1: kill: No such process

 http://www.avalanche-technology.com/technology/embedded-mram-technology/       valid
Crawling page: http://www.avalanche-technology.com/technology/papers-conferences/       valid
Crawling page: http://www.avalanche-technology.com/technology/3d-cross-point-mram-technology/       valid
Crawling page: https://pyrexar.com/new-study-supporting-focus-and-steering       Crawling page: https://www.pyrexar.com/clinical/clinical-trials       Crawling page: https://pyrexar.com/hyperthermia-proton-beam       Crawling page: https://pyrexar.com/2018-website-update       Crawling page: https://pyrexar.com/astro-2018-in-review       Crawling page: https://www.pyrexar.com/hyperthermia/bsd-2000-3d-mr       Crawling page: https://pyrexar.com/sarcoma-patients-living-longer-with-hyperthermia       Crawling page: https://pyrexar.com/about-us       Crawling page: https://pyrexar.com/hyperthermia-blog       Crawling page: https://pyrexar.com/about-us/our-team       Crawling page: https://pyrexar.com/about-us/featured-event       Crawling page: https://pyrexar.com/knowledgebase/treatment-centers       Crawling page: https://pyrexar.com/knowledgebase/organizations       Crawling page: https://biologicaldynamics.com/company-events       valid
Crawling page: https://biologicaldynamics.com/news-all/2018/9/24/biological-dynamics-appoints-irwin-m-jacobs-as-chairman-of-the-board       valid
Crawling page: https://biologicaldynamics.com/terms-privacy       valid
Crawling page: https://pyrexar.com/knowledgebase/pyrexar-video       Crawling page: https://pyrexar.com/clinical/clinical-trials       Crawling page: https://pyrexar.com/knowledgebase/hyperthermia-faq       Crawling page: https://pyrexar.com/clinical/by-indication       Crawling page: https://biologicaldynamics.com/publications-all/2015/8/8/rapid-electrokinetic-isolation-of-cancer-related-circulating-cell-free-dna-directly-from-blood       valid
Crawling page: https://biologicaldynamics.com/news-all/?category=Press+Release       valid
Crawling page: https://biologicaldynamics.com/news-all/?category=News       valid
Crawling page: https://biologicaldynamics.com/news-all/2018bod       valid
Crawling page: https://biologicaldynamics.com/publications-all/2014/4/14/dielectrophoretic-isolation-and-detection-of-cancer-related-circulating-cell-free       valid
Crawling page: https://biologicaldynamics.com/publications-all/2018/2/27/detecting-cancer-biomarkers-in-blood-challenges-for-new-molecular-diagnostic-and-point-of-care-tests-using-cell-free-nucleic-acids       valid
Crawling page: https://biologicaldynamics.com/careers/       valid
Crawling page: https://biologicaldynamics.com/publications-all/2018/7/21/cancer-detection-at-your-fingertips-smartphone-enabled-dna-testing       valid
Crawling page: https://biologicaldynamics.com/corporate-board/       valid
Crawling page: https://biologicaldynamics.com/contact/       valid
Crawling page: https://biologicaldynamics.com/leadership/       valid
Crawling page: https://biologicaldynamics.com/sab/       valid
Crawling page: https://biologicaldynamics.com/oncology       valid
Crawling page: https://www.slb.com/services/completions/15k-psi-completion-systems.aspx       valid
Crawling page: https://www.slb.com/services/completions.aspx       valid
Crawling page: https://pyrexar.com/hyperthermia/bsd-2000       Crawling page: https://pyrexar.com/hyperthermia/bsd-500       Crawling page: https://pyrexar.com/hyperthermia       Crawling page: https://biologicaldynamics.com/news/       valid
Crawling page: https://biologicaldynamics.com/publications/       valid
Crawling page: https://biologicaldynamics.com/ace-platform/       valid
Crawling page: https://biologicaldynamics.com/pointofcare/       valid
Crawling page: https://www.slb.com/services/characterization/petrotechnical_reservoir_consulting.aspx       valid
Crawling page: https://www.slb.com/services/characterization/testing.aspx       valid
Crawling page: https://biologicaldynamics.com/research/       valid
Crawling page: https://biologicaldynamics.com/oncology/       valid
Crawling page: https://www.slb.com/services/characterization/reservoir.aspx       valid
Crawling page: https://www.slb.com/services/characterization/geomechanics.aspx       valid
Crawling page: https://www.slb.com/services/characterization/geology.aspx       valid
Crawling page: https://www.slb.com/services/drilling/cementing.aspx       valid
Crawling page: https://www.slb.com/services/characterization.aspx       valid
Crawling page: https://www.slb.com/services/drilling/tools_services.aspx       valid
Crawling page: https://www.slb.com/services/drilling/solids_control.aspx       valid
Crawling page: https://www.slb.com/services/drilling/equipment-sales-rental.aspx       valid
Crawling page: https://www.slb.com/services/drilling/drill_bits.aspx       valid
Crawling page: https://www.slb.com/services/drilling/drilling_services_systems.aspx       valid
Crawling page: https://www.slb.com/services/drilling/engineering_modeling.aspx       valid
Crawling page: https://www.slb.com/services/drilling.aspx       valid
Crawling page: https://www.slb.com/services/drilling/wellhead-systems.aspx       valid
Crawling page: https://www.slb.com/services/seismic/seismic-drilling-support.aspx       valid
Crawling page: https://www.slb.com/services/seismic/borehole_seis.aspx       valid
Crawling page: https://www.slb.com/services/seismic/geophysical-processing.aspx       valid
Crawling page: http://bruinbiometrics.com/us/log-in       valid
Crawling page: https://www.slb.com/services/seismic/seismic-reservoir-characterization.aspx       valid
Crawling page: https://www.slb.com/services.aspx       valid
Crawling page: http://bruinbiometrics.com/us/terms-of-use       valid
Crawling page: https://www.slb.com/services/seismic.aspx       valid
Crawling page: https://www.slb.com/registrations/registration.aspx       valid
Crawling page: https://www.slb.com/registrations/login.aspx       valid
Crawling page: http://bruinbiometrics.com/us/careers       valid
Crawling page: http://bruinbiometrics.com/us/component/k2/item/65-virgin-care-adopts-bbi-s-sem-scanner-as-new-standard-of-care-for-pressure-ulcer-prevention       valid
Crawling page: http://bruinbiometrics.com/us/component/k2/item/66-bbi-receives-frost-sullivan-award-for-new-product-innovation-for-pressure-ulcer-diagnostics       valid
Crawling page: http://bruinbiometrics.com/us/contact-us       valid
Crawling page: http://bruinbiometrics.com/us/       valid
Crawling page: http://bruinbiometrics.com/us/orthosonos       valid
Crawling page: http://bruinbiometrics.com/us/news       valid
Crawling page: http://bruinbiometrics.com/us/iisportal       valid
Crawling page: http://bruinbiometrics.com/us/sem/clinical-benefits       valid
Crawling page: http://bruinbiometrics.com/us/sem/implementation-2       valid
Crawling page: http://bruinbiometrics.com/us/sem/conference       valid
Crawling page: http://bruinbiometrics.com/us/sem/resources       valid
Crawling page: http://www.polar-light-technologies.com/about/       valid
Crawling page: http://www.polar-light-technologies.com/lesson/optical-interconnects/       valid
Crawling page: http://www.polar-light-technologies.com/lesson/micro-nano-led/       valid
Crawling page: http://www.polar-light-technologies.com/polarized-light-quantum-dot-led/       valid
Crawling page: http://bruinbiometrics.com/us/sem/sem-scanner-overview       valid
Crawling page: http://bruinbiometrics.com/us/sem/sem-2       valid
Crawling page: http://bruinbiometrics.com/us/sem/pressure-ulcers-dtis       valid
Crawling page: http://bruinbiometrics.com/us/sem/financial-benefits       valid
Crawling page: http://www.polar-light-technologies.com/lesson/quantum-cryptography/       valid
Crawling page: http://www.polar-light-technologies.com/team/       valid
Crawling page: https://www.wentworthlabs.com/contact-us/       valid
Crawling page: https://www.wentworthlabs.com/employment-opportunities/       valid
Crawling page: https://www.wentworthlabs.com/representatives/       valid
Crawling page: https://www.wentworthlabs.com/category/upcoming-events/       valid
Crawling page: http://bruinbiometrics.com/us/sem       /bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:37:23 [scrapy.extensions.logstats] INFO: Crawled 516 pages (at 7 pages/min), scraped 355 items (at 15 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:37:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/store?p=privacy-policy.html> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 8610-8611: character maps to <undefined>
2018-11-12 05:37:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/store?p=terms.html> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 8610-8611: character maps to <undefined>
/bin/sh: 1: kill: No such process

2018-11-12 05:37:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/store?o=BiscoCareer> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 8873-8874: character maps to <undefined>
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:38:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Unicorp> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10354-10355: character maps to <undefined>
2018-11-12 05:38:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/MicroPlastics> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10091-10092: character maps to <undefined>
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:38:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Nord-Lock> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10284-10285: character maps to <undefined>
2018-11-12 05:38:44 [scrapy.extensions.logstats] INFO: Crawled 544 pages (at 28 pages/min), scraped 365 items (at 10 items/min)
/bin/sh: 1: kill: No such process

2018-11-12 05:38:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/AlphaWire> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10137-10138: character maps to <undefined>
2018-11-12 05:39:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/CaptiveFastenerCorp> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10525-10526: character maps to <undefined>
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:39:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/zago-mfg> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10044-10045: character maps to <undefined>
2018-11-12 05:39:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Panduit-cableties-acces> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10342-10343: character maps to <undefined>
2018-11-12 05:39:31 [scrapy.extensions.logstats] INFO: Crawled 548 pages (at 4 pages/min), scraped 369 items (at 4 items/min)
2018-11-12 05:39:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/LynTronInc> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10039-10040: character maps to <undefined>
2018-11-12 05:39:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Optifuse> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10014-10015: character maps to <undefined>
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:40:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Conta-Clip> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10045-10046: character maps to <undefined>
2018-11-12 05:40:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/bisco-clinchfastener> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10200-10201: character maps to <undefined>
2018-11-12 05:40:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Birtcher-Pentair> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10234-10235: character maps to <undefined>
2018-11-12 05:40:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Bivar/1000001/LED+Indication> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 13227-13228: character maps to <undefined>
2018-11-12 05:40:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Accuride> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10056-10057: character maps to <undefined>
2018-11-12 05:40:31 [scrapy.extensions.logstats] INFO: Crawled 552 pages (at 4 pages/min), scraped 372 items (at 3 items/min)
2018-11-12 05:40:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/AdvancedInterconnections> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10378-10379: character maps to <undefined>
2018-11-12 05:40:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Hantronix> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10029-10030: character maps to <undefined>
2018-11-12 05:40:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Richco/5200004/Fiber--Wire---Cable-Management> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10373-10374: character maps to <undefined>
2018-11-12 05:40:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Tyton> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 9999-10000: character maps to <undefined>
2018-11-12 05:41:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/PSM-CaptiveHardware> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10174-10175: character maps to <undefined>
2018-11-12 05:41:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/3M> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10279-10280: character maps to <undefined>
2018-11-12 05:41:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/MolexConnectors> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10483-10484: character maps to <undefined>
2018-11-12 05:41:28 [scrapy.extensions.logstats] INFO: Crawled 562 pages (at 10 pages/min), scraped 372 items (at 0 items/min)
2018-11-12 05:41:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/KATO> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 9954-9955: character maps to <undefined>
2018-11-12 05:41:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Schroff-Pentair> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10112-10113: character maps to <undefined>
2018-11-12 05:41:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Richco> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10017-10018: character maps to <undefined>
2018-11-12 05:41:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Heyco> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 9969-9970: character maps to <undefined>
2018-11-12 05:41:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Keystone> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10051-10052: character maps to <undefined>
2018-11-12 05:42:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Bivar> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10075-10076: character maps to <undefined>
2018-11-12 05:42:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Panduit> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 9998-9999: character maps to <undefined>
2018-11-12 05:42:35 [scrapy.extensions.logstats] INFO: Crawled 569 pages (at 7 pages/min), scraped 372 items (at 0 items/min)
2018-11-12 05:42:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Acme-Industrial> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10119-10120: character maps to <undefined>
2018-11-12 05:42:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Marathon> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10065-10066: character maps to <undefined>
2018-11-12 05:42:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Sunon> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 9969-9970: character maps to <undefined>
2018-11-12 05:43:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Altech> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10106-10107: character maps to <undefined>
2018-11-12 05:43:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Hoffman-Enclosures> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10154-10155: character maps to <undefined>
2018-11-12 05:43:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Pomona-Electronics> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10165-10166: character maps to <undefined>
2018-11-12 05:43:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/RAF> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 9998-9999: character maps to <undefined>
2018-11-12 05:43:17 [scrapy.extensions.logstats] INFO: Crawled 577 pages (at 8 pages/min), scraped 372 items (at 0 items/min)
2018-11-12 05:43:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Southco> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10105-10106: character maps to <undefined>
2018-11-12 05:43:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Hammond> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10041-10042: character maps to <undefined>
2018-11-12 05:43:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/store?goto=1> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 8800-8801: character maps to <undefined>
2018-11-12 05:43:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/MillMaxPins> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 10231-10232: character maps to <undefined>
2018-11-12 05:43:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/store?body=address> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 8610-8611: character maps to <undefined>
2018-11-12 05:43:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/store?body=basket> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 8610-8611: character maps to <undefined>
2018-11-12 05:43:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/store?body=basket&typ=B> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 8610-8611: character maps to <undefined>
2018-11-12 05:44:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/store?o=BiscoProfile> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 8798-8799: character maps to <undefined>
2018-11-12 05:44:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/store?o=BiscoLocation> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 8845-8846: character maps to <undefined>
2018-11-12 05:44:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/store?o=BiscoValueAdded> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 8877-8878: character maps to <undefined>
2018-11-12 05:44:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/store?o=BiscoProducts> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 8874-8875: character maps to <undefined>
2018-11-12 05:44:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/Manufacturers> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 9224-9225: character maps to <undefined>
2018-11-12 05:44:20 [scrapy.extensions.logstats] INFO: Crawled 585 pages (at 8 pages/min), scraped 372 items (at 0 items/min)
2018-11-12 05:44:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/store?p=aog_info.html> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 8610-8611: character maps to <undefined>
2018-11-12 05:44:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/store?o=BiscoContact> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 8879-8880: character maps to <undefined>
2018-11-12 05:44:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/store> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 8991-8992: character maps to <undefined>
2018-11-12 05:44:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/store?forgot=1> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 8610-8611: character maps to <undefined>
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:45:36 [scrapy.extensions.logstats] INFO: Crawled 595 pages (at 10 pages/min), scraped 380 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:46:38 [scrapy.extensions.logstats] INFO: Crawled 602 pages (at 7 pages/min), scraped 386 items (at 6 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:47:42 [scrapy.extensions.logstats] INFO: Crawled 610 pages (at 8 pages/min), scraped 393 items (at 7 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:48:15 [scrapy.extensions.logstats] INFO: Crawled 610 pages (at 0 pages/min), scraped 396 items (at 3 items/min)
2018-11-12 05:48:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.biscoind.com/> (referer: https://www.biscoind.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/iso8859_15.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 8991-8992: character maps to <undefined>
/bin/sh: 1: kill: No such process

2018-11-12 05:48:26 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <406 http://www.brightleafpower.com/system-design-leadership/>: HTTP status code is not handled or not allowed
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:49:20 [scrapy.extensions.logstats] INFO: Crawled 628 pages (at 18 pages/min), scraped 409 items (at 13 items/min)
/bin/sh: 1: kill: No such process

valid
Crawling page: http://bruinbiometrics.com/us/about-us/careers       valid
Crawling page: http://bruinbiometrics.com/us/about-us/team       valid
Crawling page: http://bruinbiometrics.com/us/component/k2/item/114-top-prize-for-best-product-or-innovation-for-patient-safety       valid
Crawling page: https://www.wentworthlabs.com/cantilever-probe-cards/       valid
Crawling page: http://bruinbiometrics.com/us/about-us/board-of-directors       valid
Crawling page: http://bruinbiometrics.com/us/about-us/mission-values       valid
Crawling page: http://bruinbiometrics.com/us/about-us/about-us-2       valid
Crawling page: http://bruinbiometrics.com/us/about-us       valid
Crawling page: https://www.wentworthlabs.com/prober-accessories-probes/       valid
Crawling page: https://www.wentworthlabs.com/category/news/       valid
Crawling page: https://www.wentworthlabs.com/manufacturing-equipment/       valid
Crawling page: https://www.wentworthlabs.com/about-us/       valid
Crawling page: https://www.wentworthlabs.com/production-wafer-probers/       valid
Crawling page: https://www.wentworthlabs.com/analytical-wafer-probers/       valid
Crawling page: https://www.wentworthlabs.com/       valid
Crawling page: https://www.biscoind.com/store?p=privacy-policy.html       Crawling page: https://www.biscoind.com/store?p=terms.html       Crawling page: http://latitudepharma.com/cgmp-manufacturing/       valid
Crawling page: https://www.biscoind.com/store?o=BiscoCareer       Crawling page: http://latitudepharma.com/case-studies/       valid
Crawling page: http://www.brightleafpower.com/contact-us/       valid
Crawling page: http://www.brightleafpower.com/innovation/       valid
Crawling page: http://latitudepharma.com/rd-and-pilot-scale-manufacturing/       valid
Crawling page: http://latitudepharma.com/licensing-opportunities/       valid
Crawling page: https://www.biscoind.com/Unicorp       Crawling page: https://www.biscoind.com/MicroPlastics       Crawling page: http://www.brightleafpower.com/category/brightleaf-blogs/       valid
Crawling page: http://www.brightleafpower.com/employment/       valid
Crawling page: https://www.biscoind.com/Nord-Lock       Crawling page: http://www.brightleafpower.com       valid
Crawling page: https://www.biscoind.com/AlphaWire       Crawling page: https://www.biscoind.com/CaptiveFastenerCorp       Crawling page: http://latitudepharma.com/equipment-list/       valid
Crawling page: http://latitudepharma.com/analytical-methods/       valid
Crawling page: http://latitudepharma.com/deformulation-services/       valid
Crawling page: https://www.biscoind.com/zago-mfg       Crawling page: https://www.biscoind.com/Panduit-cableties-acces       Crawling page: https://www.biscoind.com/LynTronInc       Crawling page: https://www.biscoind.com/Optifuse       Crawling page: http://latitudepharma.com/formulation-services/       valid
Crawling page: http://latitudepharma.com/preformulation-services/       valid
Crawling page: http://latitudepharma.com/expertise-and-specialties/       valid
Crawling page: https://www.biscoind.com/Conta-Clip       Crawling page: https://www.biscoind.com/bisco-clinchfastener       Crawling page: https://www.biscoind.com/Birtcher-Pentair       Crawling page: https://www.biscoind.com/Bivar/1000001/LED+Indication       Crawling page: https://www.biscoind.com/Accuride       Crawling page: https://www.biscoind.com/AdvancedInterconnections       Crawling page: https://www.biscoind.com/Hantronix       Crawling page: https://www.biscoind.com/Richco/5200004/Fiber--Wire---Cable-Management       Crawling page: https://www.biscoind.com/Tyton       Crawling page: https://www.biscoind.com/PSM-CaptiveHardware       Crawling page: https://www.biscoind.com/3M       Crawling page: https://www.biscoind.com/MolexConnectors       Crawling page: https://www.biscoind.com/KATO       Crawling page: https://www.biscoind.com/Schroff-Pentair       Crawling page: https://www.biscoind.com/Richco       Crawling page: https://www.biscoind.com/Heyco       Crawling page: https://www.biscoind.com/Keystone       Crawling page: https://www.biscoind.com/Bivar       Crawling page: https://www.biscoind.com/Panduit       Crawling page: https://www.biscoind.com/Acme-Industrial       Crawling page: https://www.biscoind.com/Marathon       Crawling page: https://www.biscoind.com/Sunon       Crawling page: https://www.biscoind.com/Altech       Crawling page: https://www.biscoind.com/Hoffman-Enclosures       Crawling page: https://www.biscoind.com/Pomona-Electronics       Crawling page: https://www.biscoind.com/RAF       Crawling page: https://www.biscoind.com/Southco       Crawling page: https://www.biscoind.com/Hammond       Crawling page: https://www.biscoind.com/store?goto=1       Crawling page: https://www.biscoind.com/MillMaxPins       Crawling page: https://www.biscoind.com/store?body=address       Crawling page: https://www.biscoind.com/store?body=basket       Crawling page: https://www.biscoind.com/store?body=basket&typ=B       Crawling page: https://www.biscoind.com/store?o=BiscoProfile       Crawling page: https://www.biscoind.com/store?o=BiscoLocation       Crawling page: https://www.biscoind.com/store?o=BiscoValueAdded       Crawling page: https://www.biscoind.com/store?o=BiscoProducts       Crawling page: https://www.biscoind.com/Manufacturers       Crawling page: https://www.biscoind.com/store?p=aog_info.html       Crawling page: https://www.biscoind.com/store?o=BiscoContact       Crawling page: https://www.biscoind.com/store       Crawling page: https://www.biscoind.com/store?forgot=1       Crawling page: https://dynatrap.com/shipping-policy/       valid
Crawling page: https://dynatrap.com/privacy-policy/       valid
Crawling page: https://dynatrap.com/careers/       valid
Crawling page: https://dynatrap.com/return-policy/       valid
Crawling page: https://dynatrap.com/manuals/       valid
Crawling page: https://dynatrap.com/warranty-information/       valid
Crawling page: https://dynatrap.com/patents/       valid
Crawling page: https://dynatrap.com/sitemap_index.xml       valid
Crawling page: https://dynatrap.com/42day/       valid
Crawling page: https://dynatrap.com/product-category/renewed-products/       valid
Crawling page: https://dynatrap.com/retailers/       valid
Crawling page: https://dynatrap.com/products/       valid
Crawling page: https://dynatrap.com/contact/       valid
Crawling page: https://dynatrap.com/blog/       valid
Crawling page: https://dynatrap.com/reviews/       valid
Crawling page: https://dynatrap.com/cart/       valid
Crawling page: https://dynatrap.com       valid
Crawling page: https://dynatrap.com/store/cage-dt1050-decora/       valid
Crawling page: https://dynatrap.com/store/dynazap/       valid
Crawling page: https://dynatrap.com/registration/       valid
Crawling page: https://dynatrap.com/how-to-videos/       valid
Crawling page: https://dynatrap.com/how-to-use-your-dynatrap/       valid
Crawling page: https://dynatrap.com/faq/       valid
Crawling page: https://dynatrap.com/what-we-catch/       valid
Crawling page: https://www.biscoind.com/       Crawling page: https://www.slb.com/       valid
Crawling page: https://www.slb.com/services/seismic/unconventional_seismic.aspx       valid
Crawling page: https://www.slb.com/regional_sites.aspx       valid
Crawling page: http://www.brightleafpower.com/our-mission/       valid
Crawling page: http://bruinbiometrics.com/us/privacy-policy       valid
Crawling page: http://bruinbiometrics.com/us/forward-looking-statement       valid
Crawling page: http://www.brightleafpower.com/values/       valid
Crawling page: https://www.slb.com/contact_us.aspx       valid
Crawling page: https://www.slb.com/services/seismic/survey_design.aspx       valid
Crawling page: https://www.slb.com/services/drilling/mwd_lwd.aspx       valid
Crawling page: https://www.slb.com/services/drilling/contract-drilling.aspx       valid
Crawling page: https://www.slb.com/services/drilling/drilling_fluid.aspx       valid
Crawling page: https://www.slb.com/services/drilling/surface_logging.aspx       valid
Crawling page: https://www.slb.com/services/characterization/geophysics.aspx       valid/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:49:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/hyperthermia/bsd-2000-3d> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:49:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/clinical/bsd-2000-clinical-studies> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:50:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/about-us/in-the-news> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:50:33 [scrapy.extensions.logstats] INFO: Crawled 651 pages (at 23 pages/min), scraped 420 items (at 11 items/min)
/bin/sh: 1: kill: No such process

2018-11-12 05:50:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/about-us/how-to-buy> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:51:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/about-us/contact-us> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:51:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/about-us/distributor-login> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-12 05:51:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/wan-fang-opens-their-doors-to-share-hyperthermia> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:51:44 [scrapy.extensions.logstats] INFO: Crawled 652 pages (at 1 pages/min), scraped 429 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:52:17 [scrapy.extensions.logstats] INFO: Crawled 662 pages (at 10 pages/min), scraped 433 items (at 4 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:53:33 [scrapy.extensions.logstats] INFO: Crawled 670 pages (at 8 pages/min), scraped 442 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:54:16 [scrapy.extensions.logstats] INFO: Crawled 673 pages (at 3 pages/min), scraped 448 items (at 6 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:54:41 [scrapy.core.scraper] ERROR: Error downloading <GET https://pyrexar.com/pyrexar-signs-with-viet-nam>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 05:54:41 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.xyleco.com/patents/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:54:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://pyrexar.com/pyrexar-adds-thailand-distributor>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 05:54:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://pyrexar.com/visit-us-at-astro-2018>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 05:54:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.xyleco.com/facilities/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 05:54:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.xyleco.com/impacts/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 05:54:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.xyleco.com/news/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 05:54:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://latitudepharma.com/feed/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:55:30 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.xyleco.com/2018/01/21/xyleco-inc-announces-comparative-life-cycle-assessment-cellulosic-ethanol/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 05:55:30 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.xyleco.com/contact-us/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 05:55:30 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.xyleco.com/hiring/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 05:55:30 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.avalanche-technology.com/applications/consumer>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 05:55:30 [scrapy.core.scraper] ERROR: Error downloading <GET https://biologicaldynamics.com/licenses-accreditation>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 05:55:30 [scrapy.core.scraper] ERROR: Error downloading <GET https://marketing.pyrexar.com/acton/form/15060/0027:d-0001/1/-/-/-/-/index.htm>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 05:55:30 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.avalanche-technology.com/applications/iot>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 05:55:30 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.avalanche-technology.com/company/board/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 05:55:30 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.avalanche-technology.com/company/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 05:55:30 [scrapy.extensions.logstats] INFO: Crawled 679 pages (at 6 pages/min), scraped 457 items (at 9 items/min)
2018-11-12 05:55:30 [scrapy.core.scraper] ERROR: Error downloading <GET https://biologicaldynamics.com/careers>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 05:55:30 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.avalanche-technology.com/products/embedded-mram/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 05:55:30 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.avalanche-technology.com/products/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 05:55:30 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.avalanche-technology.com/products/discrete-mram/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 05:55:30 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.avalanche-technology.com/technology/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
/bin/sh: 1: kill: No such process

2018-11-12 05:55:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <500 http://bruinbiometrics.com/us/site-map>: HTTP status code is not handled or not allowed
/bin/sh: 1: kill: No such process

2018-11-12 05:55:42 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.wentworthlabs.com/privacy-policy/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:56:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/clinical> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:56:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/hyperthermia/bsd-2000-3d-mr> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:56:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <406 http://www.brightleafpower.com/executive-leadership/>: HTTP status code is not handled or not allowed
2018-11-12 05:56:52 [scrapy.core.scraper] ERROR: Error downloading <GET http://dynatrap.com/products/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 05:56:52 [scrapy.extensions.logstats] INFO: Crawled 704 pages (at 25 pages/min), scraped 470 items (at 13 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:57:17 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.slb.com/about/privacy.aspx>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 05:57:17 [scrapy.core.scraper] ERROR: Error downloading <GET https://pyrexar.com/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 05:57:17 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.xyleco.com/cellulosic-fuels/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 05:57:17 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.xyleco.com/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 05:57:17 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.xyleco.com/technology/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 05:57:17 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.xyleco.com/end-products/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 05:57:17 [scrapy.extensions.logstats] INFO: Crawled 704 pages (at 0 pages/min), scraped 474 items (at 4 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:57:34 [scrapy.core.scraper] ERROR: Error downloading <GET https://gold.tanaka.co.jp/english/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 05:57:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 http://blog.ethox.com/page/2>: HTTP status code is not handled or not allowed
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:58:50 [scrapy.extensions.logstats] INFO: Crawled 739 pages (at 35 pages/min), scraped 486 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 05:59:18 [scrapy.extensions.logstats] INFO: Crawled 743 pages (at 4 pages/min), scraped 490 items (at 4 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process


Crawling page: https://www.slb.com/services/drilling/fluids_processing.aspx       valid
Crawling page: https://www.slb.com/services/characterization/petrophysics.aspx       valid
Crawling page: https://www.slb.com/services/drilling/specialty_drilling_applications.aspx       valid
Crawling page: https://www.slb.com/services/characterization/wireline_conveyance.aspx       valid
Crawling page: https://www.slb.com/services/characterization/real_time_operations_support.aspx       valid
Crawling page: https://pyrexar.com/hyperthermia/bsd-2000-3d       Crawling page: https://pyrexar.com/clinical/bsd-2000-clinical-studies       Crawling page: http://www.avalanche-technology.com/applications/storage/       valid
Crawling page: http://www.avalanche-technology.com/company/leadership/       valid
Crawling page: https://www.slb.com/services/processing-separation/oilfield-water-services.aspx       valid
Crawling page: https://www.slb.com/services/processing-separation/acid-gas-membrane-performance.aspx       valid
Crawling page: https://www.slb.com/services/processing-separation/integrated-facilities.aspx       valid
Crawling page: https://pyrexar.com/about-us/in-the-news       Crawling page: https://www.slb.com/services/processing-separation/gas-treatment.aspx       valid
Crawling page: https://pyrexar.com/about-us/how-to-buy       Crawling page: https://www.slb.com/services/processing-separation/water-treatment.aspx       valid
Crawling page: https://www.slb.com/services/processing-separation/oil-treatment.aspx       valid
Crawling page: https://www.slb.com/services/processing-separation/solids-management.aspx       valid
Crawling page: https://pyrexar.com/about-us/contact-us       Crawling page: https://pyrexar.com/about-us/distributor-login       Crawling page: https://pyrexar.com/wan-fang-opens-their-doors-to-share-hyperthermia       Crawling page: https://www.slb.com/services/well_intervention.aspx       valid
Crawling page: https://www.slb.com/services/well_intervention/coiled_tubing.aspx       valid
Crawling page: https://www.slb.com/services/well_intervention/wireline_cased_hole_intervention.aspx       valid
Crawling page: https://www.slb.com/services/well_intervention/slickline.aspx       valid
Crawling page: http://www.avalanche-technology.com/applications/storage/       valid
Crawling page: https://www.slb.com/services/well_intervention/thru_tubing_intervention.aspx       valid
Crawling page: https://www.hyperionmt.com/en/sitemap/       valid
Crawling page: https://www.hyperionmt.com/en/events/       valid
Crawling page: https://www.hyperionmt.com/en/certifications/       valid
Crawling page: https://www.hyperionmt.com/en/copyright/       valid
Crawling page: https://www.hyperionmt.com/en/offerings/toolmaker-solutions/pcd/       valid
Crawling page: https://www.hyperionmt.com/en/offerings/toolmaker-solutions/Rotary-Tool-Blanks/       valid
Crawling page: https://www.hyperionmt.com/en/offerings/toolmaker-solutions/insert-blanks/       valid
Crawling page: https://www.hyperionmt.com/en/offerings/toolmaker-solutions/Slurries/       valid
Crawling page: https://www.hyperionmt.com/en/offerings/toolmaker-solutions/gear-hob-blanks/       valid
Crawling page: https://www.slb.com/resources/other_resources/catalogs.aspx       valid
Crawling page: https://www.hyperionmt.com/en/offerings/toolmaker-solutions/wire-dies/       valid
Crawling page: https://www.hyperionmt.com/en/offerings/toolmaker-solutions/abrasives/       valid
Crawling page: https://www.hyperionmt.com/en/offerings/toolmaker-solutions/base-materials/       valid
Crawling page: https://www.slb.com/services/drilling/drilling_services_systems/rhino_system/cutter_blocks/stingblock_cutter_block.aspx       valid
Crawling page: https://www.slb.com/services/drilling/mwd_lwd/mwd/gyro-while-drilling.aspx       valid
Crawling page: https://www.hyperionmt.com/en/offerings/engineered-solutions/wear-parts/       valid
Crawling page: https://www.hyperionmt.com/en/offerings/engineered-solutions/hot-rolls/       valid
Crawling page: https://www.hyperionmt.com/en/offerings/engineered-solutions/Components-and-Sub-Assemblies/       valid
Crawling page: https://www.hyperionmt.com/en/offerings/engineered-solutions/rotary-cutters/       valid
Crawling page: https://www.hyperionmt.com/en/offerings/?section=offeringSection4       valid
Crawling page: https://www.hyperionmt.com/       valid
Crawling page: https://www.hyperionmt.com/en/offerings/       valid
Crawling page: https://www.hyperionmt.com/en/technologies/       valid
Crawling page: https://www.hyperionmt.com/en/contact-us/       valid
Crawling page: https://www.hyperionmt.com/en/about-us/       valid
Crawling page: https://www.hyperionmt.com/en/       valid
Crawling page: https://www.hyperionmt.com/en/sitemap/       valid
Crawling page: https://biologicaldynamics.com/publications-all/2018/3/29/integrated-analysis-of-exosomal-protein-biomarkers-on-alternating-current-electrokinetic-chips-enables-rapid-detection-of-pancreatic-cancer-in-patient-blood       valid
Crawling page: https://biologicaldynamics.com/publications-all/2016/5/10/dielectrophoretic-recovery-of-dna-from-plasma-for-the-identification-of-chronic-lymphocytic-leukemia-point-mutations       valid
Crawling page: https://biologicaldynamics.com/publications-all/2015/8/27/recovery-of-drug-delivery-nanoparticles-from-human-plasma-using-an-electrokinetic-platform-technology       valid
Crawling page: https://biologicaldynamics.com/publications-all/2017/7/6/rapid-isolation-exosomes-biomarkers-plasma       valid
Crawling page: http://www.polar-light-technologies.com/contact/       valid
Crawling page: http://www.polar-light-technologies.com/blog/       valid
Crawling page: https://pyrexar.com/clinical       Crawling page: http://www.polar-light-technologies.com/2018/01/02/hello-world/       valid
Crawling page: http://www.polar-light-technologies.com/       valid
Crawling page: https://pyrexar.com/hyperthermia/bsd-2000-3d-mr       Crawling page: http://www.brightleafpower.com/about-us/       valid
Crawling page: http://www.brightleafpower.com/manufacturing-leadership/       valid
Crawling page: http://www.brightleafpower.com/leadership/       valid
Crawling page: http://www.avalanche-technology.com/       valid
Crawling page: http://www.brightleafpower.com/board-of-directors/       valid
Crawling page: https://biologicaldynamics.com/       valid
Crawling page: http://www.brightleafpower.com/education/       valid
Crawling page: http://www.brightleafpower.com/solutions/       valid
Crawling page: http://www.brightleafpower.com/hotels/       valid
Crawling page: http://latitudepharma.com/       valid
Crawling page: http://latitudepharma.com/formulation-platforms/       valid
Crawling page: http://latitudepharma.com/the-latitude-team/       valid
Crawling page: http://www.brightleafpower.com/commercial/       valid
Crawling page: https://www.plex.tv/       valid
Crawling page: https://www.plex.tv/about/privacy-legal/       valid
Crawling page: https://www.plex.tv/plex-pass/gift/       valid
Crawling page: http://blog.ethox.com/resurgence-of-the-textile-industry       valid
Crawling page: https://www.plex.tv/contact/?option=plex-pass-billing       valid
Crawling page: http://blog.ethox.com/page/1       valid
Crawling page: https://www.plex.tv/plex-pass/perks/       valid
Crawling page: https://www.plex.tv/contact/       valid
Crawling page: https://www.plex.tv/plex-labs/       valid
Crawling page: https://www.harris.com//       valid
Crawling page: http://www.ewa.com/warriorROH.php       valid
Crawling page: https://www.harris.com/terms-of-use       valid
Crawling page: http://ewa.com/sta.php       valid
Crawling page: http://ewa.com/fowler.php       valid
Crawling page: http://ewa.com/contact-us.php       valid
Crawling page: http://ewa.com/warrior.php       valid
Crawling page: https://www.harris.com/privacy-policy       valid
Crawling page: http://ewa.com/pressRelease.php       valid
Crawling page: http://ewa.com/rd.php       valid
Crawling page: http://ewa.com/small-business-opportunities.php       valid
Crawling page: http://ewa.com/facilities.php       valid
Crawling page: http://ewa.com/technical-expertise.php/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:00:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://investors.siennabio.com/contact-ir> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
/bin/sh: 1: kill: No such process

2018-11-12 06:00:38 [scrapy.extensions.logstats] INFO: Crawled 760 pages (at 17 pages/min), scraped 506 items (at 16 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:01:29 [scrapy.extensions.logstats] INFO: Crawled 775 pages (at 15 pages/min), scraped 517 items (at 11 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:02:14 [scrapy.extensions.logstats] INFO: Crawled 783 pages (at 8 pages/min), scraped 526 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:03:20 [scrapy.extensions.logstats] INFO: Crawled 797 pages (at 14 pages/min), scraped 539 items (at 13 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:04:22 [scrapy.extensions.logstats] INFO: Crawled 808 pages (at 11 pages/min), scraped 551 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

       valid
Crawling page: http://ewa.com/services.php       valid
Crawling page: http://www.yageo.com/portal/about_yageo/about.jsp?SWITCH_CATEGORY=/about_yageo/Corporate%20overview&menuid=0&title1=c1       valid
Crawling page: https://www.harris.com/stock-quote-disclaimer       valid
Crawling page: http://www.yageo.com/portal/Investor_relations/Investor.jsp?menuid=631&SWITCH_CATEGORY=/Investor_relations/Shareholders%20Services/Stock%20info       valid
Crawling page: https://www.harris.com/about/the-harris-story       valid
Crawling page: https://investors.siennabio.com/contact-ir       Crawling page: https://astx.com/       valid
Crawling page: http://www.yageo.com/portal/privacy_policy/privacy.jsp?menuid=921&SWITCH_CATEGORY=/privacy_policy/Terms%20of%20Use       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp       valid
Crawling page: http://www.yageo.com/portal/privacy_policy/privacy.jsp?menuid=911&SWITCH_CATEGORY=/privacy_policy/Privacy%20Policy       valid
Crawling page: http://www.yageo.com/portal/sitemap/Sitemap.jsp       valid
Crawling page: http://www.yageo.com/portal/product/open_literature.jsp?SWITCH_CATEGORY=/product/open_qualityReport       valid
Crawling page: http://www.yageo.com/portal/product/open_literature.jsp?SWITCH_CATEGORY=/product/open_literature       valid
Crawling page: http://www.yageo.com/portal/product/open_literature.jsp?SWITCH_CATEGORY=/product/open_certificates       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/High%20Frequency/RF%20Components       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Inductors/Power%20Inductors       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/High%20Frequency/Metal%20Antenna       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/High%20Frequency/Chip/Patch/Active%20Antenna       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Inductors/Wirewound%20Chip%20Inductors       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Inductors/Multilayer%20Chip%20Inductors       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Inductors/Chip%20Beads       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Ecap/Polymer       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Ecap/Large%20Can       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Ecap/Surface%20Mount       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Ecap/Screw%20Type       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Ecap/Miniature       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/LeadedR/Aluminum%20House       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/LeadedR/Power%20Wirewound       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/LeadedR/High%20Temp._%20Humidity%20Resistors       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/LeadedR/Cement       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/LeadedR/Jumper%20Wires       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/LeadedR/Fusible%20Wirewound       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/LeadedR/Wirewound       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/LeadedR/Low%20Ohmic%20Wire       valid
Crawling page: http://www.asm.com/privacy-statement       valid
Crawling page: http://www.asm.com/disclaimer       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/LeadedR/Zero%20Ohm       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/LeadedR/Pulse-Loading       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/LeadedR/High%20Voltage%20and%20High%20Ohmic       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/LeadedR/Biased%20Humidity       valid
Crawling page: http://www.asm.com/Pages/Press-releases/ASM-INTERNATIONAL-NV-ANNOUNCES-COMPLETION-OF-SHARE-BUYBACK-PROGRAM-2018-10.aspx?Overview=http://www.asm.com:80/Pages/home.aspx       valid
Crawling page: http://www.asm.com/Pages/Press-releases/ASM-INTERNATIONAL-NV-REPORTS-THIRD-QUARTER-2018-RESULTS.aspx?Overview=http://www.asm.com:80/Pages/home.aspx       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/LeadedR/Melf%20Carbon%20Film       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/LeadedR/Carbon%20Film       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/LeadedR/Melf%20Metal%20Film       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/LeadedR/Metal%20Oxide%20Film       valid
Crawling page: http://www.asm.com/Pages/Press-releases/ASM-INTERNATIONAL-NV-ANNOUNCES-AVAILABILITY-AND-TIMING-OF-THE-THIRD-QUARTER-2018-CONFERENCE-CALL-AND-WEBCAST.aspx?Overview=http://www.asm.com:80/Pages/home.aspx       valid
Crawling page: http://www.asm.com/Pages/Press-releases/ASM-INTERNATIONAL-RESPONDS-TO-MARKET-RUMOURS-REGARDING-INTEREST-FOR-ITS-STAKE-IN-ASM-PT.aspx?Overview=http://www.asm.com:80/Pages/home.aspx       valid
Crawling page: http://www.asm.com/investors/your-documents       valid
Crawling page: http://www.asm.com/investors/contact       valid
Crawling page: http://www.asm.com/Pages/Press-releases/ASM-INTERNATIONAL-NV-RECEIVES-INTEL-S-PREFERRED-QUALITY-SUPPLIER-AWARD-2018-03.aspx       valid
Crawling page: http://www.asm.com/investors/webcasts-presentations/2018/third-quarter-earnings       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/LeadedR/Metal%20Film       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/MLCC/MLV       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/MLCC/Low-inductance       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/MLCC/X2Y       valid
Crawling page: http://www.asm.com/investors/press-releases/sign-up       valid
Crawling page: http://www.asm.com/investors/calendar       valid
Crawling page: http://www.asm.com/investors/investor-library/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:05:17 [scrapy.extensions.logstats] INFO: Crawled 815 pages (at 7 pages/min), scraped 561 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:06:19 [scrapy.extensions.logstats] INFO: Crawled 832 pages (at 17 pages/min), scraped 573 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:07:15 [scrapy.extensions.logstats] INFO: Crawled 849 pages (at 17 pages/min), scraped 584 items (at 11 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:08:39 [scrapy.extensions.logstats] INFO: Crawled 859 pages (at 10 pages/min), scraped 599 items (at 15 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:09:15 [scrapy.extensions.logstats] INFO: Crawled 863 pages (at 4 pages/min), scraped 606 items (at 7 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:10:28 [scrapy.extensions.logstats] INFO: Crawled 872 pages (at 9 pages/min), scraped 619 items (at 13 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

       valid
Crawling page: http://www.asm.com/investors/press-releases       valid
Crawling page: http://www.asm.com/investors/corporate-reports/annual-reports-(us-gaap)-forms-20-f       valid
Crawling page: http://www.asm.com/investors/webcasts-presentations       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/MLCC/Microwave       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/MLCC/High-Voltage       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/MLCC/Mid-Voltage       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/MLCC/High%20Frequency       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/MLCC/SoftTermination       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/MLCC/C-Array       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/MLCC/General%20Purpose       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/MLCC/High-Cap       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Rchip/RF%20Attenuator       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Rchip/R%20wNi/Au%20terminations       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Rchip/Anti-Sulfurated       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Rchip/Surge       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Rchip/High%20voltage       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Rchip/Trimmable       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Rchip/Array%20Network       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Rchip/Current%20Sensor       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Rchip/Thin%20Film       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Rchip/Low%20Ohmic       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Rchip/High%20Ohmic       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Rchip/General%20Purpose%20High%20Precison       valid
Crawling page: http://www.asm.com/investors/corporate-reports/quarterly-reports       valid
Crawling page: http://www.asm.com/investors/corporate-reports       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Rchip/Automotive%20Grade       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/fullProductSearch.jsp&SWITCH_CATEGORY=/product/Search%20Product/Full%20Search%20By%20Number       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Rchip/General%20Purpose       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/crossReferenceSearch.jsp&SWITCH_CATEGORY=/product/Search%20Product/Cross%20Reference%20Search       valid
Crawling page: http://www.asm.com/investors/corporate-governance/policies       valid
Crawling page: http://www.asm.com/investors/corporate-governance/articles-of-association       valid
Crawling page: http://www.asm.com/investors/corporate-reports/corporate-responsibility-reports       valid
Crawling page: http://www.asm.com/investors/corporate-reports/statutory-annual-reports-interim-reports-(ifrs)       valid
Crawling page: http://www.yageo.com/portal/contact_us/contact.jsp?menuid=811&SWITCH_CATEGORY=/contact_us/Yageo%20Offices       valid
Crawling page: http://www.yageo.com/portal/contact_us/contact.jsp?menuid=821&SWITCH_CATEGORY=/contact_us/Destributors       valid
Crawling page: http://www.yageo.com/portal/contact_us/contact.jsp?menuid=831&SWITCH_CATEGORY=/contact_us/Feedback       valid
Crawling page: http://www.yageo.com/portal/career/career.jsp?menuid=721&SWITCH_CATEGORY=/career/Learning%20and%20Development       valid
Crawling page: http://www.asm.com/investors/corporate-governance/management-board       valid
Crawling page: http://www.asm.com/investors/corporate-governance/supervisory-board       valid
Crawling page: http://www.asm.com/investors/corporate-governance       valid
Crawling page: http://www.asm.com/investors/corporate-governance/agm       valid
Crawling page: http://www.yageo.com/portal/career/career.jsp?menuid=711&SWITCH_CATEGORY=/career/Join%20Yageo       valid
Crawling page: http://www.yageo.com/portal/Investor_relations/Investor.jsp?menuid=651&SWITCH_CATEGORY=/Investor_relations/FAQ       valid
Crawling page: http://www.yageo.com/portal/Investor_relations/Investor.jsp?menuid=641&SWITCH_CATEGORY=/Investor_relations/Press%20Room/Latest%20News       valid
Crawling page: http://www.asm.com/investors/share-information/dividend-and-capital-repayment       valid
Crawling page: http://www.asm.com/investors/share-information/research-coverage       valid
Crawling page: http://www.asm.com/investors/share-information/investment-calculator       valid
Crawling page: http://www.asm.com/investors/share-information/share-buyback       valid
Crawling page: http://www.asm.com/investors/share-information/historical-lookup       valid
Crawling page: http://www.asm.com/investors/share-information/share-price       valid
Crawling page: http://www.asm.com/investors/share-information       valid
Crawling page: http://www.asm.com/investors/asmi-investors       valid
Crawling page: http://www.asm.com/investors       valid
Crawling page: http://www.asm.com/en/careers/contact       valid
Crawling page: http://www.asm.com/en/careers/faq       valid
Crawling page: http://www.asm.com/en/careers/video-gallery       valid
Crawling page: http://www.asm.com/en/careers/join-us/open-application       valid
Crawling page: http://www.asm.com/en/careers/join-us/job-list-europe       valid
Crawling page: http://www.asm.com/en/careers/join-us/how-to-apply       valid
Crawling page: http://www.asm.com/en/careers/join-us/job-list-north-america       valid
Crawling page: http://www.asm.com/en/careers/join-us/job-list-asia       valid
Crawling page: http://www.asm.com/en/careers/why-work-for-asm/meet-our-young-professionals       valid
Crawling page: http://www.asm.com/en/careers/why-work-for-asm/job-benefits       valid
Crawling page: http://www.asm.com/en/careers/why-work-for-asm/top-reasons-to-work-for-asm       valid
Crawling page: http://www.asm.com/en/careers/why-work-for-asm       valid
Crawling page: http://www.asm.com/en/careers/why-work-for-asm/career-development       valid
Crawling page: http://www.asm.com/en/careers/experienced-professionals/the-next-step       valid
Crawling page: http://www.asm.com/en/careers/join-us       valid
Crawling page: http://www.asm.com/en/careers/graduates/new-college-graduate-program       valid
Crawling page: http://www.yageo.com/portal/Investor_relations/Investor.jsp?menuid=621&SWITCH_CATEGORY=/Investor_relations/Corporate%20Goverance/Statement%20and%20Memorandum/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:11:32 [scrapy.extensions.logstats] INFO: Crawled 888 pages (at 16 pages/min), scraped 631 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:12:24 [scrapy.extensions.logstats] INFO: Crawled 894 pages (at 6 pages/min), scraped 641 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:13:29 [scrapy.extensions.logstats] INFO: Crawled 904 pages (at 10 pages/min), scraped 654 items (at 13 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:14:27 [scrapy.extensions.logstats] INFO: Crawled 919 pages (at 15 pages/min), scraped 666 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:15:47 [scrapy.extensions.logstats] INFO: Crawled 934 pages (at 15 pages/min), scraped 676 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:16:16 [scrapy.extensions.logstats] INFO: Crawled 938 pages (at 4 pages/min), scraped 680 items (at 4 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:17:22 [scrapy.extensions.logstats] INFO: Crawled 944 pages (at 6 pages/min), scraped 688 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:18:20 [scrapy.extensions.logstats] INFO: Crawled 953 pages (at 9 pages/min), scraped 698 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

       valid
Crawling page: http://www.asm.com/en/careers/experienced-professionals/leading-edge-technologies       valid
Crawling page: http://www.asm.com/en/careers/experienced-professionals       valid
Crawling page: http://www.asm.com/en/careers/graduates/your-future       valid
Crawling page: http://www.asm.com/en/careers/graduates/internships       valid
Crawling page: http://www.yageo.com/portal/customers/customers.jsp?SWITCH_CATEGORY=/customers/Literature&menuid=3&title1=c3       valid
Crawling page: http://www.yageo.com/portal/customers/customers.jsp?SWITCH_CATEGORY=/customers/Request%20for%20Quotation&menuid=2&title1=c2       valid
Crawling page: http://www.yageo.com/portal/Investor_relations/Investor.jsp?menuid=611&SWITCH_CATEGORY=/Investor_relations/Financials/Business%20Overview       valid
Crawling page: http://www.asm.com/en/careers/graduates       valid
Crawling page: http://www.asm.com/en/careers/discover-asm/product-lines       valid
Crawling page: http://www.asm.com/en/careers/discover-asm/our-role-in-the-world       valid
Crawling page: http://www.yageo.com/portal/customers/customers.jsp?SWITCH_CATEGORY=/customers/B2B&menuid=0&title1=c1       valid
Crawling page: http://www.asm.com/en/careers/discover-asm/inside-asm       valid
Crawling page: http://www.asm.com/en/careers/discover-asm/innovation-driven       valid
Crawling page: http://www.yageo.com/portal/services/services.jsp?SWITCH_CATEGORY=/services/Value%20Added%20Services&menuid=3&title1=c4       valid
Crawling page: http://www.yageo.com/portal/services/services.jsp?SWITCH_CATEGORY=/services/Design%20In%20Service&menuid=4&title1=c5       valid
Crawling page: http://www.yageo.com/portal/services/services.jsp?SWITCH_CATEGORY=/services/EDI&menuid=5&title1=c6       valid
Crawling page: http://www.asm.com/en/careers/discover-asm/fields-of-interest       valid
Crawling page: http://www.yageo.com/portal/services/services.jsp?SWITCH_CATEGORY=/services/Vendor%20Managed%20Inventory&menuid=2&title1=c3       valid
Crawling page: http://www.yageo.com/portal/applications/applications.jsp?SWITCH_CATEGORY=/applications/DSC%20Application%20Overview&menuid=10&title1=c11&title2=c111       valid
Crawling page: http://www.yageo.com/portal/applications/applications.jsp?SWITCH_CATEGORY=/applications/LCD%20TV%20Application%20Overview&menuid=9&title1=c10&title2=c101       valid
Crawling page: http://www.yageo.com/portal/services/services.jsp?SWITCH_CATEGORY=/services/JIT%20Warehouse&menuid=1&title1=c2       valid
Crawling page: http://www.yageo.com/portal/services/services.jsp?SWITCH_CATEGORY=/services/Global%20Logistics&menuid=0&title1=c1       valid
Crawling page: http://www.yageo.com/portal/applications/applications.jsp?SWITCH_CATEGORY=/applications/Networking%20Application%20Overview&menuid=8&title1=c9&title2=c91       valid
Crawling page: http://www.yageo.com/portal/applications/applications.jsp?SWITCH_CATEGORY=/applications/Home%20Appliance%20Application%20Overview&menuid=7&title1=c8&title2=c81       valid
Crawling page: http://www.yageo.com/portal/applications/applications.jsp?SWITCH_CATEGORY=/applications/Medical%20Application%20Overview&menuid=6&title1=c7&title2=c71       valid
Crawling page: http://www.yageo.com/portal/applications/applications.jsp?SWITCH_CATEGORY=/applications/Automotive%20Application%20Overview&menuid=5&title1=c6&title2=c61       valid
Crawling page: http://www.yageo.com/portal/applications/applications.jsp?SWITCH_CATEGORY=/applications/Alternative%20Energy%20Application%20Overview&menuid=4&title1=c5&title2=c5       valid
Crawling page: http://www.yageo.com/portal/applications/applications.jsp?SWITCH_CATEGORY=/applications/Industrial%20Application%20Overview&menuid=2&title1=c3&title2=c31       valid
Crawling page: http://www.yageo.com/portal/applications/applications.jsp?SWITCH_CATEGORY=/applications/Power%20Application%20Overview&menuid=3&title1=c4&title2=c41       valid
Crawling page: http://www.yageo.com/portal/applications/applications.jsp?SWITCH_CATEGORY=/applications/Mobile%20Application%20Overview&menuid=1&title1=c2&title2=c21       valid
Crawling page: http://www.yageo.com/portal/applications/applications.jsp?SWITCH_CATEGORY=/applications/Notebook%20Application%20Overview&menuid=0&title1=c01&title2=c011       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/High%20Frequency/High%20Frequency       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Inductors/Inductors       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Ecap/E-Cap       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/LeadedR/Leaded%20Resistors       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/MLCC/MLCC       valid
Crawling page: http://www.yageo.com/portal/product/product.jsp?NEXT_PAGE=/product/categorySearch.jsp&SWITCH_CATEGORY=/product/Rchip/Chip%20Resistor       valid
Crawling page: http://www.yageo.com/portal/about_yageo/about.jsp?SWITCH_CATEGORY=/about_yageo/Certificates%20and%20Awards&menuid=4&title1=c5       valid
Crawling page: http://www.yageo.com/portal/about_yageo/about.jsp?SWITCH_CATEGORY=/about_yageo/RoHS/ELV%20Compliant%20Product%20List&menuid=3&title1=c4       valid
Crawling page: http://www.yageo.com/portal/about_yageo/about.jsp?SWITCH_CATEGORY=/about_yageo/Environment%20and%20Safety&menuid=2&title1=c3       valid
Crawling page: http://www.yageo.com/portal/about_yageo/about.jsp?SWITCH_CATEGORY=/about_yageo/Social%20Responsibility&menuid=1&title1=c2       valid
Crawling page: https://webmail.tri-intl.com/gw/webacc       valid
Crawling page: https://astx.com/?s=       valid
Crawling page: https://tri-intl.com/project/ecomass-compounds/       valid
Crawling page: https://tri-intl.com/slope-stability-interface/       valid
Crawling page: https://tri-intl.com/project/cbw-safe-water-pouch/       valid
Crawling page: https://tri-intl.com/timeline-of-innovation/       valid
Crawling page: https://astx.com/sitemap/       valid
Crawling page: https://tri-intl.com/project/thermasafe/       valid
Crawling page: https://tri-intl.com/project/rubber-cement/       valid
Crawling page: https://tri-intl.com/project/lahmp-system/       valid
Crawling page: https://tri-intl.com/project/flex-hose/       valid
Crawling page: https://tri-intl.com/project/compressed-air-testing-kit/       valid
Crawling page: https://tri-intl.com/portfolio/       valid
Crawling page: https://tri-intl.com/contact-us/       valid
Crawling page: https://tri-intl.com/newsroom/upcoming-events/       valid
Crawling page: https://tri-intl.com/newsroom/press-releases/       valid
Crawling page: https://tri-intl.com/newsroom/       valid
Crawling page: https://tri-intl.com/services/geo-u/       valid
Crawling page: https://tri-intl.com/services/marketing-technologies/       valid
Crawling page: http://www.asm.com/en/careers/discover-asm/diversity       valid
Crawling page: http://www.asm.com/jp/careers/faq       valid
Crawling page: http://www.asm.com/en/careers/discover-asm       valid
Crawling page: http://www.asm.com/en/careers/discover-asm/culture       valid
Crawling page: http://www.asm.com/en/careers       valid
Crawling page: https://tri-intl.com/services/technology-teams/       valid
Crawling page: http://www.asm.com/jp/careers/video-gallery       valid
Crawling page: http://www.asm.com/jp/careers/join-us/job-list-europe       valid
Crawling page: http://www.asm.com/jp/careers/join-us/job-list-asia       valid
Crawling page: http://www.asm.com/jp/careers/join-us/job-list-north-america       valid
Crawling page: http://www.asm.com/jp/careers/why-work-for-asm/top-reasons-to-work-for-asm       valid
Crawling page: http://www.asm.com/jp/careers/join-us/how-to-apply       valid
Crawling page: http://www.asm.com/jp/careers/why-work-for-asm/job-benefits       valid
Crawling page: http://www.asm.com/jp/careers/why-work-for-asm/career-development/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:19:30 [scrapy.extensions.logstats] INFO: Crawled 965 pages (at 12 pages/min), scraped 711 items (at 13 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:19:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <408 https://astx.com/astex-pharmaceuticals-and-otsuka-announce-results-of-the-phase-3-astral-1-study-of-guadecitabine-sgi-110-in-treatment-naive-aml-patients-ineligible-to-receive-intense-induction-chemotherapy/>: HTTP status code is not handled or not allowed
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:20:24 [scrapy.extensions.logstats] INFO: Crawled 976 pages (at 11 pages/min), scraped 721 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:21:27 [scrapy.extensions.logstats] INFO: Crawled 986 pages (at 10 pages/min), scraped 733 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:22:20 [scrapy.extensions.logstats] INFO: Crawled 998 pages (at 12 pages/min), scraped 743 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:22:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://en.wikipedia.org/wiki/CeramTec> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 06:22:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <999 https://www.linkedin.com/company/ceramtec-%e2%80%93-the-ceramic-experts/>: HTTP status code is not handled or not allowed
2018-11-12 06:22:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.ceramtec.com/feed/news/> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:23:13 [scrapy.extensions.logstats] INFO: Crawled 1006 pages (at 8 pages/min), scraped 749 items (at 6 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:23:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://plus.google.com/browser-not-supported/?ref=/u/0/108274519983787163310/posts> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 06:23:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://twitter.com/ceramtec> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
/bin/sh: 1: kill: No such process

2018-11-12 06:24:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.youtube.com/user/ceramtecceramics> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:24:58 [scrapy.extensions.logstats] INFO: Crawled 1015 pages (at 9 pages/min), scraped 758 items (at 9 items/min)
/bin/sh: 1: kill: No such process

2018-11-12 06:25:13 [scrapy.extensions.logstats] INFO: Crawled 1016 pages (at 1 pages/min), scraped 759 items (at 1 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:26:12 [scrapy.extensions.logstats] INFO: Crawled 1020 pages (at 4 pages/min), scraped 763 items (at 4 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:27:24 [scrapy.extensions.logstats] INFO: Crawled 1027 pages (at 7 pages/min), scraped 768 items (at 5 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:28:15 [scrapy.extensions.logstats] INFO: Crawled 1030 pages (at 3 pages/min), scraped 773 items (at 5 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:29:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://investors.siennabio.com/email-alerts> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:29:59 [scrapy.extensions.logstats] INFO: Crawled 1047 pages (at 17 pages/min), scraped 782 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:30:34 [scrapy.extensions.logstats] INFO: Crawled 1048 pages (at 1 pages/min), scraped 786 items (at 4 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:31:17 [scrapy.extensions.logstats] INFO: Crawled 1052 pages (at 4 pages/min), scraped 791 items (at 5 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

       valid
Crawling page: http://www.asm.com/jp/careers/why-work-for-asm/meet-our-young-professionals       valid
Crawling page: http://www.asm.com/jp/careers/join-us       valid
Crawling page: http://www.asm.com/jp/careers/why-work-for-asm       valid
Crawling page: http://www.asm.com/jp/careers/experienced-professionals/the-next-step       valid
Crawling page: http://www.asm.com/jp/careers/experienced-professionals/leading-edge-technologies       valid
Crawling page: http://www.asm.com/jp/careers/experienced-professionals       valid
Crawling page: http://www.asm.com/jp/careers/graduates/your-future       valid
Crawling page: http://www.asm.com/jp/careers/graduates/internships       valid
Crawling page: http://www.asm.com/jp/careers/graduates/new-college-graduate-program       valid
Crawling page: http://www.asm.com/jp/careers/discover-asm/inside-asm       valid
Crawling page: http://www.asm.com/jp/careers/graduates       valid
Crawling page: http://www.asm.com/jp/careers/discover-asm/our-role-in-the-world       valid
Crawling page: http://www.asm.com/jp/careers/discover-asm/product-lines       valid
Crawling page: http://www.asm.com/jp/careers/discover-asm/culture       valid
Crawling page: http://www.asm.com/jp/careers/discover-asm/innovation-driven       valid
Crawling page: http://www.asm.com/jp/careers/discover-asm/diversity       valid
Crawling page: http://www.asm.com/jp/careers       valid
Crawling page: http://www.asm.com/support/pre-owned-advantage-services       valid
Crawling page: http://www.asm.com/jp/careers/discover-asm       valid
Crawling page: http://www.asm.com/jp/careers/discover-asm/fields-of-interest       valid
Crawling page: http://www.asm.com/support/spare-parts-support       valid
Crawling page: http://www.asm.com/support/product-performance-upgrade       valid
Crawling page: http://www.asm.com/support/field-support-services       valid
Crawling page: http://www.asm.com/support/technical-service-and-spares-support/performax-spares-programs       valid
Crawling page: http://www.asm.com/support/technical-service-and-spares-support/performax-service-programs       valid
Crawling page: http://www.asm.com/support/technical-service-and-spares-support       valid
Crawling page: http://www.asm.com/support/customer-training       valid
Crawling page: http://www.asm.com/support       valid
Crawling page: http://www.asm.com/news-room/press-releases-alerts       valid
Crawling page: http://www.asm.com/news-room/events-calendar       valid
Crawling page: http://www.asm.com/news-room/contact-pr       valid
Crawling page: http://www.asm.com/news-room       valid
Crawling page: http://www.asm.com/technology/chip-making/chip-making-process       valid
Crawling page: http://www.asm.com/news-room/news-overview       valid
Crawling page: https://www.ceramtec.com/downloads/tag/agb/lang/en/       valid
Crawling page: https://www.ceramtec.com/press/       valid
Crawling page: https://www.ceramtec.com/legal-information/       valid
Crawling page: https://www.ceramtec.com/investor-relations/       valid
Crawling page: https://www.ceramtec.com/quality/       valid
Crawling page: https://www.ceramtec.com/products/by-alphabet/       valid
Crawling page: https://www.ceramtec.com/group/       valid
Crawling page: https://www.ceramtec.com/brochure/where-can-advanced-ceramics-be-found/       valid
Crawling page: https://www.ceramtec.com/manual/technical-ceramics/       valid
Crawling page: https://www.ceramtec.com/downloads/tag/ceramtec/       valid
Crawling page: https://www.ceramtec.com/events/       valid
Crawling page: https://en.wikipedia.org/wiki/CeramTec       Crawling page: https://www.ceramtec.com/feed/news/       Crawling page: https://astx.com/partnering/       valid
Crawling page: https://astx.com/our-people-and-culture/       valid
Crawling page: https://www.ceramtec.com/news/id/1964/joint-start-in-the-professional-world/       valid
Crawling page: https://astx.com/research-development/sustaining-innovation/       valid
Crawling page: https://www.ceramtec.com/compamed/       valid
Crawling page: https://www.ceramtec.com/news/id/1965/pink-ribbon-and-pink-gerberas-for-mindfulness/       valid
Crawling page: https://plus.google.com/browser-not-supported/?ref=/u/0/108274519983787163310/posts       Crawling page: https://twitter.com/ceramtec       Crawling page: https://www.ceramtec.com/news/id/1967/richard-boulter-is-the-new-president-industrial/       valid
Crawling page: https://www.youtube.com/user/ceramtecceramics       Crawling page: https://astx.com/research-development/partnered-products-and-programs/kisqali-ribociclib-cdk46-inhibitor-oncology/       valid
Crawling page: https://astx.com/research-development/partnered-products-and-programs/azd3293-lanabecestat-beta-secretase-bace-inhibitor-alzheimers-disease/       valid
Crawling page: https://astx.com/research-development/partnered-products-and-programs/jnj42756493-erdafitinib-fgfr-inhibitor-oncology/       valid
Crawling page: https://astx.com/research-development/partnered-products-and-programs/azd5363-pkbakt-inhibitor-oncology/       valid
Crawling page: https://astx.com/research-development/clinical-pipeline/guadecitabine-sgi-110-dnmt-inhibitor-relapsed-refractory-mds-or-cmml/       valid
Crawling page: https://astx.com/research-development/clinical-pipeline/guadecitabine-sgi-110-dnmt-inhibitor-treatment-naive-aml/       valid
Crawling page: https://astx.com/research-development/clinical-pipeline/       valid
Crawling page: https://astx.com/research-development/clinical-pipeline/guadecitabine-sgi-110-dnmt-inhibitor-relapsed-refractory-aml/       valid
Crawling page: https://astx.com/research-development/       valid
Crawling page: https://astx.com/portfolio-item/charles-lapree/       valid
Crawling page: https://astx.com/portfolio-item/joseph-iovino/       valid
Crawling page: https://astx.com/portfolio-item/shinichi-sato/       valid
Crawling page: https://astx.com/portfolio-item/samuel-jason/       valid
Crawling page: https://astx.com/portfolio-item/martin-buckland/       valid
Crawling page: https://astx.com/portfolio-item/harold-keer/       valid
Crawling page: https://siennabio.com/privacy/       valid
Crawling page: https://astx.com/portfolio-item/nipun-davar/       valid
Crawling page: https://astx.com/portfolio-item/mohammad-azab/       valid
Crawling page: https://siennabio.com/contact/       valid
Crawling page: https://tri-intl.com/services/compressed-airgas-testing/       valid
Crawling page: https://astx.com/portfolio-item/jeremy-carmichael/       valid
Crawling page: https://tri-intl.com/services/pipe-testing/       valid
Crawling page: https://tri-intl.com/services/real-estate-development/       valid
Crawling page: https://tri-intl.com/services/mold-bacteria-testing-analysis/       valid
Crawling page: https://tri-intl.com/services/methane-water-vapor-barrier-testing-certification/       valid
Crawling page: https://investors.siennabio.com/email-alerts       Crawling page: https://astx.com/portfolio-item/chris-murray/       valid
Crawling page: https://astx.com/portfolio-item/david-rees/       valid
Crawling page: https://astx.com/portfolio-item/john-lyons/       valid
Crawling page: https://tri-intl.com/services/independent-sampling-auditing-services/       valid
Crawling page: https://tri-intl.com/services/education-training/       valid
Crawling page: https://tri-intl.com/services/protective-clothing-testing-certification/       valid
Crawling page: https://tri-intl.com/services/design-support/       valid
Crawling page: https://tri-intl.com/services/erosion-control-materials-testing-analysis/       valid
Crawling page: https://tri-intl.com/services/geosynthetics-geotechnical-testing-services/       valid
Crawling page: https://tri-intl.com/services/nditoolbox/       valid
Crawling page: https://tri-intl.com/services/uas-reliability-testing/       valid
Crawling page: https://tri-intl.com/services/composite-materials-dev-evaluation/       valid
Crawling page: https://tri-intl.com/services/non-destructive-testing/       valid
Crawling page: https://tri-intl.com/services/structural-health-monitoring/       valid
Crawling page: https://tri-intl.com/services/advanced-materials-development-testing//bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:32:16 [scrapy.extensions.logstats] INFO: Crawled 1064 pages (at 12 pages/min), scraped 798 items (at 7 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:33:25 [scrapy.extensions.logstats] INFO: Crawled 1068 pages (at 4 pages/min), scraped 806 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:34:35 [scrapy.extensions.logstats] INFO: Crawled 1080 pages (at 12 pages/min), scraped 814 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:35:30 [scrapy.extensions.logstats] INFO: Crawled 1084 pages (at 4 pages/min), scraped 822 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:36:30 [scrapy.extensions.logstats] INFO: Crawled 1096 pages (at 12 pages/min), scraped 834 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:37:15 [scrapy.extensions.logstats] INFO: Crawled 1104 pages (at 8 pages/min), scraped 842 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:38:17 [scrapy.extensions.logstats] INFO: Crawled 1116 pages (at 12 pages/min), scraped 854 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:39:14 [scrapy.extensions.logstats] INFO: Crawled 1123 pages (at 7 pages/min), scraped 865 items (at 11 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:40:24 [scrapy.extensions.logstats] INFO: Crawled 1142 pages (at 19 pages/min), scraped 878 items (at 13 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:40:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://investors.siennabio.com/events> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 06:40:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://investors.siennabio.com/corporate-governance> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 06:40:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://investors.siennabio.com/sec-filings> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 06:41:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://investors.siennabio.com/stock-information> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:41:18 [scrapy.extensions.logstats] INFO: Crawled 1164 pages (at 22 pages/min), scraped 885 items (at 7 items/min)
/bin/sh: 1: kill: No such process

2018-11-12 06:41:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://investors.siennabio.com/> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 06:41:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://investors.siennabio.com/press-releases?field_nir_news_date_value%5Bmin%5D=2017> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

       valid
Crawling page: https://tri-intl.com/project/marine-grease/       valid
Crawling page: https://tri-intl.com/tag-portfolio/products/       valid
Crawling page: https://tri-intl.com/services/       valid
Crawling page: https://tri-intl.com/project/seal-plus/       valid
Crawling page: https://tri-intl.com/project/bond-coat/       valid
Crawling page: https://tri-intl.com/portfolio/patents/       valid
Crawling page: https://tri-intl.com/portfolio/products/       valid
Crawling page: https://tri-intl.com/our-companies/tri-real-estate/       valid
Crawling page: https://tri-intl.com/our-companies/tri-minerva/       valid
Crawling page: https://tri-intl.com/our-companies/tri-solutions/       valid
Crawling page: https://tri-intl.com/our-companies/tri-china/       valid
Crawling page: https://tri-intl.com/our-companies/tri-southamerica/       valid
Crawling page: https://tri-intl.com/our-companies/tri-air-testing/       valid
Crawling page: https://tri-intl.com/our-companies/tri-environmental/       valid
Crawling page: https://tri-intl.com/our-companies/tri-applied-technologies/       valid
Crawling page: https://tri-intl.com/our-companies/tri-australasia/       valid
Crawling page: https://tri-intl.com/community-outreach/       valid
Crawling page: https://tri-intl.com/our-companies/       valid
Crawling page: https://tri-intl.com/our-companies/tri-austin/       valid
Crawling page: https://tri-intl.com/careers/       valid
Crawling page: http://www.asm.com/technology/key-technologies/plasma-enhanced-atomic-layer-deposition       valid
Crawling page: http://www.asm.com/technology/chip-making       valid
Crawling page: http://www.asm.com/technology/innovations/future-innovations       valid
Crawling page: http://www.asm.com/technology/innovations       valid
Crawling page: https://tri-intl.com/our-companies/leadership/       valid
Crawling page: https://tri-intl.com/about-tri/       valid
Crawling page: https://tri-intl.com/about-tri/history/       valid
Crawling page: https://tri-intl.com/about-tri/global-presence/       valid
Crawling page: http://www.asm.com/technology/key-technologies/lpcvd-diffusion-and-oxidation       valid
Crawling page: http://www.asm.com/technology/key-technologies/plasma-enhanced-chemical-vapor-deposition       valid
Crawling page: http://www.asm.com/technology/key-technologies/epitaxy       valid
Crawling page: http://www.asm.com/technology/key-technologies/atomic-layer-deposition       valid
Crawling page: http://www.asm.com/solutions/trademark-list       valid
Crawling page: http://www.asm.com/technology       valid
Crawling page: http://www.asm.com/technology/key-technologies/all-about-ald       valid
Crawling page: http://www.asm.com/technology/key-technologies       valid
Crawling page: http://www.asm.com/solutions/products/epitaxy-products       valid
Crawling page: http://www.asm.com/solutions/products/plasma-enhanced-ald       valid
Crawling page: http://www.asm.com/solutions/products/plasma-enhanced-chemical-vapor-deposition       valid
Crawling page: http://www.asm.com/solutions/patents       valid
Crawling page: http://www.asm.com/solutions       valid
Crawling page: http://www.asm.com/solutions/products/atomic-layer-deposition-products       valid
Crawling page: http://www.asm.com/solutions/products       valid
Crawling page: http://www.asm.com/solutions/products/low-pressure-chemical-vapor-deposition-and-diffusion-products       valid
Crawling page: http://www.asm.com/about/supplier-management       valid
Crawling page: http://www.asm.com/about/research-and-our-partners       valid
Crawling page: http://www.asm.com/about/corporate-calendar       valid
Crawling page: http://www.asm.com/about/locations       valid
Crawling page: http://www.asm.com/about/organization       valid
Crawling page: http://www.asm.com/about/organization/management-board       valid
Crawling page: http://www.asm.com/about/corporate-responsibility/supply-chain       valid
Crawling page: http://www.asm.com/about/organization/supervisory-board       valid
Crawling page: http://www.asm.com/about/corporate-responsibility/ethics       valid
Crawling page: http://www.asm.com/about/corporate-responsibility/labor       valid
Crawling page: http://www.asm.com/about/corporate-responsibility/environmental       valid
Crawling page: http://www.asm.com/about/corporate-responsibility/health-and-safety       valid
Crawling page: http://www.asm.com/about/corporate-responsibility       valid
Crawling page: http://www.asm.com/about/strategy-and-focus-areas/markets-and-products       valid
Crawling page: http://www.asm.com/about/corporate-responsibility/executive-commitment-statement       valid
Crawling page: http://www.asm.com/about/strategy-and-focus-areas/mission-and-strategy       valid
Crawling page: http://www.asm.com/about/strategy-and-focus-areas/focus-areas       valid
Crawling page: http://www.asm.com/about/strategy-and-focus-areas       valid
Crawling page: http://www.asm.com/about/strategy-and-focus-areas/breakthrough-technology       valid
Crawling page: http://www.asm.com/about/our-founder       valid
Crawling page: http://www.asm.com/about/layering-the-future       valid
Crawling page: http://www.asm.com/about/our-story       valid
Crawling page: http://www.asm.com/about/company-profile       valid
Crawling page: http://www.asm.com/about/history       valid
Crawling page: http://www.asm.com/about       valid
Crawling page: http://www.asm.com/home       valid
Crawling page: http://www.asm.com/about/contact-us       valid
Crawling page: https://www.ceramtec.com/ceramaseal/       valid
Crawling page: https://www.ceramtec.com/cyrol/       valid
Crawling page: https://www.ceramtec.com/rf-ceramic-capacitors/       valid
Crawling page: https://www.ceramtec.com/applications/piezo-applications/actuator-technology/       valid
Crawling page: https://www.ceramtec.com/perlucor/       valid
Crawling page: https://www.ceramtec.com/markets/equipment-mechanical-engineering/       valid
Crawling page: https://www.ceramtec.com/biolox/       valid
Crawling page: https://www.ceramtec.com/markets/energy-environment/       valid
Crawling page: https://www.ceramtec.com/markets/electronics/       valid
Crawling page: https://www.ceramtec.com/markets/automotive/       valid
Crawling page: https://www.ceramtec.com/markets/medical-technology/       valid
Crawling page: https://www.ceramtec.com/products/       valid
Crawling page: https://www.ceramtec.com/ceramic-materials/       valid
Crawling page: https://www.ceramtec.com/news-events/       valid
Crawling page: https://www.ceramtec.com/careers/       valid
Crawling page: https://www.ceramtec.com/markets/       valid
Crawling page: https://www.ceramtec.com/contact/       valid
Crawling page: https://investors.siennabio.com/events       Crawling page: https://investors.siennabio.com/corporate-governance       Crawling page: https://investors.siennabio.com/sec-filings       Crawling page: https://investors.siennabio.com/stock-information       Crawling page: https://www.ceramtec.com/location/poland/       valid
Crawling page: https://www.ceramtec.com/about-us/       valid
Crawling page: https://www.ceramtec.com/location/mexico/       valid
Crawling page: https://www.ceramtec.com/location/wilhermsdorf/       valid
Crawling page: https://investors.siennabio.com/       Crawling page: https://investors.siennabio.com/press-releases?field_nir_news_date_value%5Bmin%5D=2017       Crawling page: https://siennabio.com/company/corporate-advisors/       valid
Crawling page: https://siennabio.com/technology/       valid
Crawling page: https://siennabio.com/pipeline/       valid
Crawling page: https://www.kinestral.com/news/       valid
Crawling page: https://www.kinestral.com/careers/       valid
Crawling page: https://siennabio.com/pipeline/ongoing-clinical-trials/       valid
Crawling page: https://www.ceramtec.com/       valid
Crawling page: https://siennabio.com/company/board/       valid
Crawling page: https://siennabio.com/company/management/       valid
Crawling page: https://siennabio.com/company/overview/       valid
Crawling page: https://siennabio.com/       valid
Crawling page: https://www.kinestral.com/about//bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:42:53 [scrapy.extensions.logstats] INFO: Crawled 1165 pages (at 1 pages/min), scraped 900 items (at 15 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:43:19 [scrapy.extensions.logstats] INFO: Crawled 1177 pages (at 12 pages/min), scraped 905 items (at 5 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:44:22 [scrapy.extensions.logstats] INFO: Crawled 1185 pages (at 8 pages/min), scraped 917 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:45:30 [scrapy.extensions.logstats] INFO: Crawled 1197 pages (at 12 pages/min), scraped 929 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:46:35 [scrapy.extensions.logstats] INFO: Crawled 1207 pages (at 10 pages/min), scraped 940 items (at 11 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:47:41 [scrapy.extensions.logstats] INFO: Crawled 1233 pages (at 26 pages/min), scraped 955 items (at 15 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:48:24 [scrapy.extensions.logstats] INFO: Crawled 1233 pages (at 0 pages/min), scraped 963 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:49:13 [scrapy.extensions.logstats] INFO: Crawled 1241 pages (at 8 pages/min), scraped 973 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:50:17 [scrapy.extensions.logstats] INFO: Crawled 1253 pages (at 12 pages/min), scraped 985 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:51:05 [scrapy.core.scraper] ERROR: Spider error processing <GET http://ewa.com/products.php> (referer: http://ewa.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 54, in replace
    return Response.replace(self, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 31, in __init__
    super(TextResponse, self).__init__(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/text.py", line 48, in _set_body
    self._body = body.encode(self._encoding)
  File "/usr/lib/python3.6/encodings/cp1252.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode character '\u2011' in position 34614: character maps to <undefined>
/bin/sh: 1: kill: No such process

       valid
Crawling page: https://www.kinestral.com/technology/       valid
Crawling page: https://tri-intl.com       valid
Crawling page: http://www.asm.com/       valid
Crawling page: https://www.harris.com/press-releases/2018/06/harris-corporation-delivers-advanced-environmental-monitoring-sensor-for       valid
Crawling page: https://www.harris.com/impact/2018/06/quality-starts-with-the-customer       valid
Crawling page: https://www.harris.com/press-releases/2018/07/harris-corporation-to-provide-astronaut-audio-system-for-nasas-first-human       valid
Crawling page: https://www.harris.com/press-releases/2018/07/harris-corporation-awarded-15-billion-in-idiq-contracts-to-enhance-ngas       valid
Crawling page: https://www.harris.com/press-releases/2018/08/harris-corporation-provides-grants-to-support-stem-training       valid
Crawling page: https://www.harris.com/press-releases/2018/08/harris-corporation-increases-quarterly-dividend-20-percent-to-685-cents-per       valid
Crawling page: https://www.harris.com/press-releases/2018/08/harris-corporation-awarded-400-million-increase-to-electronic-warfare-idiq       valid
Crawling page: https://www.harris.com/press-releases/2018/09/harris-corporation-receives-orders-for-1540-two-channel-handheld-radios-from       valid
Crawling page: https://www.harris.com/press-releases/2018/09/lockheed-martin-selects-harris-corporation-to-deliver-f-35s-next-generation       valid
Crawling page: https://www.harris.com/impact/2018/10/harris-system-withstands-an-ef3-tornado-in-marshalltown-ia       valid
Crawling page: https://www.harris.com/press-releases/2018/10/harris-corporation-reports-strong-fiscal-2019-first-quarter-results-with       valid
Crawling page: https://www.harris.com/content/harris-corporation-and-l3-technologies-to-combine-in-merger-of-equals-to-create-a-global       valid
Crawling page: https://www.harris.com/impact/2018/10/florida-chief-of-public-safety-praises-pspc-team-in-personal-memo-to-harris-ceo       valid
Crawling page: https://www.harris.com/perspectives/innovation/phil-farmer-a-great-leader-a-great-legacy       valid
Crawling page: https://www.harris.com/suppliers       valid
Crawling page: https://www.harris.com/investors       valid
Crawling page: https://www.harris.com/impact/2018/07/assuring-residents-that-those-helping-them-are-well-connected       valid
Crawling page: https://www.harris.com/employees       valid
Crawling page: https://www.harris.com/customer-portals-and-service-contacts       valid
Crawling page: https://www.harris.com/about/contact-us       valid
Crawling page: https://www.harris.com/careers/employee-resource-groups       valid
Crawling page: https://www.harris.com/content/women-at-harris       valid
Crawling page: https://www.harris.com/careers/military-and-veterans       valid
Crawling page: https://www.harris.com/careers/recruiting-events-connect-with-harris       valid
Crawling page: https://www.harris.com/careers       valid
Crawling page: https://www.harris.com/careers/why-harris       valid
Crawling page: https://www.harris.com/careers/benefits       valid
Crawling page: https://www.harris.com/idiq       valid
Crawling page: https://www.harris.com/content/code-of-conduct       valid
Crawling page: https://www.harris.com/about/corporate-governance       valid
Crawling page: https://www.harris.com/about/management-team       valid
Crawling page: https://www.harris.com/about/board-of-directors       valid
Crawling page: https://www.harris.com/corporate-responsibility       valid
Crawling page: http://www.appnano.com/form/freeproberequest       valid
Crawling page: http://www.appnano.com/job-opportunities       valid
Crawling page: http://www.appnano.com/contact-us       valid
Crawling page: https://www.harris.com/timeline/the-history-of-harris-corporation       valid
Crawling page: https://www.harris.com/about/our-mission-and-values       valid
Crawling page: http://www.appnano.com/distributors       valid
Crawling page: http://www.appnano.com/awards       valid
Crawling page: http://www.appnano.com/terms-conditions       valid
Crawling page: http://www.appnano.com/our-mission       valid
Crawling page: https://www.harris.com/about       valid
Crawling page: http://www.appnano.com/support       valid
Crawling page: http://www.appnano.com/characterization-and-metrology       valid
Crawling page: http://www.appnano.com/custom-probe-mounting       valid
Crawling page: https://www.harris.com/events/upcoming       valid
Crawling page: http://www.appnano.com/services       valid
Crawling page: http://www.appnano.com/appnano-faq       valid
Crawling page: https://www.harris.com/perspectives       valid
Crawling page: http://www.appnano.com/product/category/vertisensetm-sthm       valid
Crawling page: https://www.harris.com/press-releases       valid
Crawling page: http://www.appnano.com/stm-tip-etcher       valid
Crawling page: http://www.appnano.com/product/category/membranes       valid
Crawling page: http://www.appnano.com/product/category/nanopores       valid
Crawling page: http://www.appnano.com/custom-mems-nanofabrication       valid
Crawling page: https://www.harris.com/media-resources       valid
Crawling page: https://www.harris.com/in-the-news       valid
Crawling page: https://www.harris.com/events/past       valid
Crawling page: https://www.harris.com/news       valid
Crawling page: http://www.appnano.com/product/category/standards       valid
Crawling page: http://www.appnano.com/product/category/spmprobes       valid
Crawling page: http://www.appnano.com/product/category/product-category       valid
Crawling page: http://www.appnano.com/home       valid
Crawling page: https://www.harris.com/locations       valid
Crawling page: https://www.harris.com/global       valid
Crawling page: https://www.harris.com/impact       valid
Crawling page: https://www.harris.com/what-we-do/universe-exploration       valid
Crawling page: https://www.harris.com/what-we-do/undersea-systems       valid
Crawling page: https://www.harris.com/what-we-do/tactical-communications       valid
Crawling page: https://www.harris.com/what-we-do/strategic-tactical-and-maritime-satcom       valid
Crawling page: https://www.harris.com/what-we-do/spaceport-operations       valid
Crawling page: https://www.harris.com/what-we-do/space-antennas       valid
Crawling page: https://www.harris.com/what-we-do/robotics       valid
Crawling page: https://www.harris.com/what-we-do/space-payloads-and-electronics       valid
Crawling page: https://www.harris.com/what-we-do/space-superiority       valid
Crawling page: https://www.harris.com/what-we-do/public-safety-and-professional-communications       valid
Crawling page: https://www.harris.com/what-we-do/positioning-navigation-and-timing       valid
Crawling page: https://www.harris.com/what-we-do/piezoelectric-ceramics       valid
Crawling page: https://www.harris.com/what-we-do/radar       valid
Crawling page: https://www.harris.com/what-we-do/integrated-battlefield-management       valid
Crawling page: https://www.harris.com/what-we-do/intelligence-surveillance-and-reconnaissance       valid
Crawling page: https://www.harris.com/what-we-do/geospatial-solutions       valid
Crawling page: https://www.harris.com/what-we-do/night-vision       valid
Crawling page: https://www.harris.com/what-we-do/broadband-communications       valid
Crawling page: https://www.harris.com/what-we-do/energy-solutions       valid
Crawling page: https://www.harris.com/what-we-do/environmental-solutions       valid
Crawling page: https://www.harris.com/what-we-do/electronic-warfare       valid
Crawling page: https://www.harris.com/what-we-do/analytical-instrumentation       valid
Crawling page: https://www.harris.com/what-we-do/avionics       valid
Crawling page: http://ewa.com/contracts.php       valid
Crawling page: http://inventusholdings.com/index.shtml       valid
Crawling page: https://www.harris.com/what-we-do/antenna-products       valid
Crawling page: https://www.harris.com/what-we-do/air-traffic-management       valid
Crawling page: http://ewa.com/products.php       Crawling page: https://www.harris.com/       valid
Crawling page: http://ewa.com/organization.php/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:51:31 [scrapy.extensions.logstats] INFO: Crawled 1268 pages (at 15 pages/min), scraped 1000 items (at 15 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:52:25 [scrapy.extensions.logstats] INFO: Crawled 1281 pages (at 13 pages/min), scraped 1008 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:53:27 [scrapy.extensions.logstats] INFO: Crawled 1287 pages (at 6 pages/min), scraped 1016 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:54:19 [scrapy.extensions.logstats] INFO: Crawled 1287 pages (at 0 pages/min), scraped 1022 items (at 6 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:55:25 [scrapy.extensions.logstats] INFO: Crawled 1303 pages (at 16 pages/min), scraped 1033 items (at 11 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:55:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 http://www.plex.tv/%7B%7Badminurl>: HTTP status code is not handled or not allowed
/bin/sh: 1: kill: No such process

2018-11-12 06:55:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 http://www.plex.tv/%7B%7Bapiurl%7D%7D/admin/users/unbecome>: HTTP status code is not handled or not allowed
2018-11-12 06:55:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 http://www.plex.tv/%7B%7Bapiurl%7D%7D/users/edit>: HTTP status code is not handled or not allowed
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:56:17 [scrapy.extensions.logstats] INFO: Crawled 1321 pages (at 18 pages/min), scraped 1041 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:56:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.kinestral.com/> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:57:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.kinestral.com/?lang=ja> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 06:57:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.kinestral.com/?lang=zh-hant> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:58:01 [scrapy.extensions.logstats] INFO: Crawled 1321 pages (at 0 pages/min), scraped 1049 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:58:23 [scrapy.extensions.logstats] INFO: Crawled 1326 pages (at 5 pages/min), scraped 1052 items (at 3 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 06:59:04 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-12 06:59:05 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.plex.tv/about/charity/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
/bin/sh: 1: kill: No such process

2018-11-12 06:59:19 [scrapy.extensions.logstats] INFO: Crawled 1329 pages (at 3 pages/min), scraped 1056 items (at 4 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:00:01 [scrapy.core.scraper] ERROR: Error downloading <GET https://tri-intl.com/cqa-training-week-australia/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
/bin/sh: 1: kill: No such process

2018-11-12 07:00:16 [scrapy.extensions.logstats] INFO: Crawled 1335 pages (at 6 pages/min), scraped 1060 items (at 4 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:01:12 [scrapy.extensions.logstats] INFO: Crawled 1337 pages (at 2 pages/min), scraped 1065 items (at 5 items/min)
/bin/sh: 1: kill: No such process

2018-11-12 07:01:21 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.ceramtec.com/news/id/1963/eric-oellerer-is-the-new-cfo-at-ceramtec/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 07:01:21 [scrapy.core.scraper] ERROR: Error downloading <GET https://tri-intl.com/services/accelerated-life-testing/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 07:01:21 [scrapy.core.scraper] ERROR: Error downloading <GET https://tri-intl.com/services/liner-integrity-surveyleak-detection/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:02:22 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.plex.tv/%7B%7BwebURL%7D%7D>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 07:02:22 [scrapy.extensions.logstats] INFO: Crawled 1342 pages (at 5 pages/min), scraped 1071 items (at 6 items/min)
2018-11-12 07:02:22 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.ceramtec.com/ceramcool/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 07:02:22 [scrapy.core.scraper] ERROR: Error downloading <GET https://astx.com/research-development/clinical-pipeline/onalespib-at13387-hsp90-inhibitor-solid-tumors/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 07:02:22 [scrapy.core.scraper] ERROR: Error downloading <GET https://astx.com/research-development/clinical-pipeline/guadecitabine-sgi-110-dnmt-inhibitor-solid-tumors/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 07:02:22 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.ceramtec.com/aloslide/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 07:02:22 [scrapy.core.scraper] ERROR: Error downloading <GET https://astx.com/media-center/conferences-and-meetings/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 07:02:22 [scrapy.core.scraper] ERROR: Error downloading <GET https://astx.com/portfolio-item/nicola-wallis/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 07:02:22 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.ceramtec.com/privacy-policy/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 07:02:22 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.ceramtec.com/machining/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
/bin/sh: 1: kill: No such process

2018-11-12 07:02:29 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.plex.tv/your-media/movies-tv/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 07:02:29 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.plex.tv/live-tv-dvr/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 07:02:29 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.plex.tv/your-media/photos-videos/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 07:02:29 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.plex.tv/your-media/music/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
/bin/sh: 1: kill: No such process

2018-11-12 07:02:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <408 https://astx.com/portfolio-item/neil-jones/>: HTTP status code is not handled or not allowed
2018-11-12 07:02:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <408 https://astx.com/portfolio-item/lee-dawson/>: HTTP status code is not handled or not allowed
2018-11-12 07:02:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <408 https://astx.com/portfolio-item/tom-heightman/>: HTTP status code is not handled or not allowed
2018-11-12 07:02:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <408 https://astx.com/terms-of-use/>: HTTP status code is not handled or not allowed
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:03:26 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <408 https://astx.com/legal/>: HTTP status code is not handled or not allowed
2018-11-12 07:03:26 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <408 https://astx.com/media-center/>: HTTP status code is not handled or not allowed
2018-11-12 07:03:26 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <408 https://astx.com/terms-of-use>: HTTP status code is not handled or not allowed
2018-11-12 07:03:26 [scrapy.extensions.logstats] INFO: Crawled 1376 pages (at 34 pages/min), scraped 1082 items (at 11 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:04:19 [scrapy.extensions.logstats] INFO: Crawled 1385 pages (at 9 pages/min), scraped 1091 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

       valid
Crawling page: http://ewa.com/locations.php       valid
Crawling page: https://www.harris.com/mission-scenario       valid
Crawling page: https://www.harris.com/solutions       valid
Crawling page: http://ewa.com/site-map.php       valid
Crawling page: https://www.harris.com/search/site       valid
Crawling page: http://ewa.com/index.php       valid
Crawling page: https://www.plex.tv/your-media/virtual-reality/       valid
Crawling page: https://www.plex.tv/online-content/news/       valid
Crawling page: https://www.plex.tv/your-media/       valid
Crawling page: https://www.plex.tv/online-content/web-shows/       valid
Crawling page: http://blog.ethox.com/looking-for-a-custom-fatty-amine-ethoxylate       valid
Crawling page: https://www.plex.tv/online-content/       valid
Crawling page: http://blog.ethox.com/why-choosing-ethox-will-help-your-bottom-line       valid
Crawling page: https://www.plex.tv/online-content/podcasts/       valid
Crawling page: http://blog.ethox.com/5-reasons-to-choose-toll/contract-manufacturing       valid
Crawling page: https://www.plex.tv/sign-in/?forward=https://www.plex.tv/       valid
Crawling page: http://blog.ethox.com/customized-blend-emulsions-does-not-have-to-be-difficult       valid
Crawling page: http://blog.ethox.com/how-to-reduce-your-heap-leach-time-and-increase-productivity       valid
Crawling page: http://blog.ethox.com/looking-for-a-problem-solver       valid
Crawling page: http://blog.ethox.com/get-better-results-with-e-sperse-pigment-dispersants       valid
Crawling page: https://www.plex.tv/blog/       valid
Crawling page: http://blog.ethox.com/ethox-chemicals-celebrating-50-years       valid
Crawling page: http://blog.ethox.com/what-will-specialty-and-agro-chemicals-be-like-in-the-next-50-years       valid
Crawling page: http://blog.ethox.com/       valid
Crawling page: https://www.plex.tv/plex-pass/       valid
Crawling page: https://www.plex.tv/blog/web-shows-amirite/       valid
Crawling page: http://www.appnano.com/       valid
Crawling page: http://www.appnano.com/about-us       valid
Crawling page: https://www.plex.tv/blog/podcasts-now-with-less-beta/       valid
Crawling page: http://www.appnano.com/form/helpchoosingrightprobe       valid
Crawling page: https://www.plex.tv/media-server-downloads/       valid
Crawling page: http://www.appnano.com/product/category/special-coated-probes       valid
Crawling page: https://www.plex.tv/blog/subtitles-and-sunsets-big-improvements-little-housekeeping/       valid
Crawling page: http://www.appnano.com/product/category/silicon-nitride-probes       valid
Crawling page: http://www.appnano.com/product/category/access-series       valid
Crawling page: http://www.appnano.com/product/category/quantitative-force-spectroscopy-probes       valid
Crawling page: https://www.plex.tv/apps-devices/       valid
Crawling page: https://www.plex.tv/about/careers/       valid
Crawling page: http://www.appnano.com/form/contact       valid
Crawling page: https://www.plex.tv/about/       valid
Crawling page: https://www.ceramtec.com/ceramic-materials/aluminum-nitride/       valid
Crawling page: https://www.ceramtec.com/alotec/       valid
Crawling page: https://www.ceramtec.com/competence/piezo-ceramics/       valid
Crawling page: https://www.ceramtec.com/rocar/       valid
Crawling page: https://www.ceramtec.com/triduon/       valid
Crawling page: https://astx.com/discover-astex/       valid
Crawling page: https://www.kinestral.com/       Crawling page: https://www.kinestral.com/contact/       valid
Crawling page: https://tri-intl.com/services/dashboards-reports/       valid
Crawling page: https://tri-intl.com/services/applied-reliability-engineering/       valid
Crawling page: https://astx.com/discover-astex/our-mission-and-values/       valid
Crawling page: https://www.kinestral.com/?lang=ja       Crawling page: https://www.kinestral.com/?lang=zh-hant       Crawling page: https://astx.com/discover-astex/our-leadership-team/       valid
Crawling page: https://astx.com/portfolio-item/harren-jhoti/       valid
Crawling page: https://tri-intl.com/services/erp-solutions/       valid
Crawling page: https://www.ceramtec.com/news/       valid
Crawling page: https://astx.com/discover-astex/corporate-social-responsibility/       valid
Crawling page: https://astx.com/discover-astex/our-history/       valid
Crawling page: https://astx.com/portfolio-item/nancy-worrell/       valid
Crawling page: https://astx.com/portfolio-item/yong-hao/       valid
Crawling page: https://astx.com/research-development/partnered-products-and-programs/multiple-targets-and-therapeutic-areas/       valid
Crawling page: https://astx.com/research-development/pyramid-discovery-platform/       valid
Crawling page: https://astx.com/research-development/oncology-and-cns-discovery/       valid
Crawling page: https://astx.com/media-center/press-releases-archive/       valid
Crawling page: https://astx.com/media-center/presentations-and-publications/       valid
Crawling page: https://astx.com/media-center/astex-in-the-news/       valid
Crawling page: https://astx.com/contact/       valid
Crawling page: https://tri-intl.com/cookies-policy/       valid
Crawling page: https://tri-intl.com/privacy-policy/       valid
Crawling page: https://astx.com/privacy-policy/       valid
Crawling page: https://tri-intl.com/disclaimer/       valid
Crawling page: https://www.kinestral.com/contact/       valid
Crawling page: https://astx.com/our-people-and-culture/career-opportunities/       valid
Crawling page: https://astx.com/media-center/press-releases/       valid
Crawling page: https://astx.com/research-development/clinical-pipeline/astx727-oral-dnmt-inhibitor-hematologic-malignancies/       valid
Crawling page: https://astx.com/research-development/clinical-pipeline/astx660-dual-iap-antagonist-solid-tumors-lymphomas/       valid
Crawling page: https://www.kinestral.com/       valid
Crawling page: https://www.plex.tv/sign-up/       valid
Crawling page: http://toraytpa.com/       valid
Crawling page: http://toraytpa.com/Matte-Film       valid
Crawling page: http://toraytpa.com/White-Film       valid
Crawling page: http://toraytpa.com/Thermal-blanket-base-film       valid
Crawling page: https://www.fujifilmusa.com/index.html       valid
Crawling page: http://www.adfors.com/us/       valid
Crawling page: https://carboceramics.com/Environmental/Oil-and-gas-applications-overview/SITEGUARD-seamless-secondary-containment-system/Surface-mounted-containment       valid
Crawling page: https://acell.com/       valid
Crawling page: https://acell.com/patents/       valid
Crawling page: https://www.relypsa.com/what-we-do/our-perspective/hyperkalemia-from-the-inside-out/       valid
Crawling page: https://www.relypsa.com/our-medicine/veltassa/       valid
Crawling page: https://www.relypsa.com/       valid
Crawling page: https://www.zygo.com/       valid
Crawling page: https://www.relypsa.com/terms-of-use/       valid
Crawling page: https://carboceramics.com/news-and-resources/resources/animations-and-videos       valid
Crawling page: https://carboceramics.com/news-and-resources/resources       valid
Crawling page: https://www.relypsa.com/privacy-policy/       valid
Crawling page: https://www.relypsa.com/contact/directions/       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/high-conductivity-sand-proppant       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/proppant-delivered-flow-enhancement-(1)       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/proppant-delivered-flow-enhancement-(1)/RPM-relative-permeability-modification-technology       valid
Crawling page: https://carboceramics.com/news-and-resources/events,-conferences-and-classes/Previous-Events       valid
Crawling page: http://www.kolonindustries.com/Eng//Customer/KeywordSearch.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//MalgumBoard/index.asp?BoardCode=noticeeng&BoardMode=List       valid
Crawling page: https://carboceramics.com/Store/ASSETGUARD/TANKGUARD       valid
Crawling page: https://carboceramics.com/Store/ASSETGUARD/SHOPGUARD       valid/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:05:31 [scrapy.extensions.logstats] INFO: Crawled 1400 pages (at 15 pages/min), scraped 1101 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:06:25 [scrapy.extensions.logstats] INFO: Crawled 1409 pages (at 9 pages/min), scraped 1107 items (at 6 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:07:38 [scrapy.extensions.logstats] INFO: Crawled 1414 pages (at 5 pages/min), scraped 1116 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:08:33 [scrapy.extensions.logstats] INFO: Crawled 1416 pages (at 2 pages/min), scraped 1123 items (at 7 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:09:12 [scrapy.extensions.logstats] INFO: Crawled 1423 pages (at 7 pages/min), scraped 1128 items (at 5 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:10:25 [scrapy.extensions.logstats] INFO: Crawled 1427 pages (at 4 pages/min), scraped 1136 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:11:26 [scrapy.extensions.logstats] INFO: Crawled 1435 pages (at 8 pages/min), scraped 1143 items (at 7 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:12:35 [scrapy.extensions.logstats] INFO: Crawled 1443 pages (at 8 pages/min), scraped 1151 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:13:42 [scrapy.extensions.logstats] INFO: Crawled 1449 pages (at 6 pages/min), scraped 1159 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:14:32 [scrapy.extensions.logstats] INFO: Crawled 1458 pages (at 9 pages/min), scraped 1166 items (at 7 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:15:22 [scrapy.extensions.logstats] INFO: Crawled 1464 pages (at 6 pages/min), scraped 1173 items (at 7 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:16:23 [scrapy.extensions.logstats] INFO: Crawled 1476 pages (at 12 pages/min), scraped 1183 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process


Crawling page: https://carboceramics.com/Store/ASSETGUARD/GROUNDGUARD       valid
Crawling page: http://www.kolonindustries.com/Product/product05_01.asp       valid
Crawling page: http://www.kolonindustries.com       valid
Crawling page: http://www.kolonindustries.com/Eng//Service/service0303.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Company/company03_05.asp       valid
Crawling page: http://kolonindustries.com/Eng/MalgumBoard/Index.asp?BoardCode=irdataeng&BoardMode=View&Seq=78       valid
Crawling page: http://www.kolonindustries.com/Eng//Company/company03_04.asp       valid
Crawling page: https://carboceramics.com/special-pages/site-map       valid
Crawling page: http://www.kolonindustries.com/Eng//Company/company03_03.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Company/company03_02.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Customer/card_info.asp       valid
Crawling page: http://kolonindustries.com/Eng/MalgumBoard/Index.asp?BoardCode=noticeeng&BoardMode=View&Seq=4       valid
Crawling page: https://carboceramics.com/legal/Privacy       valid
Crawling page: http://www.kolonindustries.com/Eng//MalgumBoard/index.asp?BoardCode=irdataeng&BoardMode=List       valid
Crawling page: http://kolonindustries.com/Eng/MalgumBoard/Index.asp?BoardCode=irdataeng&BoardMode=View&Seq=80       valid
Crawling page: http://kolonindustries.com/Eng/MalgumBoard/Index.asp?BoardCode=irdataeng&BoardMode=View&Seq=81       valid
Crawling page: https://carboceramics.com/legal/Terms-and-conditions       valid
Crawling page: http://www.kolonindustries.com/Eng//Service/service02.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//RnD/rnd05_01.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//RnD/rnd04.asp       valid
Crawling page: http://kolonindustries.com/Eng/MalgumBoard/Index.asp?BoardCode=noticeeng&BoardMode=View&Seq=6       valid
Crawling page: http://kolonindustries.com/Eng/MalgumBoard/Index.asp?BoardCode=irdataeng&BoardMode=View&Seq=82       valid
Crawling page: http://www.kolonindustries.com/Eng//RnD/rnd03.asp       valid
Crawling page: http://kolonindustries.com/Eng/MalgumBoard/index.asp?BoardCode=onlineinquiryeng&BoardMode=Write       valid
Crawling page: http://www.kolonindustries.com/Eng//RnD/rnd02.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Infocenter/ci.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Service/service01_01.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//RnD/rnd01.asp       valid
Crawling page: http://kolonindustries.com/Eng/main.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product01.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product05_05.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product05_04.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product05_06.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product05_02.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product05_09.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product05_01.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product04_04.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product04_03.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product04_02.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product03_07.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product03_06.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product03_05.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product04_01.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product03_04.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product03_01.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product03_03.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product03_02.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product02_07.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product02_08.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product02_06.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product02_05.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product02_04.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product02_03.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product02_02.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Product/product02_01.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Company/company04_01.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Company/company03_01.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Company/company02_01.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Company/company01_01.asp       valid
Crawling page: http://www.kolonindustries.com/Eng/Customer/KeywordSearch.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Etc/Sitemap.asp       valid
Crawling page: http://www.kolonindustries.com/Eng//Customer/ContactUs.asp       valid
Crawling page: http://www.kolonindustries.com/Eng/main.asp       valid
Crawling page: https://carboceramics.com/news-and-resources/resources/brochures/Fracture-Technologies-Brochure       valid
Crawling page: https://carboceramics.com/news-and-resources/resources/brochures/FRACPRO-leaflet       valid
Crawling page: https://carboceramics.com/getmedia/27c48c80-c0de-4cae-821d-380f21fa59bd/corporate-player.html.aspx       valid
Crawling page: https://carboceramics.com/contact/business/carbo-corporate       valid
Crawling page: https://carboceramics.com/news-and-resources/resources/brochures/FALCON-TECHNOLOGIES-brochure       valid
Crawling page: https://carboceramics.com/products-and-services/fracpro       valid
Crawling page: https://carboceramics.com/products-and-services/falcon-technologies       valid
Crawling page: https://carboceramics.com/products-and-services/stratagen       valid
Crawling page: http://kolonindustries.com/eng/main.asp       valid
Crawling page: https://carboceramics.com/Careers       valid
Crawling page: https://carboceramics.com/Store.aspx       valid
Crawling page: https://carboceramics.com/Store/My-Account       valid
Crawling page: https://carboceramics.com/Store/ASSETGUARD/EQUIPMENTGUARD       valid
Crawling page: https://carboceramics.com/Store/ASSETGUARD/WASTEGUARD       valid
Crawling page: https://carboceramics.com/Store/ASSETGUARD/MOBILEGUARD       valid
Crawling page: https://carboceramics.com/contact/training-registration       valid
Crawling page: https://carboceramics.com/Store       valid
Crawling page: https://carboceramics.com/Store/ASSETGUARD       valid
Crawling page: https://carboceramics.com/contact/subscribe       valid
Crawling page: https://carboceramics.com/Store/Special-Pages/Login       valid
Crawling page: https://carboceramics.com/contact/carbo-corporate/media-relations       valid
Crawling page: https://carboceramics.com/contact/carbo-corporate/investor-relations       valid
Crawling page: https://carboceramics.com/contact/carbo-corporate/assetguard       valid
Crawling page: https://carboceramics.com/contact/carbo-corporate/falcon-technologies       valid
Crawling page: https://carboceramics.com/contact/carbo-corporate/stratagen       valid
Crawling page: https://carboceramics.com/contact/carbo-corporate/carboaccucast-industrial-ceramics       valid
Crawling page: https://carboceramics.com/contact/carbo-corporate/carbo-ceramics-and-technologies       valid
Crawling page: https://carboceramics.com/contact/carbo-corporate/software-technical-support       valid
Crawling page: https://carboceramics.com/contact/carbo-corporate       valid
Crawling page: https://carboceramics.com/contact/carbo-corporate/fracpro/bin/sh: 1: kill: No such process

2018-11-12 07:17:16 [scrapy.extensions.logstats] INFO: Crawled 1484 pages (at 8 pages/min), scraped 1192 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:18:16 [scrapy.extensions.logstats] INFO: Crawled 1493 pages (at 9 pages/min), scraped 1201 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:19:18 [scrapy.extensions.logstats] INFO: Crawled 1501 pages (at 8 pages/min), scraped 1209 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:20:23 [scrapy.extensions.logstats] INFO: Crawled 1504 pages (at 3 pages/min), scraped 1216 items (at 7 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:21:21 [scrapy.extensions.logstats] INFO: Crawled 1514 pages (at 10 pages/min), scraped 1225 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:22:16 [scrapy.extensions.logstats] INFO: Crawled 1524 pages (at 10 pages/min), scraped 1233 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:23:20 [scrapy.extensions.logstats] INFO: Crawled 1533 pages (at 9 pages/min), scraped 1243 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:24:24 [scrapy.extensions.logstats] INFO: Crawled 1545 pages (at 12 pages/min), scraped 1253 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:25:16 [scrapy.extensions.logstats] INFO: Crawled 1549 pages (at 4 pages/min), scraped 1261 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

       valid
Crawling page: https://carboceramics.com/investors/webcasts-and-presentations       valid
Crawling page: https://carboceramics.com/investors/investor-news-and-information/investor-information-requests       valid
Crawling page: https://carboceramics.com/investors/investor-news-and-information/investor-faqs       valid
Crawling page: https://carboceramics.com/investors/investor-news-and-information/investor-events       valid
Crawling page: https://carboceramics.com/investors/investor-news-and-information/investor-news       valid
Crawling page: https://carboceramics.com/investors/financial-reports-and-information/Analysts       valid
Crawling page: https://carboceramics.com/investors/financial-reports-and-information/sec-filings       valid
Crawling page: https://carboceramics.com/investors/financial-reports-and-information/annual-reports       valid
Crawling page: https://carboceramics.com/investors/financial-reports-and-information/stock-information       valid
Crawling page: https://carboceramics.com/investors/governance-and-management       valid
Crawling page: https://carboceramics.com/investors/company-profile       valid
Crawling page: https://carboceramics.com/investors       valid
Crawling page: https://carboceramics.com/news-and-resources/resources/technical-presentations       valid
Crawling page: https://carboceramics.com/news-and-resources/resources/technical-papers       valid
Crawling page: https://carboceramics.com/news-and-resources/resources/technical-data-sheets       valid
Crawling page: https://carboceramics.com/news-and-resources/resources/Safety-data-sheets       valid
Crawling page: https://carboceramics.com/news-and-resources/resources/published-articles       valid
Crawling page: https://carboceramics.com/news-and-resources/resources/case-histories       valid
Crawling page: https://carboceramics.com/news-and-resources/resources/interactive-resources-map       valid
Crawling page: https://carboceramics.com/news-and-resources/resources/brochures       valid
Crawling page: https://carboceramics.com/news-and-resources/events,-conferences-and-classes/Upcoming-Events       valid
Crawling page: https://carboceramics.com/news-and-resources/news       valid
Crawling page: https://carboceramics.com/news-and-resources       valid
Crawling page: https://carboceramics.com/news-and-resources/events,-conferences-and-classes       valid
Crawling page: https://carboceramics.com/Environmental/Industrial-applications-overview/ASSETGUARD       valid
Crawling page: https://carboceramics.com/Environmental/Oil-and-gas-applications-overview/GROUNDGUARD       valid
Crawling page: https://carboceramics.com/Environmental/Industrial-applications-overview       valid
Crawling page: https://carboceramics.com/Environmental/Oil-and-gas-applications-overview/MOBILEGUARD-reusable-spill-containment       valid
Crawling page: https://carboceramics.com/Environmental/Oil-and-gas-applications-overview/Tank-bases       valid
Crawling page: https://carboceramics.com/Environmental/Oil-and-gas-applications-overview/Asset-protection       valid
Crawling page: https://carboceramics.com/Environmental/Oil-and-gas-applications-overview/SITEGUARD-seamless-secondary-containment-system       valid
Crawling page: https://carboceramics.com/Environmental/Oil-and-gas-applications-overview/Proprietary-liner-technology       valid
Crawling page: https://carboceramics.com/Environmental/Oil-and-gas-applications-overview/Risk-mitigation       valid
Crawling page: https://carboceramics.com/Environmental       valid
Crawling page: https://carboceramics.com/Environmental/Oil-and-gas-applications-overview/Environmental-stewardship       valid
Crawling page: https://carboceramics.com/Environmental/Oil-and-gas-applications-overview/falcon-technologies       valid
Crawling page: https://carboceramics.com/Environmental/Oil-and-gas-applications-overview       valid
Crawling page: https://carboceramics.com/Oil-gas/stratagen/Fracture-consulting-experience       valid
Crawling page: https://carboceramics.com/Oil-gas/stratagen/Field-consulting-services       valid
Crawling page: https://carboceramics.com/contact/business/falcon-technologies       valid
Crawling page: https://carboceramics.com/Oil-gas/stratagen/BASINWORX-multi-well-evaluation       valid
Crawling page: https://carboceramics.com/Oil-gas/stratagen/WELLWORX-single-well-evaluation       valid
Crawling page: https://carboceramics.com/Oil-gas/stratagen/FRACWORX-frac-design       valid
Crawling page: https://carboceramics.com/Oil-gas/stratagen/Models-and-workflows       valid
Crawling page: https://carboceramics.com/Oil-gas/stratagen/Reservoir-and-fracture-evaluation-services       valid
Crawling page: https://carboceramics.com/Oil-gas/stratagen/STRATAGEN-fracture-consulting-services       valid
Crawling page: https://carboceramics.com/Oil-gas/stratagen       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/Proppant-delivered-evaluation-service/CARBONRT-GP-Inert-tracer-technology-for-gravel-pac       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/Proppant-delivered-evaluation-service/CARBONRT-ULTRA-inert-tracer-technology       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/Proppant-delivered-evaluation-service/Fracture-evaluation       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/Proppant-delivered-evaluation-service/CARBONRT-non-radioactive-tracer-technology       valid
Crawling page: https://carboceramics.com/contact/business/stratagen       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/Proppant-delivered-evaluation-service       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/resin-coated-proppant/CARBOBOND-LITE-resin-coated-low-density-ceramic       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/GUARD-proppant-delivered-production-assu       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/GUARD-proppant-delivered-production-assu/scaleguard-proppant-delivered-scale-inhibiting-tec       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/resin-coated-proppant       valid
Crawling page: https://carboceramics.com/contact/business/carbo-ceramics-and-technologies       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/ceramic-proppant/carbolite-high-performance,-low-density       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/ceramic-proppant/carbohsp-high-strength       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/ceramic-proppant/carboprop-intermediate-density       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/ceramic-proppant/carbohydroprop-low-density,-high-transport       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/ceramic-proppant/carboeconoprop-low-density       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/ceramic-proppant/Low-density-ceramic-advantages       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/ceramic-proppant/Economic-conductivity       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/ceramic-proppant       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/Microproppant/NANOMITE-S-microproppant       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/Microproppant/NANOMITE-C-ceramic-microproppant       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/ceramic-proppant/Contact-conductivity       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/Ultra-conductive-proppant-technology/kryptosphere-hd-ultra-conductive,-high-density-cer       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/Microproppant       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/High-transport-proppant/carboair       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/Ultra-conductive-proppant-technology/kryptosphere-ld-ultra-conductive,-low-density-cer/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:26:16 [scrapy.extensions.logstats] INFO: Crawled 1561 pages (at 12 pages/min), scraped 1270 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:27:21 [scrapy.extensions.logstats] INFO: Crawled 1576 pages (at 15 pages/min), scraped 1281 items (at 11 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:28:23 [scrapy.extensions.logstats] INFO: Crawled 1582 pages (at 6 pages/min), scraped 1292 items (at 11 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:29:13 [scrapy.extensions.logstats] INFO: Crawled 1595 pages (at 13 pages/min), scraped 1300 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:30:14 [scrapy.extensions.logstats] INFO: Crawled 1601 pages (at 6 pages/min), scraped 1309 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:31:13 [scrapy.extensions.logstats] INFO: Crawled 1610 pages (at 9 pages/min), scraped 1318 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:32:52 [scrapy.extensions.logstats] INFO: Crawled 1623 pages (at 13 pages/min), scraped 1335 items (at 17 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:33:18 [scrapy.extensions.logstats] INFO: Crawled 1631 pages (at 8 pages/min), scraped 1339 items (at 4 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:34:36 [scrapy.extensions.logstats] INFO: Crawled 1640 pages (at 9 pages/min), scraped 1351 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:35:24 [scrapy.extensions.logstats] INFO: Crawled 1650 pages (at 10 pages/min), scraped 1358 items (at 7 items/min)
/bin/sh: 1: kill: No such process

       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/Fracture-technologies-overview       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/Ultra-conductive-proppant-technology       valid
Crawling page: https://carboceramics.com/Oil-gas/fracpro/Software-training       valid
Crawling page: https://carboceramics.com/Oil-gas/fracpro/Fracture-software-overview-(1)       valid
Crawling page: https://carboceramics.com/Oil-gas/fracpro/Software-support       valid
Crawling page: https://carboceramics.com/Oil-gas/fracpro       valid
Crawling page: https://carboceramics.com/Oil-gas/Production-enhancement-challenges/Fracturing-and-completion-efficiency       valid
Crawling page: https://carboceramics.com/Oil-gas/Production-enhancement-challenges/Fracture-conductivity-and-proppant-pack-integrity       valid
Crawling page: https://carboceramics.com/Oil-gas/Production-enhancement-challenges/Fracture-and-completion-evaluation       valid
Crawling page: https://carboceramics.com/Oil-gas/Production-enhancement-challenges/Offshore       valid
Crawling page: https://carboceramics.com/Oil-gas/Production-enhancement-challenges/Production-assurance       valid
Crawling page: https://carboceramics.com/Oil-gas/Production-enhancement-challenges/Unconventional-reservoirs       valid
Crawling page: https://carboceramics.com/Oil-gas/Production-enhancement-challenges/HPHT       valid
Crawling page: https://carboceramics.com/Fracture-technologies/Fracture-technologies-overview       valid
Crawling page: https://carboceramics.com/Oil-gas/Production-enhancement-challenges/Deepwater       valid
Crawling page: https://carboceramics.com/contact/business/fracpro       valid
Crawling page: http://nlight.net/contact/worldwide       valid
Crawling page: http://nlight.net/careers/benefits       valid
Crawling page: https://carboceramics.com/products-and-services/fracture-software/software-download,-training-and-support/downloads       valid
Crawling page: https://carboceramics.com/Oil-gas/Production-enhancement-challenges       valid
Crawling page: http://nlight.net/news/articles       valid
Crawling page: https://carboceramics.com/Oil-gas/product-enhancement-solutions/build-the-fracture       valid
Crawling page: https://carboceramics.com/Oil-gas/product-enhancement-solutions/optimize-the-fracture       valid
Crawling page: http://nlight.net/news/technical-papers       valid
Crawling page: https://carboceramics.com/Oil-gas       valid
Crawling page: https://carboceramics.com/Oil-gas/product-enhancement-solutions       valid
Crawling page: https://carboceramics.com/Oil-gas/product-enhancement-solutions/design-the-fracture       valid
Crawling page: http://nlight.net/service-request.php       valid
Crawling page: http://nlight.net/news/events/       valid
Crawling page: https://carboceramics.com/contact/business/carboaccucast-industrial-ceramics       valid
Crawling page: https://numverify.com/forgot       valid
Crawling page: https://numverify.com/login       valid
Crawling page: https://numverify.com/product       valid
Crawling page: https://numverify.com/about       valid
Crawling page: https://numverify.com/documentation       valid
Crawling page: http://www.adfors.com/us/sitemap       valid
Crawling page: http://www.adfors.com/us/es       valid
Crawling page: http://www.adfors.com/us/fr       valid
Crawling page: http://www.adfors.com/us/pt       valid
Crawling page: http://nlight.net/products/optical-fibers/       valid
Crawling page: http://nlight.net/products/fiber-lasers/       valid
Crawling page: http://nlight.net/products/diode-lasers/       valid
Crawling page: http://www.adfors.com/us/about-adfors       valid
Crawling page: http://www.adfors.com/us/news/new-fibatape-wall-repair-kit       valid
Crawling page: http://www.adfors.com/us/news/jlc-live-new-england       valid
Crawling page: http://www.adfors.com/us/news/new-fibalath-mesh       valid
Crawling page: http://www.adfors.com/us/news/new-improved-screen       valid
Crawling page: http://www.adfors.com/us/technologies       valid
Crawling page: http://www.adfors.com/us/brands/novelio/decoration-range       valid
Crawling page: http://www.adfors.com/us/brands/novelio/acoustic       valid
Crawling page: http://www.adfors.com/us/brands/novelio/renovation-range       valid
Crawling page: http://www.adfors.com/us/brands/novelio/design-range       valid
Crawling page: http://www.adfors.com/us/brands/novelio/thermal       valid
Crawling page: http://www.adfors.com/us/brands/novelio/cleanair       valid
Crawling page: http://www.adfors.com/us/brands/novelio/functional-range       valid
Crawling page: http://www.adfors.com/us/brands/novelio/mold-x       valid
Crawling page: http://www.adfors.com/us/brands/novelio/easyfix       valid
Crawling page: https://www.fujifilmusa.com/rss/index.html       valid
Crawling page: https://www.fujifilmusa.com/privacy/index.html       valid
Crawling page: https://www.fujifilmusa.com/cybersecurity/index.html       valid
Crawling page: https://www.fujifilmusa.com/terms_of_use/index.html       valid
Crawling page: https://www.fujifilmusa.com/about/modern_slavery_act_statements/index.html       valid
Crawling page: http://www.adfors.com/us/road-reinforcement/reinforcement-patches/glasgrid-pg       valid
Crawling page: http://www.adfors.com/us/brands/fibatape       valid
Crawling page: http://www.adfors.com/us/brands/novelio       valid
Crawling page: http://www.adfors.com/us/brands       valid
Crawling page: https://carboceramics.com/Industrial/Contract-manufacturing       valid
Crawling page: https://carboceramics.com/Industrial/Industrial-media-and-additives/CARBOBEAD-high-performance-ceramic-media       valid
Crawling page: https://carboceramics.com/Industrial/Industrial-grinding-media/CARBOGRIND-XT-high-performance-intermediate-densit       valid
Crawling page: https://carboceramics.com/Industrial/Industrial-media-and-additives       valid
Crawling page: https://carboceramics.com/Industrial/Industrial-grinding-media       valid
Crawling page: https://carboceramics.com/Industrial/Industrial-grinding-media/CARBOGRIND-ceramic-grinding-media       valid
Crawling page: https://carboceramics.com/Industrial/Industrial-casting-media/CARBOACCUCAST-ceramic-casting-media       valid
Crawling page: https://carboceramics.com/Industrial/Industrial-casting-media       valid
Crawling page: https://carboceramics.com/Industrial/Industrial-technologies-overview       valid
Crawling page: https://carboceramics.com/about/Technology-and-innovation       valid
Crawling page: https://carboceramics.com/Industrial       valid
Crawling page: https://carboceramics.com/about/manufacturing-and-supply       valid
Crawling page: https://carboceramics.com/about/our-values       valid
Crawling page: https://carboceramics.com/about/health,-safety-and-environment       valid
Crawling page: https://carboceramics.com/about/health,-safety-and-environment/HSE-news-awards       valid
Crawling page: https://carboceramics.com/about/health,-safety-and-environment/sustainability       valid
Crawling page: https://carboceramics.com/about/Our-approach       valid
Crawling page: https://carboceramics.com/about/about-carbo/Environmental-protection-solutions       valid
Crawling page: https://carboceramics.com/about/about-carbo/industrial-ceramics2       valid
Crawling page: https://carboceramics.com/about/about-carbo/ASSETGUARD       valid
Crawling page: https://carboceramics.com/about/about-carbo/fracture-consulting-overview       valid
Crawling page: https://carboceramics.com/about/about-carbo/Fracture-technologies-overview       valid
Crawling page: https://carboceramics.com/about/about-carbo/Fracture-software-overview-(1)       valid
Crawling page: https://carboceramics.com/about/about-carbo?lang=ru-RU       valid
Crawling page: https://carboceramics.com/Store/Checkout/Shopping-Cart.aspx       valid
Crawling page: https://carboceramics.com/       valid
Crawling page: https://carboceramics.com/home       valid
Crawling page: https://carboceramics.com/Industrial/Water-filtration-sand       valid
Crawling page: https://carboceramics.com/Industrial/Industrial-media-and-additives/METAKAO-Metakaolin-additives/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:36:19 [scrapy.extensions.logstats] INFO: Crawled 1664 pages (at 14 pages/min), scraped 1368 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:37:12 [scrapy.extensions.logstats] INFO: Crawled 1673 pages (at 9 pages/min), scraped 1378 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:38:14 [scrapy.extensions.logstats] INFO: Crawled 1682 pages (at 9 pages/min), scraped 1390 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:39:27 [scrapy.extensions.logstats] INFO: Crawled 1689 pages (at 7 pages/min), scraped 1401 items (at 11 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:40:34 [scrapy.extensions.logstats] INFO: Crawled 1707 pages (at 18 pages/min), scraped 1411 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:41:18 [scrapy.extensions.logstats] INFO: Crawled 1711 pages (at 4 pages/min), scraped 1419 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:42:31 [scrapy.extensions.logstats] INFO: Crawled 1724 pages (at 13 pages/min), scraped 1432 items (at 13 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

       valid
Crawling page: https://carboceramics.com/about/about-carbo?lang=en-US       valid
Crawling page: https://carboceramics.com/Industrial/Minerals-processing-services       valid
Crawling page: http://investors.nlight.net/       valid
Crawling page: http://nlight.net/careers/       valid
Crawling page: https://carboceramics.com/Industrial/Industrial-grinding-media/CARBOGRIND-NANO-ultra-high-performance-ceramic-gri       valid
Crawling page: https://carboceramics.com/Industrial/Industrial-grinding-media/CARBOGRIND-ZIRMAX-ultra-high-performance,-high-den       valid
Crawling page: https://carboceramics.com/products-and-services/industrial-technologies/CARBOBEAD-high-performance-ceramic-media       valid
Crawling page: https://carboceramics.com/Industrial/Industrial-grinding-media/CARBO-XT-high-performance-intermediate-density-ceramic-grinding-media       valid
Crawling page: http://nlight.net/news/releases/192~nLIGHT-Launches-Smallest-Form-Factor-10kW-Industrial-Fiber-Laser       valid
Crawling page: http://nlight.net/news/releases/194~nLIGHT-Launches-5kW-Corona-Fiber-Laser       valid
Crawling page: http://nlight.net/privacy_policy.php       valid
Crawling page: https://carboceramics.com/Industrial/Industrial-grinding-media/CARBOGRIND-MAX-ultra-high-performance-ceramic-gri       valid
Crawling page: https://carboceramics.com/products-and-services/industrial-ceramics2/CARBOGRIND-ceramic-grinding-media       valid
Crawling page: https://carboceramics.com/products-and-services/industrial-ceramics2/CARBOACCUCAST-ceramic-casting-media       valid
Crawling page: https://carboceramics.com/Environmental/Industrial-applications-overview/WASTEGUARD-portable-multipurpose-catch-basin       valid
Crawling page: http://nlight.net/www.bursasacisleme.com       valid
Crawling page: https://carboceramics.com/Environmental/Industrial-applications-overview/SUMP-BOX-sub-grade-catch-basin       valid
Crawling page: https://carboceramics.com/Environmental/Industrial-applications-overview/TANKGUARD-maintenance-free-impermeable-tank-bases       valid
Crawling page: http://nlight.net/products/fiber-lasers/6-10kW-FiberLasers       valid
Crawling page: http://nlight.net/products/corona/       valid
Crawling page: http://nlight.net/contact.php       valid
Crawling page: http://nlight.net/news/releases/       valid
Crawling page: http://nlight.net/service.php       valid
Crawling page: http://nlight.net/       valid
Crawling page: http://nlight.net/company/introduction       valid
Crawling page: http://www.adfors.com/us/building-products/wall-coverings/easyglue       valid
Crawling page: http://www.adfors.com/us/building-products/wall-coverings/easyfix       valid
Crawling page: http://www.adfors.com/us/building-products/wall-coverings/renovation-range       valid
Crawling page: http://www.adfors.com/us/building-products/wall-coverings/decoration-range       valid
Crawling page: http://www.adfors.com/us/building-products/wall-coverings/prepainted       valid
Crawling page: http://www.adfors.com/us/building-products/wall-coverings       valid
Crawling page: http://www.adfors.com/us/building-products/wall-coverings/standard       valid
Crawling page: http://www.adfors.com/us/building-products/wall-coverings/classic-range       valid
Crawling page: http://www.adfors.com/us/building-products/wall-repair/wall-bandage-repair-patch       valid
Crawling page: http://www.adfors.com/us/building-products/wall-repair       valid
Crawling page: http://www.adfors.com/us/building-products/wall-finishing-accessories/fiberglass-mesh-tape-applicator       valid
Crawling page: http://www.adfors.com/us/building-products/wall-repair/fibatape-crackstop       valid
Crawling page: http://www.adfors.com/us/building-products/wall-repair/fibatape-super-crackstop       valid
Crawling page: http://www.adfors.com/us/building-products/wall-finishing-accessories/fibatape-sandpaper-sheets       valid
Crawling page: http://www.adfors.com/us/building-products/wall-finishing-accessories/sanding-screens       valid
Crawling page: http://www.adfors.com/us/building-products/wall-finishing-accessories/fibatape-sanding-finishing-sponge       valid
Crawling page: https://www.fujifilmusa.com/products/industrial_inkjet_printheads/video/page_04.html       valid
Crawling page: https://www.fujifilmusa.com/products/industrial_inkjet_printheads/video/page_03.html       valid
Crawling page: https://www.fujifilmusa.com/about/corporate_profile/fujifilm_companies/dimatix/video/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/industrial_inkjet_printheads/video/index.html       valid
Crawling page: https://www.fujifilmusa.com/rss/newsfeed.xml       valid
Crawling page: https://www.fujifilmusa.com/support/ServiceSupportContactForm.do?catid=464128&prodcat=879586       valid
Crawling page: http://www.adfors.com/us/building-products/wall-finishing-tapes/fibatape-standard-drywall-tape       valid
Crawling page: http://www.adfors.com/us/building-products/wall-finishing-tapes/fibatape-cement-board-tape       valid
Crawling page: http://www.adfors.com/us/building-products/wall-finishing-tapes/fibatape-corner-tapes       valid
Crawling page: http://www.adfors.com/us/building-products/wall-finishing-tapes/fibafuse-paperless-drywall-tape       valid
Crawling page: https://www.fujifilmusa.com/press/news/list_news_for_category?newsCatID=543226&pageNbr=1       valid
Crawling page: https://www.fujifilmusa.com/press/news/display_news?newsID=881328       valid
Crawling page: https://www.fujifilmusa.com/press/news/display_news?newsID=881480       valid
Crawling page: https://www.fujifilmusa.com/press/news/display_news?newsID=881359       valid
Crawling page: https://www.fujifilmusa.com/products/industrial_inkjet_printheads/markets/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/industrial_inkjet_printheads/technology/technical-papers/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/industrial_inkjet_printheads/technology/versadrop/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/industrial_inkjet_printheads/technology/piezoelectric/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/industrial_inkjet_printheads/technology/samba/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/industrial_inkjet_printheads/technology/silicon-mems/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/industrial_inkjet_printheads/technology/mems/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/industrial_inkjet_printheads/technology/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/industrial_inkjet_printheads/deposition-products/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/industrial_inkjet_printheads/deposition-products/materials-cartridge/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/industrial_inkjet_printheads/deposition-products/dmp-2800/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/industrial_inkjet_printheads/print-products/technology_integration/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/industrial_inkjet_printheads/print-products/inks/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/industrial_inkjet_printheads/print-products/versadrop/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/industrial_inkjet_printheads/print-products/printhead-support/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/industrial_inkjet_printheads/print-products/product-dev-tools/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/industrial_inkjet_printheads/print-products/printheads/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/industrial_inkjet_printheads/print-products/index.html       valid
Crawling page: https://www.fujifilmusa.com/about/corporate_profile/fujifilm_companies/dimatix/about/index.html       valid
Crawling page: https://www.fujifilmusa.com/gateway/index.html       valid
Crawling page:/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:43:34 [scrapy.extensions.logstats] INFO: Crawled 1735 pages (at 11 pages/min), scraped 1443 items (at 11 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:44:23 [scrapy.extensions.logstats] INFO: Crawled 1743 pages (at 8 pages/min), scraped 1451 items (at 8 items/min)
2018-11-12 07:44:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://carboceramics.com/products-and-services>: HTTP status code is not handled or not allowed
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:45:28 [scrapy.extensions.logstats] INFO: Crawled 1754 pages (at 11 pages/min), scraped 1461 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:45:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://carboceramics.com/Oil-gas/stratagen/fracture-design-evaluation-and-optimization/STRATASTIM-matrix-based-reservoir-stimulation-work>: HTTP status code is not handled or not allowed
2018-11-12 07:45:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://carboceramics.com/Oil-gas/stratagen/fracture-design-evaluation-and-optimization/STRATASHALE-shale-based-reservoir-stimulation-work>: HTTP status code is not handled or not allowed
2018-11-12 07:45:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://carboceramics.com/Oil-gas/stratagen/Well-performance-analysis/NETWORX-fracture-network-stimulator-model>: HTTP status code is not handled or not allowed
2018-11-12 07:45:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://carboceramics.com/Oil-gas/stratagen/Well-performance-analysis/EFWORX-Eagle-Ford-completion-model>: HTTP status code is not handled or not allowed
/bin/sh: 1: kill: No such process

2018-11-12 07:46:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://carboceramics.com/Oil-gas/stratagen/Well-performance-analysis/BAKKENWORX-Bakken-completion-model>: HTTP status code is not handled or not allowed
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:46:25 [scrapy.extensions.logstats] INFO: Crawled 1767 pages (at 13 pages/min), scraped 1470 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:46:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.carboceramics.com/home?lang=en-US> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 07:47:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.carboceramics.com/home?lang=ru-RU> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:47:36 [scrapy.extensions.logstats] INFO: Crawled 1778 pages (at 11 pages/min), scraped 1478 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:48:12 [scrapy.extensions.logstats] INFO: Crawled 1783 pages (at 5 pages/min), scraped 1483 items (at 5 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:49:16 [scrapy.extensions.logstats] INFO: Crawled 1796 pages (at 13 pages/min), scraped 1492 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

 https://www.fujifilmusa.com/press/news/recent_news       valid
Crawling page: https://carboceramics.com/Environmental/Industrial-applications-overview/MOBILEGUARD-reusable-spill-containment       valid
Crawling page: https://carboceramics.com/Environmental/Oil-and-gas-applications-overview/LOCATIONGUARD       valid
Crawling page: https://carboceramics.com/Environmental/Industrial-applications-overview/SHOPGUARD-resilient-flooring-and-matting       valid
Crawling page: https://carboceramics.com/Environmental/Industrial-applications-overview/EQUIPMENTGUARD-AC       valid
Crawling page: https://carboceramics.com/Environmental/Industrial-applications-overview/AQUAGUARD-water-storage-insulation-panels       valid
Crawling page: https://carboceramics.com/Environmental/Industrial-applications-overview/GROUNDGUARD-pre-fabricated-liner       valid
Crawling page: https://carboceramics.com/Environmental/Industrial-applications-overview/EQUIPMENTGUARD-maintenance-free-modular-equipment       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/Production-services       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/FUSION-proppant-pack       valid
Crawling page: https://carboceramics.com/products-and-services/fracture-technologies/Proppant-delivered-evaluation-service-(1)/CARBONRT-non-radioactive-detectable-technology-(1)       valid
Crawling page: https://carboceramics.com/products-and-services/fracture-technologies/Proppant-delivered-evaluation-service/CARBONRT-GP-Inert-tracer-technology-for-gravel-pac       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/GUARD-proppant-delivered-production-assu/SALTGUARD-proppant-delivered-halite-inhibiting-tec       valid
Crawling page: https://carboceramics.com/products-and-services/fracture-technologies/proppant-delivered-evaluation-service/Fracture-evaluation       valid
Crawling page: https://carboceramics.com/products-and-services/fracture-technologies/proppant-delivered-evaluation-service/CARBONRT-non-radioactive-detectable-technology       valid
Crawling page: https://carboceramics.com/products-and-services/fracture-technologies/proppant-delivered-production-assu/scaleguard-proppant-delivered-scale-inhibiting-tec       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/CARBOBALL       valid
Crawling page: https://carboceramics.com/products-and-services/fracture-technologies/proppant-delivered-flow-enhancement-(1)/RPM-relative-permeability-modification-technology       valid
Crawling page: https://carboceramics.com/products-and-services/fracture-technologies/high-conductivity-sand-proppant       valid
Crawling page: https://carboceramics.com/products-and-services/fracture-technologies/ceramic-proppant/carbolite-high-performance,-low-density       valid
Crawling page: https://carboceramics.com/products-and-services/fracture-technologies/ceramic-proppant/carboprop-intermediate-density       valid
Crawling page: https://carboceramics.com/products-and-services/fracture-technologies/proppant-delivered-flow-enhancement/CARBOBOND-LITE-resin-coated-low-density-ceramic       valid
Crawling page: https://carboceramics.com/products-and-services/fracture-technologies/ceramic-proppant/carbohsp-high-strength       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/High-transport-proppant/CARBOAIR-for-gravel-pack-applications       valid
Crawling page: https://carboceramics.com/products-and-services/fracture-technologies/ceramic-proppant/carboeconoprop-low-density       valid
Crawling page: https://carboceramics.com/products-and-services/fracture-technologies/ceramic-proppant/carbohydroprop-low-density,-high-transport       valid
Crawling page: https://carboceramics.com/Oil-gas/Fracture-technologies/Microproppant/NANOMITE-microproppant       valid
Crawling page: https://carboceramics.com/products-and-services/fracture-technologies/Ultra-conductive-proppant-technology/kryptosphere-ld-ultra-conductive,-low-density-cer       valid
Crawling page: https://carboceramics.com/products-and-services/fracture-technologies/ceramic-proppant/carboair       valid
Crawling page: https://carboceramics.com/products-and-services/fracture-technologies/ceramic-proppant/kryptosphere-hd-ultra-conductive,-high-density-cer       valid
Crawling page: https://carboceramics.com/Oil-gas/fracpro/STIMPRO-matrix-acidizing-analysis-software       valid
Crawling page: http://www.adfors.com/us/building-products/insect-screens/screen-accessories       valid
Crawling page: http://www.adfors.com/us/building-products/pool-enclosure-patio-contractors/screen-enclosure-app       valid
Crawling page: http://www.adfors.com/us/building-products/pool-enclosure-patio-contractors/customizable-postcards       valid
Crawling page: http://www.adfors.com/us/building-products/pool-enclosure-patio-contractors/new-improved-screen       valid
Crawling page: https://carboceramics.com/Oil-gas/fracpro/FRACPRO-fracture-design-and-analysis-software       valid
Crawling page: https://carboceramics.com/Oil-gas/fracpro/fracpro-xchange-onsite-fracture-monitoring-softwar       valid
Crawling page: https://carboceramics.com/Oil-gas/fracpro/fracpro-remote-fracture-monitoring-app       valid
Crawling page: https://carboceramics.com/careers       valid
Crawling page: https://www.carboceramics.com/home?lang=en-US       Crawling page: https://www.carboceramics.com/home?lang=ru-RU       Crawling page: http://www.adfors.com/us/building-products/pool-enclosure-patio-contractors/small-insect-screen       valid
Crawling page: http://www.adfors.com/us/building-products/pool-enclosure-patio-contractors/pet-resistant-screen       valid
Crawling page: http://www.adfors.com/us/building-products/pool-enclosure-patio-contractors/pool-patio-screen       valid
Crawling page: http://www.adfors.com/us/building-products/insect-screens/pool-enclosure-patio-contractors       valid
Crawling page: http://www.adfors.com/us/building-products/window-door-manufacturers/screen-enclosure-app       valid
Crawling page: http://www.adfors.com/us/building-products/window-door-manufacturers/new-improved-screen       valid
Crawling page: http://www.adfors.com/us/building-products/window-door-manufacturers/customizable-postcards       valid
Crawling page: http://www.adfors.com/us/building-products/window-door-manufacturers/aluminum-screen       valid
Crawling page: http://www.adfors.com/us/building-products/insect-screens/window-door-manufacturers/small-insect-screen       valid
Crawling page: http://www.adfors.com/us/building-products/insect-screens/stiff-viper-stiff-screen       valid
Crawling page: http://www.adfors.com/us/building-products/window-door-manufacturers/sun-guard-solar-screen       valid
Crawling page: http://www.adfors.com/us/building-products/window-door-manufacturers/pool-patio-screen       valid
Crawling page: http://www.adfors.com/us/building-products/window-door-manufacturers/pollen-guard-screen       valid
Crawling page: http://www.adfors.com/us/building-products/window-door-manufacturers/extra-strength       valid
Crawling page: http://www.adfors.com/us/building-products/window-door-manufacturers/pet-resistant-screen       valid
Crawling page: http://www.adfors.com/us/building-products/diy-contractors/screen-warranty-information       valid
Crawling page: http://www.adfors.com/us/building-products/insect-screens/window-door-manufacturers       valid
Crawling page: https://www.fujifilmusa.com/press/index.html       valid
Crawling page: https://www.fujifilmusa.com/about/sns/index.html       valid
Crawling page: https://www.fujifilmusa.com/press/news/search/search_results?pageNb=0&mode=_submitSearchForm&year=2017       valid
Crawling page: https://www.fujifilmusa.com/press/news/search/search_results?pageNb=0&mode=_submitSearchForm&year=2016       valid
Crawling page: https://www.fujifilmusa.com/press/news/search/search_results?pageNb=0&mode=_submitSearchForm&year=2018       valid
Crawling page: http://www.adfors.com/us/building-products/diy-contractors/aluminum-screen       valid
Crawling page: http://www.adfors.com/us/building-products/diy-contractors/fiberglass-screen /bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:50:20 [scrapy.extensions.logstats] INFO: Crawled 1806 pages (at 10 pages/min), scraped 1503 items (at 11 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:51:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.fujifilmusa.com:443/support/msds/search> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:51:20 [scrapy.extensions.logstats] INFO: Crawled 1815 pages (at 9 pages/min), scraped 1514 items (at 11 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:52:28 [scrapy.extensions.logstats] INFO: Crawled 1826 pages (at 11 pages/min), scraped 1526 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:52:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.fujifilmusa.com:443/products/instax/index.html> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 07:52:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.fujifilmusa.com:443/products/tape_data_storage/index.html> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 07:53:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.fujifilmusa.com:443/products/medical/index.html> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 07:53:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.fujifilmusa.com:443/products/digital_cameras/index.html> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 07:53:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.fujifilmusa.com:443/products/graphic_arts_printing/index.html> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:53:53 [scrapy.extensions.logstats] INFO: Crawled 1836 pages (at 10 pages/min), scraped 1534 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:54:25 [scrapy.extensions.logstats] INFO: Crawled 1845 pages (at 9 pages/min), scraped 1539 items (at 5 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:55:22 [scrapy.extensions.logstats] INFO: Crawled 1854 pages (at 9 pages/min), scraped 1548 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:56:14 [scrapy.extensions.logstats] INFO: Crawled 1871 pages (at 17 pages/min), scraped 1557 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:57:19 [scrapy.extensions.logstats] INFO: Crawled 1875 pages (at 4 pages/min), scraped 1569 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

      valid
Crawling page: http://www.adfors.com/us/building-products/insect-screens/screen-replacement-kits       valid
Crawling page: http://www.adfors.com/us/building-products/diy-contractors/galvanized-steel       valid
Crawling page: https://www.fujifilmusa.com/about/Technology/index.html       valid
Crawling page: https://www.fujifilmusa.com/about/business_fields/index.html       valid
Crawling page: https://www.fujifilmusa.com/about/sustainability/index.html       valid
Crawling page: https://www.fujifilmusa.com/about/careers/page_03.html       valid
Crawling page: https://www.fujifilmusa.com/about/corporate_profile/index.html       valid
Crawling page: https://www.fujifilmusa.com/about/index.html       valid
Crawling page: https://www.fujifilmusa.com/support/ServiceSupportContactInfo.do?catid=464128&prodcat=234702       valid
Crawling page: https://www.fujifilmusa.com/support/ServiceSupportContactForm.do?catid=531319&prodcat=852007       valid
Crawling page: https://www.fujifilmusa.com/support/ServiceSupportContactInfo.do?catid=464128&prodcat=879895       valid
Crawling page: https://www.fujifilmusa.com/support/ServiceSupportContactInfo.do?catid=464128&prodcat=879893       valid
Crawling page: https://www.fujifilmusa.com/support/ServiceSupportContactInfo.do?catid=464128&prodcat=879870       valid
Crawling page: https://www.fujifilmusa.com/support/ServiceSupportContactInfo.do?catid=464128&prodcat=879563       valid
Crawling page: https://www.fujifilmusa.com:443/support/msds/search       Crawling page: https://www.fujifilmusa.com/support/ServiceSupportContactInfo.do?catid=464128&prodcat=852005       valid
Crawling page: https://www.fujifilmusa.com/support/ServiceSupportContactInfo.do?catid=531323&prodcat=879835       valid
Crawling page: https://www.fujifilmusa.com/support/ServiceSupportContactInfo.do?catid=464128&prodcat=839405       valid
Crawling page: https://www.fujifilmusa.com/support/ServiceSupportContactInfo.do?catid=464124&prodcat=211151       valid
Crawling page: https://www.fujifilmusa.com/support/ServiceSupportContactInfo.do?catid=464128&prodcat=233838       valid
Crawling page: https://www.fujifilmusa.com/support/ServiceSupportContactInfo.do?catid=464128&prodcat=879985       valid
Crawling page: https://www.fujifilmusa.com/support/ServiceSupportContactInfo.do?catid=464128&prodcat=879586       valid
Crawling page: https://www.fujifilmusa.com/support/ServiceSupportContactInfo.do?catid=464128&prodcat=852004       valid
Crawling page: https://www.fujifilmusa.com/support/ServiceSupportContactInfo.do?catid=464128&prodcat=880933       valid
Crawling page: https://www.fujifilmusa.com/support/ServiceSupportContactInfo.do?catid=531311&prodcat=879051       valid
Crawling page: https://www.fujifilmusa.com/support/ServiceSupportContactInfo.do?catid=531316&prodcat=852006       valid
Crawling page: https://www.fujifilmusa.com/support/recall/index.html       valid
Crawling page: https://www.fujifilmusa.com/support/ServiceSupportContactInfo.do?catid=464128&prodcat=879531       valid
Crawling page: https://www.fujifilmusa.com/support/ServiceSupportContactInfo.do?catid=464128&prodcat=233844       valid
Crawling page: https://www.fujifilmusa.com/support/ServiceSupportContactInfo.do?catid=464128&prodcat=878952       valid
Crawling page: https://www.fujifilmusa.com/support/ServiceSupportContactInfo.do?catid=464128&prodcat=234644       valid
Crawling page: https://www.fujifilmusa.com/support/index.html       valid
Crawling page: https://www.fujifilmusa.com:443/products/instax/index.html       Crawling page: https://www.fujifilmusa.com:443/products/tape_data_storage/index.html       Crawling page: https://www.fujifilmusa.com:443/products/medical/index.html       Crawling page: https://www.fujifilmusa.com:443/products/digital_cameras/index.html       Crawling page: https://www.fujifilmusa.com:443/products/graphic_arts_printing/index.html       Crawling page: http://www.adfors.com/us/industrial-fabrics/specialty/sailcloth-reinforcement       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/roofing/accessories       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/specialty       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/roofing/single-ply       valid
Crawling page: https://carboceramics.com/about/about-carbo       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/papersfilmsother/brattice-cloth       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/papersfilmsother/filtration       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/papersfilmsother/geomembranes       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/papersfilmsother/wipers       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/papersfilmsother       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/flooring/radiant-flooring       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/flooring/carpet-cushion       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/flooring/carpet-backing       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/flooring       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/interiorexterior-reinforcement/cement-board       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/interiorexterior-reinforcement/gypsum-facer       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/interiorexterior-reinforcement/srg       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/rigid-boards/gypsum-boards       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/interiorexterior-reinforcement/mosaic       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/interiorexterior-reinforcement/foam-shape       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/interiorexterior-reinforcement/stucco       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/interiorexterior-reinforcement/eifs       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/interiorexterior-reinforcement       valid
Crawling page: https://www.fujifilmusa.com/products/motion_picture_archive_film/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/semiconductor_materials/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/measurement-films/index.html       valid
Crawling page: http://www.adfors.com       valid
Crawling page: https://www.fujifilmusa.com/products/technical_services/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/optical_devices/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/non-destructive_testing/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/specialty_chemicals/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/anti-counterfeit_labels/index.html       valid
Crawling page: http://www.adfors.com/us/start-conversation       valid
Crawling page: http://www.adfors.com/us/where-to-buy       valid
Crawling page: http://www.adfors.com/us/medias-gallery       valid
Crawling page: http://www.adfors.com/us/news       valid
Crawling page: https://www.fujifilmusa.com/products/personalized_photo_products/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/microfilm/       valid
Crawling page: https://www.fujifilmusa.com/products/professional_photography/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/digital_cameras/x/fujifilm_x_t1_ir/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/photofinishing/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/pro_av/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/tape_data_storage/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/imaging_colorants/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/diagnostic_assays/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/laboratory_high_purity_chemicals/index.html       /bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:58:29 [scrapy.extensions.logstats] INFO: Crawled 1891 pages (at 16 pages/min), scraped 1582 items (at 13 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 07:59:21 [scrapy.extensions.logstats] INFO: Crawled 1899 pages (at 8 pages/min), scraped 1591 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:00:30 [scrapy.extensions.logstats] INFO: Crawled 1910 pages (at 11 pages/min), scraped 1603 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:01:14 [scrapy.extensions.logstats] INFO: Crawled 1913 pages (at 3 pages/min), scraped 1609 items (at 6 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:02:26 [scrapy.extensions.logstats] INFO: Crawled 1920 pages (at 7 pages/min), scraped 1618 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:03:15 [scrapy.extensions.logstats] INFO: Crawled 1928 pages (at 8 pages/min), scraped 1623 items (at 5 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:03:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 http://toraytpa.com/mold-decoration-anti-stat-antistat>: HTTP status code is not handled or not allowed
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:04:13 [scrapy.extensions.logstats] INFO: Crawled 1949 pages (at 21 pages/min), scraped 1635 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:05:41 [scrapy.extensions.logstats] INFO: Crawled 1949 pages (at 0 pages/min), scraped 1646 items (at 11 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:06:49 [scrapy.extensions.logstats] INFO: Crawled 1979 pages (at 30 pages/min), scraped 1664 items (at 18 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:07:33 [scrapy.extensions.logstats] INFO: Crawled 1979 pages (at 0 pages/min), scraped 1671 items (at 7 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

valid
Crawling page: https://www.fujifilmusa.com/products/industrial_inkjet_printheads/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/graphic_arts_printing/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/medical/medical-informatics/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/medical/endoscopy/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/minimally_invasive/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/new_drug_development/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/ultrasound/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/sports_optics/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/medical/digital-mammography/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/medical/digital-x-ray/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/binoculars/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/film_photography/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/instax/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/digital_cameras/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/index.html       valid
Crawling page: https://www.fujifilmusa.com/innovation/achievements/       valid
Crawling page: https://www.fujifilmusa.com/innovation/open-innovation/       valid
Crawling page: https://www.fujifilmusa.com/innovation/open-innovation-hub/       valid
Crawling page: https://www.fujifilmusa.com/innovation/stories/       valid
Crawling page: https://www.fujifilmusa.com/innovation/index.html       valid
Crawling page: https://www.fujifilmusa.com/contact_us/index.html       valid
Crawling page: https://www.fujifilmusa.com/sitemap/index.html       valid
Crawling page: https://www.fujifilmusa.com/products/medical/index.html       valid
Crawling page: https://www.zygo.com/?/contact/privacypolicy.htm       valid
Crawling page: https://www.relypsa.com/newsroom/investors/       valid
Crawling page: https://www.relypsa.com/join-us/our-culture/       valid
Crawling page: https://www.relypsa.com/join-us/careers/       valid
Crawling page: https://www.relypsa.com/newsroom/for-media/       valid
Crawling page: https://www.relypsa.com/our-medicine/investigator-initiated-studies/       valid
Crawling page: https://www.zygo.com/?/aboutus/shows/       valid
Crawling page: https://www.relypsa.com/our-medicine/what-is-hyperkalemia/       valid
Crawling page: https://www.relypsa.com/resources/publications/       valid
Crawling page: https://www.relypsa.com/our-medicine/adverse-event-reporting/       valid
Crawling page: https://www.zygo.com/?/blog/facts-and-statistics-about-zygo/       valid
Crawling page: https://www.zygo.com/?/blog/measuring-roughness-on-super-smooth-surfaces/       valid
Crawling page: https://www.zygo.com/?/blog/smart-setup-streamlines-precision-surface-metrology/       valid
Crawling page: https://www.zygo.com/?/blog/precision-gear-tooth-metrology-helps-silence-humming-differential/       valid
Crawling page: https://www.zygo.com/?/blog/grazing-incidence-interferometry-for-measuring-large-or-non-specular-surfaces/       valid
Crawling page: https://www.zygo.com/?/blog/process-metrology-helps-ensure-quality-of-smartphone-camera-lenses/       valid
Crawling page: https://www.relypsa.com/who-we-are/about-us/       valid
Crawling page: https://www.zygo.com/?/blog/lead-angle-metrology-improves-shaft-seals/       valid
Crawling page: https://www.relypsa.com/who-we-are/mission-values/       valid
Crawling page: https://www.relypsa.com/who-we-are/our-history/       valid
Crawling page: https://www.zygo.com/?/blog/       valid
Crawling page: https://www.zygo.com/?/aboutus/privacy-cookie-policy.htm       valid
Crawling page: https://www.zygo.com/?/aboutus/news/20181017.htm       valid
Crawling page: https://www.zygo.com/?/met/interferometers/verifire/hdx/&utm_source=zygo&utm_medium=HomePageBanner&utm_campaign=VerifireHDX       valid
Crawling page: https://www.zygo.com/?/met/profilers/zegagepro/&utm_source=zygo&utm_medium=HomePageBanner&utm_campaign=ZeGagePro       valid
Crawling page: https://www.zygo.com/?/opt/omassembly/       valid
Crawling page: https://www.zygo.com/?/opt/markets/       valid
Crawling page: https://www.zygo.com/?/opt/sysmfg/       valid
Crawling page: http://toraytpa.com/Release-Tape       valid
Crawling page: http://toraytpa.com/Wire-and-Cable-wrap       valid
Crawling page: http://toraytpa.com/Brushing       valid
Crawling page: http://toraytpa.com/Balloon       valid
Crawling page: http://toraytpa.com/Matte       valid
Crawling page: http://toraytpa.com/Mold-Decoration-IMD       valid
Crawling page: http://toraytpa.com/Dry-Film-Resist-DFR       valid
Crawling page: http://toraytpa.com/Lamination-and-Label-Film       valid
Crawling page: http://toraytpa.com/Release-liner-base-film       valid
Crawling page: https://www.zygo.com/?/opt/services/       valid
Crawling page: http://toraytpa.com/Solar-Release-liner-base-film       valid
Crawling page: https://www.zygo.com/?/opt/eoassembly/       valid
Crawling page: https://www.zygo.com/?/opt/electro-optics/       valid
Crawling page: http://toraytpa.com/Emboss       valid
Crawling page: https://www.zygo.com/?/opt/components/       valid
Crawling page: https://www.zygo.com/?/met/support/upgrades/       valid
Crawling page: https://www.zygo.com/?/met/custommetrologysolutions/       valid
Crawling page: https://www.zygo.com/?/met/interferometers/       valid
Crawling page: http://toraytpa.com/Hot-Stamping-Foil-HSF       valid
Crawling page: http://toraytpa.com/Solar-Control-Film       valid
Crawling page: http://toraytpa.com/Metallized-Cold-Seal-Film       valid
Crawling page: http://toraytpa.com/Metallized-Heat-Seal-Film       valid
Crawling page: http://toraytpa.com/Clear-Heat-Seal-Film       valid
Crawling page: http://toraytpa.com/Retort-Film       valid
Crawling page: http://toraytpa.com/Film-Board-Lamination-Film       valid
Crawling page: http://toraytpa.com/Foil-Replacement-Lamination-Film       valid
Crawling page: http://toraytpa.com/Metallized-Lamination-Film       valid
Crawling page: http://toraytpa.com/Clear-Lamination-Film       valid
Crawling page: http://toraytpa.com/Foil-Replacement-Metallized-Film       valid
Crawling page: http://toraytpa.com/Lamination-Metallized-Film       valid
Crawling page: http://toraytpa.com/Cold-Seal-Metallized-Film       valid
Crawling page: http://toraytpa.com/Heat-Seal-Metallized-Film       valid
Crawling page: http://toraytpa.com/Non-Sealable-Printing-Film       valid
Crawling page: http://toraytpa.com/Sealable-Printing-Film       valid
Crawling page: http://toraytpa.com/metalized-barrier-film       valid
Crawling page: http://toraytpa.com/site-map       valid
Crawling page: http://toraytpa.com/privacy-policy       valid
Crawling page: https://www.zygo.com/?/aboutus/       valid
Crawling page: https://www.zygo.com/?/jobs/       valid
Crawling page: https://www.zygo.com/?/met/profilers/       valid
Crawling page: http://toraytpa.com/Clear-Cold-Seal-Film       valid
Crawling page: https://www.zygo.com/?/contact/       valid
Crawling page: http://toraytpa.com/clear-barrier-film       valid
Crawling page: http://toraytpa.com/terms-and-conditions       valid
Crawling page: http://toraytpa.com/news-updates/2017/toray-plastics-america-introduces-medical-packaging-film-use-gamma-irradiation       valid
Crawling page: https://www.zygo.com/?/met/       valid
Crawling page: http://toraytpa.com/news-updates/2017/toray-plastics-america-launches-two-ultra-high-barrier-pet-films-extended-shelf       valid
Crawling page: http://toraytpa.com/news-updates/2017/toray-plastics-america%E2%80%99s-met-opp-and-met-pet-films-offer-end-users-and-converters       valid
Crawling page: http://toraytpa.com/news-updates/2018/toray-plastics-america-launches-its-third-generation-torayfan-transparent-high       valid
Crawling page: http://toraytpa.com/news-updates/2018/toray-plastics-america-feature-new-forward-looking-packaging-innovations-pack-expo/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:08:22 [scrapy.extensions.logstats] INFO: Crawled 1992 pages (at 13 pages/min), scraped 1683 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:09:25 [scrapy.extensions.logstats] INFO: Crawled 2000 pages (at 8 pages/min), scraped 1693 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:10:18 [scrapy.extensions.logstats] INFO: Crawled 2008 pages (at 8 pages/min), scraped 1701 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:11:14 [scrapy.extensions.logstats] INFO: Crawled 2016 pages (at 8 pages/min), scraped 1709 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:12:16 [scrapy.extensions.logstats] INFO: Crawled 2028 pages (at 12 pages/min), scraped 1719 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:14:03 [scrapy.extensions.logstats] INFO: Crawled 2044 pages (at 16 pages/min), scraped 1737 items (at 18 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:14:26 [scrapy.extensions.logstats] INFO: Crawled 2048 pages (at 4 pages/min), scraped 1741 items (at 4 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:15:34 [scrapy.extensions.logstats] INFO: Crawled 2058 pages (at 10 pages/min), scraped 1751 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:16:13 [scrapy.extensions.logstats] INFO: Crawled 2064 pages (at 6 pages/min), scraped 1757 items (at 6 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:17:25 [scrapy.extensions.logstats] INFO: Crawled 2070 pages (at 6 pages/min), scraped 1767 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:18:12 [scrapy.extensions.logstats] INFO: Crawled 2081 pages (at 11 pages/min), scraped 1773 items (at 6 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

       valid
Crawling page: http://toraytpa.com/products/polypropylene-films       valid
Crawling page: http://toraytpa.com/products/polypropylene-films/mwx6       valid
Crawling page: http://toraytpa.com/polyolefin-foams       valid
Crawling page: http://toraytpa.com/products/polyester-films       valid
Crawling page: http://toraytpa.com/news-updates       valid
Crawling page: http://toraytpa.com/about-us       valid
Crawling page: http://toraytpa.com/employment       valid
Crawling page: http://toraytpa.com/content/films-contact       valid
Crawling page: http://toraytpa.com/user/password       valid
Crawling page: https://acell.com/terms-of-use/       valid
Crawling page: https://acell.com/acell-announces-participation-in-medica-international-trade-fair/       valid
Crawling page: https://acell.com/acell-privacy-policy/       valid
Crawling page: https://acell.com/event/medica-2018/       valid
Crawling page: https://acell.com/distributor-map/       valid
Crawling page: https://acell.com/acell-announces-participation-in-2018-symposium-on-advanced-wound-care-fall/       valid
Crawling page: https://acell.com/main-contact/       valid
Crawling page: https://acell.com/clinician-corner/       valid
Crawling page: https://acell.com/ethics-and-compliance/       valid
Crawling page: https://acell.com/quality/       valid
Crawling page: https://acell.com/category/news/       valid
Crawling page: https://acell.com/careers/       valid
Crawling page: https://acell.com/our-story/       valid
Crawling page: https://acell.com/advisory-board/       valid
Crawling page: https://acell.com/leadership-team/       valid
Crawling page: https://acell.com/board-of-directors/       valid
Crawling page: https://acell.com/about-us/       valid
Crawling page: https://acell.com/physician-registry/       valid
Crawling page: https://acell.com/upcoming-events/       valid
Crawling page: https://acell.com/contact-us/       valid
Crawling page: https://acell.com/publications/       valid
Crawling page: https://acell.com/resources/       valid
Crawling page: https://acell.com/product-list/       valid
Crawling page: https://acell.com/reimbursement/       valid
Crawling page: https://acell.com/ecmubm-overview/       valid
Crawling page: https://acell.com/xpansion/       valid
Crawling page: https://acell.com/micromatrix/       valid
Crawling page: https://acell.com/technology/       valid
Crawling page: https://acell.com/dynamic-tissue-system-solutions/       valid
Crawling page: https://acell.com/gentrix-incisional/       valid
Crawling page: https://acell.com/surgical-matrix-products/       valid
Crawling page: https://acell.com/wound-matrix/       valid
Crawling page: http://toraytpa.com/news-updates/2018/toray-plastics-america-expands-torayfan%C2%AE-white-films-portfolio-launch-white       valid
Crawling page: http://toraytpa.com/products/polypropylene-films/cbc       valid
Crawling page: http://toraytpa.com/product-selector?product=polypropylene_films       valid
Crawling page: http://toraytpa.com/product-selector?product=polyester_films       valid
Crawling page: https://acell.com/burn-matrix/       valid
Crawling page: https://acell.com/upper-extremity-traumatic-injury/       valid
Crawling page: https://acell.com/products/       valid
Crawling page: https://acell.com/2nd-degree-burn/       valid
Crawling page: http://toraytpa.com/Sail-Cloth       valid
Crawling page: http://toraytpa.com/Roofing-shingle-release       valid
Crawling page: https://www.zygo.com/?/opt/       valid
Crawling page: https://www.zygo.com/?/met/positionmetrology/       valid
Crawling page: https://www.relypsa.com/who-we-are/management/       valid
Crawling page: https://www.relypsa.com/resources/       valid
Crawling page: https://www.relypsa.com/who-we-are/giving-back/       valid
Crawling page: https://www.relypsa.com/what-we-do/our-perspective/       valid
Crawling page: https://acell.com/open-abdomen/       valid
Crawling page: https://acell.com/diabetic-foot-ulcer/       valid
Crawling page: https://acell.com/surgical-products/       valid
Crawling page: https://acell.com/extraordinary-cases/       valid
Crawling page: https://www.relypsa.com/newsroom/press-releases/       valid
Crawling page: https://www.relypsa.com/newsroom/digital-library/       valid
Crawling page: https://www.relypsa.com/newsroom/fact-sheets/       valid
Crawling page: https://www.relypsa.com/contact/       valid
Crawling page: https://acell.com/acute-wounds/       valid
Crawling page: https://acell.com/burns/       valid
Crawling page: https://acell.com/clinical-applications/       valid
Crawling page: https://acell.com/chronic-wounds/       valid
Crawling page: https://acell.com/patient-stories/       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/roofing/liquid-applied       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/roofing/mod-bit-roofing       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/roofing/built-roofing-bur       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/roofing       valid
Crawling page: https://acell.com/patient-information/       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/specialty/esf-technology       valid
Crawling page: http://www.adfors.com/us/building-products/exterior-reinforcement       valid
Crawling page: http://www.adfors.com/us/building-products       valid
Crawling page: http://www.adfors.com/us/building-products/insect-screens       valid
Crawling page: http://www.adfors.com/us/building-products/diy-contractors/sun-guard-solar-screen       valid
Crawling page: http://www.adfors.com/us/building-products/diy-contractors/glass-screen       valid
Crawling page: http://www.adfors.com/us/building-products/diy-contractors/small-insect-screen       valid
Crawling page: http://www.adfors.com/us/building-products/diy-contractors/pool-patio-screen       valid
Crawling page: http://www.adfors.com/us/building-products/diy-contractors/doorway-alert-screen       valid
Crawling page: http://www.adfors.com/us/building-products/diy-contractors/bronze-screen       valid
Crawling page: http://www.adfors.com/us/building-products/insect-screens/fiberglass-screen       valid
Crawling page: http://www.adfors.com/us/building-products/pool-enclosure-patio-contractors/glass-screen       valid
Crawling page: http://www.adfors.com/us/building-products/window-door-manufacturers/bronze-screen       valid
Crawling page: http://www.adfors.com/us/building-products/window-door-manufacturers/galvanized-steel       valid
Crawling page: http://www.adfors.com/us/building-products/pool-enclosure-patio-contractors/p3-pool-patio-plus       valid
Crawling page: http://www.adfors.com/us/building-products/pool-enclosure-patio-contractors/certified-contractor-program       valid
Crawling page: http://www.adfors.com/us/building-products/insect-screens/screen-curtains       valid
Crawling page: http://www.adfors.com/us/building-products/screen-accessories/adjustable-window-screens       valid
Crawling page: http://www.adfors.com/us/building-products/wall-finishing-tapes       valid
Crawling page: http://www.adfors.com/us/building-products/screen-accessories/screen-installation-tools       valid
Crawling page: http://www.adfors.com/us/building-products/wall-finishing-tapes/fibatape-extra-strength-drywall-tape       valid
Crawling page: http://www.adfors.com/us/building-products/wall-finishing-accessories/hand-and-pole-drywall-sanders       valid
Crawling page: http://www.adfors.com/us/building-products/wall-repair/fibafuse-paperless-wall-repair-fabric       valid
Crawling page: http://www.adfors.com/us/building-products/wall-finishing-accessories/handheld-vacuum-drywall-sander       valid
Crawling page: http://www.adfors.com/us/building-products/wall-finishing-accessories/vacuum-pole-drywall-sander       valid
Crawling page: http://www.adfors.com/us/building-products/wall-repair/complete-wall-repair-kit       valid
Crawling page: http://www.adfors.com/us/building-products/wall-repair/wall-ceiling-repair-patches       valid
Crawling page: http://www.adfors.com/us/road-reinforcement/reinforcement-grids/airports       valid
Crawling page:/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:19:20 [scrapy.extensions.logstats] INFO: Crawled 2090 pages (at 9 pages/min), scraped 1783 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:20:15 [scrapy.extensions.logstats] INFO: Crawled 2099 pages (at 9 pages/min), scraped 1791 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:20:48 [scrapy.core.scraper] ERROR: Error downloading <GET https://numverify.com/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:21:20 [scrapy.extensions.logstats] INFO: Crawled 2108 pages (at 9 pages/min), scraped 1801 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:21:45 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.adfors.com/us/building-products/wall-finishing-tapes/fibatape-mold-x10-drywall-tape>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 08:21:45 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.adfors.com/us/building-products/wall-repair/electrical-outlet-patch>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 08:21:45 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.adfors.com/us/legal>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 08:21:45 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.adfors.com/us/node/1106>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:22:29 [scrapy.extensions.logstats] INFO: Crawled 2115 pages (at 7 pages/min), scraped 1811 items (at 10 items/min)
2018-11-12 08:22:29 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.adfors.com/us/building-products/window-door-manufacturers/viper-vision-perfect-screen>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 08:22:29 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.adfors.com/us/building-products/pool-enclosure-patio-contractors/custom-website-templates-new-york-wire>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 08:22:29 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.adfors.com/us/building-products/insect-screens/vent-mesh>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
/bin/sh: 1: kill: No such process

2018-11-12 08:22:36 [scrapy.core.scraper] ERROR: Error downloading <GET http://toraytpa.com/Pouch-Sealant-Web-film>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:23:15 [scrapy.extensions.logstats] INFO: Crawled 2123 pages (at 8 pages/min), scraped 1819 items (at 8 items/min)
/bin/sh: 1: kill: No such process

2018-11-12 08:23:21 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.adfors.com/us/building-products/diy-contractors/pet-resistant-screen-0>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 08:23:21 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.adfors.com/us/fr/nos-marques/fibafuse>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-12 08:23:21 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.adfors.com/us/building-products/insect-screens/diy-contractors-0>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 08:24:04 [scrapy.core.engine] INFO: Closing spider (finished)
2018-11-12 08:24:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 573,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 23,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 11,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 536,
 'downloader/request_bytes': 1457636,
 'downloader/request_count': 3001,
 'downloader/request_method_count/GET': 3001,
 'downloader/response_bytes': 67443732,
 'downloader/response_count': 2428,
 'downloader/response_status_count/200': 2107,
 'downloader/response_status_count/301': 193,
 'downloader/response_status_count/302': 69,
 'downloader/response_status_count/403': 1,
 'downloader/response_status_count/404': 11,
 'downloader/response_status_count/406': 3,
 'downloader/response_status_count/408': 38,
 'downloader/response_status_count/500': 5,
 'downloader/response_status_count/999': 1,
 'dupefilter/filtered': 138,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 11, 12, 8, 24, 4, 969737),
 'httperror/response_ignored_count': 25,
 'httperror/response_ignored_status_count/403': 1,
 'httperror/response_ignored_status_count/404': 11,
 'httperror/response_ignored_status_count/406': 3,
 'httperror/response_ignored_status_count/408': 8,
 'httperror/response_ignored_status_count/500': 1,
 'httperror/response_ignored_status_count/999': 1,
 'item_scraped_count': 1827,
 'log_count/ERROR': 284,
 'log_count/INFO': 248,
 'log_count/WARNING': 1,
 'memusage/max': 228585472,
 'memusage/startup': 101625856,
 'offsite/domains': 164,
 'offsite/filtered': 654,
 'request_depth_max': 1,
 'response_received_count': 2130,
 'retry/count': 526,
 'retry/max_reached': 90,
 'retry/reason_count/408 Request Time-out': 30,
 'retry/reason_count/500 Internal Server Error': 4,
 'retry/reason_count/twisted.internet.error.ConnectError': 23,
 'retry/reason_count/twisted.internet.error.TimeoutError': 11,
 'retry/reason_count/twisted.web._newclient.ResponseFailed': 2,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 456,
 'scheduler/dequeued': 3001,
 'scheduler/dequeued/memory': 3001,
 'scheduler/enqueued': 3001,
 'scheduler/enqueued/memory': 3001,
 'spider_exceptions/AttributeError': 27,
 'spider_exceptions/TimeoutException': 1,
 'spider_exceptions/UnexpectedAlertPresentException': 115,
 'spider_exceptions/UnicodeEncodeError': 57,
 'spider_exceptions/WebDriverException': 2,
 'start_time': datetime.datetime(2018, 11, 12, 4, 43, 12, 105864)}
2018-11-12 08:24:04 [scrapy.core.engine] INFO: Spider closed (finished)
 http://www.adfors.com/us/road-reinforcement/reinforcement-grids       valid
Crawling page: http://www.adfors.com/us/building-products/roofing-fabrics/pgm-roofing-fabrics       valid
Crawling page: http://www.adfors.com/us/road-reinforcement       valid
Crawling page: http://www.adfors.com/us/road-reinforcement/reinforcement-grids/bridges-tunnels       valid
Crawling page: http://www.adfors.com/us/road-reinforcement/reinforcement-mats       valid
Crawling page: http://www.adfors.com/us/road-reinforcement/reinforcement-grids/roads-highways       valid
Crawling page: http://www.adfors.com/us/road-reinforcement/reinforcement-mats/airports       valid
Crawling page: http://www.adfors.com/us/road-reinforcement/reinforcement-mats/roads-highways       valid
Crawling page: https://numverify.com/terms       valid
Crawling page: https://numverify.com/privacy       valid
Crawling page: https://numverify.com/sitemap.php       valid
Crawling page: https://numverify.com/faq       valid
Crawling page: https://numverify.com/contact       valid
Crawling page: http://www.adfors.com/us/brands/novelio/easyglue       valid
Crawling page: http://www.adfors.com/us/brands/new-york-wire       valid
Crawling page: http://www.adfors.com/us/brands/fibafuse/fibafuse-paperless-drywall-tape       valid
Crawling page: http://www.adfors.com/us/brands/novelio/standard       valid
Crawling page: http://www.adfors.com/us/brands/novelio/easypaint       valid
Crawling page: http://www.adfors.com/us/brands/novelio/classic-range       valid
Crawling page: http://www.adfors.com/us/screen-product-selector       valid
Crawling page: http://www.adfors.com/us/news/new-app       valid
Crawling page: http://www.adfors.com/us/road-reinforcement/reinforcement-patches/glasgrid-pm       valid
Crawling page: http://www.adfors.com/us/building-products/roofing-fabrics/fibatape-roof-repair       valid
Crawling page: http://www.adfors.com/us/building-products/wall-coverings/nature-range       valid
Crawling page: http://www.adfors.com/us/road-reinforcement/reinforcement-patches       valid
Crawling page: http://www.adfors.com/us/building-products/roofing-fabrics       valid
Crawling page: http://www.adfors.com/us/building-products/wall-finishing-tapes/fibatape-veneer-plaster-tape       valid
Crawling page: http://www.adfors.com/us/building-products/wall-finishing-accessories       valid
Crawling page: http://www.adfors.com/us/brands/fibafuse/fibafuse-paperless-wall-repair-fabric       valid
Crawling page: http://www.adfors.com/us/building-products/wall-finishing-tapes/fibatape-perfect-finish-tape       valid
Crawling page: http://www.adfors.com/us/building-products/screen-accessories/door-grilles       valid
Crawling page: http://www.adfors.com/us/building-products/screen-accessories/spline       valid
Crawling page: http://www.adfors.com/us/building-products/insect-screens/screen-frame-kits       valid
Crawling page: http://toraytpa.com/Thermal-transfer-Ribbon       valid
Crawling page: http://toraytpa.com/Lidding-Film       valid
Crawling page: http://toraytpa.com/Specialty-Film       valid
Crawling page: http://www.adfors.com/us/building-products/diy-contractors/screen-enclosure-app       valid
Crawling page: http://www.adfors.com/us/building-products/insect-screens/diy-contractors/extra-strength       valid
Crawling page: http://www.adfors.com/us/building-products/diy-contractors/new-improved-screen       valid
Crawling page: http://www.adfors.com/us/building-products/diy-contractors/clear-advantage-screen       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/facingpackaging-reinforcement/insulation-facing       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/facingpackaging-reinforcement/flex-duct       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/facingpackaging-reinforcement       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/facingpackaging-reinforcement/packaging       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/composite-reinforcement/fiberglass-tank       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/composite-reinforcement/fg-reinforced-pipe       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics       valid
Crawling page: http://www.adfors.com/us/industrial-fabrics/composite-reinforcement       valid
