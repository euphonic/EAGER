nohup: ignoring input
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 384, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 380, in _make_request
    httplib_response = conn.getresponse()
  File "/usr/lib/python3.6/http/client.py", line 1331, in getresponse
    response.begin()
  File "/usr/lib/python3.6/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/lib/python3.6/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.6/socket.py", line 586, in readinto
    return self._sock.recv_into(b)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/retry.py", line 367, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/packages/six.py", line 686, in reraise
    raise value
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 386, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 306, in _raise_timeout
    raise ReadTimeoutError(self, url, "Read timed out. (read timeout=%s)" % timeout_value)
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='king-electric.com', port=80): Read timed out. (read timeout=5)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/read_urls.py", line 22, in fix_urls
    resp = requests.get(http_url, headers=headers, timeout=timeout, verify=False)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 637, in send
    r = adapter.send(request, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 529, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: HTTPConnectionPool(host='king-electric.com', port=80): Read timed out. (read timeout=5)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 384, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 380, in _make_request
    httplib_response = conn.getresponse()
  File "/usr/lib/python3.6/http/client.py", line 1331, in getresponse
    response.begin()
  File "/usr/lib/python3.6/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/lib/python3.6/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.6/socket.py", line 586, in readinto
    return self._sock.recv_into(b)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py", line 307, in recv_into
    raise timeout('The read operation timed out')
socket.timeout: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/retry.py", line 367, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/packages/six.py", line 686, in reraise
    raise value
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 386, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 306, in _raise_timeout
    raise ReadTimeoutError(self, url, "Read timed out. (read timeout=%s)" % timeout_value)
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='king-electric.com', port=443): Read timed out. (read timeout=5)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/read_urls.py", line 36, in fix_urls
    resp = requests.get(http_www_url, headers=headers, timeout=timeout, verify=False)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 659, in send
    history = [resp for resp in gen] if allow_redirects else []
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 659, in <listcomp>
    history = [resp for resp in gen] if allow_redirects else []
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 238, in resolve_redirects
    **adapter_kwargs
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 637, in send
    r = adapter.send(request, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 529, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='king-electric.com', port=443): Read timed out. (read timeout=5)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 384, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 380, in _make_request
    httplib_response = conn.getresponse()
  File "/usr/lib/python3.6/http/client.py", line 1331, in getresponse
    response.begin()
  File "/usr/lib/python3.6/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/lib/python3.6/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.6/socket.py", line 586, in readinto
    return self._sock.recv_into(b)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py", line 307, in recv_into
    raise timeout('The read operation timed out')
socket.timeout: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/retry.py", line 367, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/packages/six.py", line 686, in reraise
    raise value
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 386, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 306, in _raise_timeout
    raise ReadTimeoutError(self, url, "Read timed out. (read timeout=%s)" % timeout_value)
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='king-electric.com', port=443): Read timed out. (read timeout=5)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/read_urls.py", line 50, in fix_urls
    resp = requests.get(https_url, headers=headers, timeout=timeout, verify=False)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 637, in send
    r = adapter.send(request, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 529, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='king-electric.com', port=443): Read timed out. (read timeout=5)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 384, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 380, in _make_request
    httplib_response = conn.getresponse()
  File "/usr/lib/python3.6/http/client.py", line 1331, in getresponse
    response.begin()
  File "/usr/lib/python3.6/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/lib/python3.6/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.6/socket.py", line 586, in readinto
    return self._sock.recv_into(b)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py", line 307, in recv_into
    raise timeout('The read operation timed out')
socket.timeout: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/retry.py", line 367, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/packages/six.py", line 686, in reraise
    raise value
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 386, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 306, in _raise_timeout
    raise ReadTimeoutError(self, url, "Read timed out. (read timeout=%s)" % timeout_value)
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='king-electric.com', port=443): Read timed out. (read timeout=5)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/read_urls.py", line 64, in fix_urls
    resp = requests.get(https_www_url, headers=headers, timeout=timeout, verify=False)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 659, in send
    history = [resp for resp in gen] if allow_redirects else []
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 659, in <listcomp>
    history = [resp for resp in gen] if allow_redirects else []
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 238, in resolve_redirects
    **adapter_kwargs
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 637, in send
    r = adapter.send(request, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 529, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='king-electric.com', port=443): Read timed out. (read timeout=5)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/connection.py", line 57, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
  File "/usr/lib/python3.6/socket.py", line 745, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno -5] No address associated with hostname

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/usr/lib/python3.6/http/client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1026, in _send_output
    self.send(msg)
  File "/usr/lib/python3.6/http/client.py", line 964, in send
    self.connect()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 181, in connect
    conn = self._new_conn()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f4457811438>: Failed to establish a new connection: [Errno -5] No address associated with hostname

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/retry.py", line 398, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='wet-flash.com', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4457811438>: Failed to establish a new connection: [Errno -5] No address associated with hostname',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/read_urls.py", line 22, in fix_urls
    resp = requests.get(http_url, headers=headers, timeout=timeout, verify=False)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 637, in send
    r = adapter.send(request, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='wet-flash.com', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4457811438>: Failed to establish a new connection: [Errno -5] No address associated with hostname',))
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/connection.py", line 57, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
  File "/usr/lib/python3.6/socket.py", line 745, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/usr/lib/python3.6/http/client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1026, in _send_output
    self.send(msg)
  File "/usr/lib/python3.6/http/client.py", line 964, in send
    self.connect()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 181, in connect
    conn = self._new_conn()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f44578114a8>: Failed to establish a new connection: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/retry.py", line 398, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='www.wet-flash.com', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f44578114a8>: Failed to establish a new connection: [Errno -2] Name or service not known',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/read_urls.py", line 36, in fix_urls
    resp = requests.get(http_www_url, headers=headers, timeout=timeout, verify=False)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 637, in send
    r = adapter.send(request, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='www.wet-flash.com', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f44578114a8>: Failed to establish a new connection: [Errno -2] Name or service not known',))
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/connection.py", line 57, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
  File "/usr/lib/python3.6/socket.py", line 745, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno -5] No address associated with hostname

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 343, in _make_request
    self._validate_conn(conn)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 839, in _validate_conn
    conn.connect()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 301, in connect
    conn = self._new_conn()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.VerifiedHTTPSConnection object at 0x7f4457811dd8>: Failed to establish a new connection: [Errno -5] No address associated with hostname

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/retry.py", line 398, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='wet-flash.com', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f4457811dd8>: Failed to establish a new connection: [Errno -5] No address associated with hostname',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/read_urls.py", line 50, in fix_urls
    resp = requests.get(https_url, headers=headers, timeout=timeout, verify=False)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 637, in send
    r = adapter.send(request, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='wet-flash.com', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f4457811dd8>: Failed to establish a new connection: [Errno -5] No address associated with hostname',))
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/connection.py", line 57, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
  File "/usr/lib/python3.6/socket.py", line 745, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 343, in _make_request
    self._validate_conn(conn)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 839, in _validate_conn
    conn.connect()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 301, in connect
    conn = self._new_conn()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.VerifiedHTTPSConnection object at 0x7f44578114e0>: Failed to establish a new connection: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/retry.py", line 398, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.wet-flash.com', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f44578114e0>: Failed to establish a new connection: [Errno -2] Name or service not known',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/read_urls.py", line 64, in fix_urls
    resp = requests.get(https_www_url, headers=headers, timeout=timeout, verify=False)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 637, in send
    r = adapter.send(request, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.wet-flash.com', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f44578114e0>: Failed to establish a new connection: [Errno -2] Name or service not known',))
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 384, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 380, in _make_request
    httplib_response = conn.getresponse()
  File "/usr/lib/python3.6/http/client.py", line 1331, in getresponse
    response.begin()
  File "/usr/lib/python3.6/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/lib/python3.6/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.6/socket.py", line 586, in readinto
    return self._sock.recv_into(b)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/retry.py", line 367, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/packages/six.py", line 686, in reraise
    raise value
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 386, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 306, in _raise_timeout
    raise ReadTimeoutError(self, url, "Read timed out. (read timeout=%s)" % timeout_value)
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='tsi-inc.net', port=80): Read timed out. (read timeout=5)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/read_urls.py", line 22, in fix_urls
    resp = requests.get(http_url, headers=headers, timeout=timeout, verify=False)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 637, in send
    r = adapter.send(request, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 529, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: HTTPConnectionPool(host='tsi-inc.net', port=80): Read timed out. (read timeout=5)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 384, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 380, in _make_request
    httplib_response = conn.getresponse()
  File "/usr/lib/python3.6/http/client.py", line 1331, in getresponse
    response.begin()
  File "/usr/lib/python3.6/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/lib/python3.6/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.6/socket.py", line 586, in readinto
    return self._sock.recv_into(b)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/retry.py", line 367, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/packages/six.py", line 686, in reraise
    raise value
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 386, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 306, in _raise_timeout
    raise ReadTimeoutError(self, url, "Read timed out. (read timeout=%s)" % timeout_value)
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='tsi-inc.net', port=80): Read timed out. (read timeout=5)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/read_urls.py", line 36, in fix_urls
    resp = requests.get(http_www_url, headers=headers, timeout=timeout, verify=False)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 659, in send
    history = [resp for resp in gen] if allow_redirects else []
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 659, in <listcomp>
    history = [resp for resp in gen] if allow_redirects else []
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 238, in resolve_redirects
    **adapter_kwargs
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 637, in send
    r = adapter.send(request, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 529, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: HTTPConnectionPool(host='tsi-inc.net', port=80): Read timed out. (read timeout=5)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py", line 453, in wrap_socket
    cnx.do_handshake()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/OpenSSL/SSL.py", line 1907, in do_handshake
    self._raise_ssl_error(self._ssl, result)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/OpenSSL/SSL.py", line 1639, in _raise_ssl_error
    _raise_current_error()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/OpenSSL/_util.py", line 54, in exception_from_error_queue
    raise exception_type(errors)
OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'tlsv1 alert internal error')]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 343, in _make_request
    self._validate_conn(conn)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 839, in _validate_conn
    conn.connect()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 344, in connect
    ssl_context=context)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/ssl_.py", line 344, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py", line 459, in wrap_socket
    raise ssl.SSLError('bad handshake: %r' % e)
ssl.SSLError: ("bad handshake: Error([('SSL routines', 'ssl3_read_bytes', 'tlsv1 alert internal error')],)",)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/retry.py", line 398, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='tsi-inc.net', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError("bad handshake: Error([('SSL routines', 'ssl3_read_bytes', 'tlsv1 alert internal error')],)",),))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/read_urls.py", line 50, in fix_urls
    resp = requests.get(https_url, headers=headers, timeout=timeout, verify=False)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 637, in send
    r = adapter.send(request, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 514, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='tsi-inc.net', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError("bad handshake: Error([('SSL routines', 'ssl3_read_bytes', 'tlsv1 alert internal error')],)",),))
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py", line 453, in wrap_socket
    cnx.do_handshake()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/OpenSSL/SSL.py", line 1907, in do_handshake
    self._raise_ssl_error(self._ssl, result)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/OpenSSL/SSL.py", line 1639, in _raise_ssl_error
    _raise_current_error()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/OpenSSL/_util.py", line 54, in exception_from_error_queue
    raise exception_type(errors)
OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'tlsv1 alert internal error')]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 343, in _make_request
    self._validate_conn(conn)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 839, in _validate_conn
    conn.connect()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 344, in connect
    ssl_context=context)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/ssl_.py", line 344, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py", line 459, in wrap_socket
    raise ssl.SSLError('bad handshake: %r' % e)
ssl.SSLError: ("bad handshake: Error([('SSL routines', 'ssl3_read_bytes', 'tlsv1 alert internal error')],)",)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/retry.py", line 398, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.tsi-inc.net', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError("bad handshake: Error([('SSL routines', 'ssl3_read_bytes', 'tlsv1 alert internal error')],)",),))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/read_urls.py", line 64, in fix_urls
    resp = requests.get(https_www_url, headers=headers, timeout=timeout, verify=False)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 637, in send
    r = adapter.send(request, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 514, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='www.tsi-inc.net', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError("bad handshake: Error([('SSL routines', 'ssl3_read_bytes', 'tlsv1 alert internal error')],)",),))
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/connection.py", line 80, in create_connection
    raise err
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 343, in _make_request
    self._validate_conn(conn)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 839, in _validate_conn
    conn.connect()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 301, in connect
    conn = self._new_conn()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.VerifiedHTTPSConnection object at 0x7f4457824f98>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/retry.py", line 398, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='trci.net', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f4457824f98>: Failed to establish a new connection: [Errno 111] Connection refused',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/read_urls.py", line 50, in fix_urls
    resp = requests.get(https_url, headers=headers, timeout=timeout, verify=False)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 637, in send
    r = adapter.send(request, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='trci.net', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f4457824f98>: Failed to establish a new connection: [Errno 111] Connection refused',))
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/connection.py", line 80, in create_connection
    raise err
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 343, in _make_request
    self._validate_conn(conn)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 839, in _validate_conn
    conn.connect()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 301, in connect
    conn = self._new_conn()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.VerifiedHTTPSConnection object at 0x7f4457824860>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/retry.py", line 398, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.trci.net', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f4457824860>: Failed to establish a new connection: [Errno 111] Connection refused',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/read_urls.py", line 64, in fix_urls
    resp = requests.get(https_www_url, headers=headers, timeout=timeout, verify=False)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 637, in send
    r = adapter.send(request, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.trci.net', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f4457824860>: Failed to establish a new connection: [Errno 111] Connection refused',))
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/connection.py", line 80, in create_connection
    raise err
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/connection.py", line 70, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/usr/lib/python3.6/http/client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1026, in _send_output
    self.send(msg)
  File "/usr/lib/python3.6/http/client.py", line 964, in send
    self.connect()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 181, in connect
    conn = self._new_conn()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 164, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x7f44578116a0>, 'Connection to yedarnd.com timed out. (connect timeout=5)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/retry.py", line 398, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='yedarnd.com', port=80): Max retries exceeded with url: /Overview (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f44578116a0>, 'Connection to yedarnd.com timed out. (connect timeout=5)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/read_urls.py", line 22, in fix_urls
    resp = requests.get(http_url, headers=headers, timeout=timeout, verify=False)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 637, in send
    r = adapter.send(request, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 504, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='yedarnd.com', port=80): Max retries exceeded with url: /Overview (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f44578116a0>, 'Connection to yedarnd.com timed out. (connect timeout=5)'))
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/connection.py", line 80, in create_connection
    raise err
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/connection.py", line 70, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/usr/lib/python3.6/http/client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1026, in _send_output
    self.send(msg)
  File "/usr/lib/python3.6/http/client.py", line 964, in send
    self.connect()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 181, in connect
    conn = self._new_conn()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 164, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x7f44577c3470>, 'Connection to www.yedarnd.com timed out. (connect timeout=5)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/retry.py", line 398, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='www.yedarnd.com', port=80): Max retries exceeded with url: /Overview (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f44577c3470>, 'Connection to www.yedarnd.com timed out. (connect timeout=5)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/read_urls.py", line 36, in fix_urls
    resp = requests.get(http_www_url, headers=headers, timeout=timeout, verify=False)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 637, in send
    r = adapter.send(request, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 504, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='www.yedarnd.com', port=80): Max retries exceeded with url: /Overview (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f44577c3470>, 'Connection to www.yedarnd.com timed out. (connect timeout=5)'))
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/connection.py", line 80, in create_connection
    raise err
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/connection.py", line 70, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 343, in _make_request
    self._validate_conn(conn)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 839, in _validate_conn
    conn.connect()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 301, in connect
    conn = self._new_conn()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 164, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.VerifiedHTTPSConnection object at 0x7f445781dbe0>, 'Connection to yedarnd.com timed out. (connect timeout=5)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/retry.py", line 398, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='yedarnd.com', port=443): Max retries exceeded with url: /Overview (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x7f445781dbe0>, 'Connection to yedarnd.com timed out. (connect timeout=5)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/read_urls.py", line 50, in fix_urls
    resp = requests.get(https_url, headers=headers, timeout=timeout, verify=False)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 637, in send
    r = adapter.send(request, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 504, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='yedarnd.com', port=443): Max retries exceeded with url: /Overview (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x7f445781dbe0>, 'Connection to yedarnd.com timed out. (connect timeout=5)'))
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/connection.py", line 80, in create_connection
    raise err
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/connection.py", line 70, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 343, in _make_request
    self._validate_conn(conn)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 839, in _validate_conn
    conn.connect()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 301, in connect
    conn = self._new_conn()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 164, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.VerifiedHTTPSConnection object at 0x7f445781d860>, 'Connection to www.yedarnd.com timed out. (connect timeout=5)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/retry.py", line 398, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.yedarnd.com', port=443): Max retries exceeded with url: /Overview (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x7f445781d860>, 'Connection to www.yedarnd.com timed out. (connect timeout=5)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/read_urls.py", line 64, in fix_urls
    resp = requests.get(https_www_url, headers=headers, timeout=timeout, verify=False)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 637, in send
    r = adapter.send(request, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 504, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='www.yedarnd.com', port=443): Max retries exceeded with url: /Overview (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x7f445781d860>, 'Connection to www.yedarnd.com timed out. (connect timeout=5)'))
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/connection.py", line 57, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
  File "/usr/lib/python3.6/socket.py", line 745, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno -5] No address associated with hostname

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/usr/lib/python3.6/http/client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1026, in _send_output
    self.send(msg)
  File "/usr/lib/python3.6/http/client.py", line 964, in send
    self.connect()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 181, in connect
    conn = self._new_conn()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
Checking url for King Electric Vehicles Inc.
	Trying: http://king-electric.com/
HTTPConnectionPool(host='king-electric.com', port=80): Read timed out. (read timeout=5)
	Trying: http://www.king-electric.com/
HTTPSConnectionPool(host='king-electric.com', port=443): Read timed out. (read timeout=5)
	Trying: https://king-electric.com/
HTTPSConnectionPool(host='king-electric.com', port=443): Read timed out. (read timeout=5)
	Trying: https://www.king-electric.com/
HTTPSConnectionPool(host='king-electric.com', port=443): Read timed out. (read timeout=5)
Checking url for NanoGram Corporation
	Trying: http://nanogram.com
	Status code: 200
Checking url for Pharmatrophix
	Trying: http://pharmatrophix.com/
	Status code: 200
Checking url for Sun Synchrony
	Trying: http://sunsynchrony.com/
	Status code: 200
Checking url for True-Safe Technologies
	Trying: http://true-safe.com/
	Status code: 200
Checking url for Building Envelope Innovations
	Trying: http://wet-flash.com/
HTTPConnectionPool(host='wet-flash.com', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4457811438>: Failed to establish a new connection: [Errno -5] No address associated with hostname',))
	Trying: http://www.wet-flash.com/
HTTPConnectionPool(host='www.wet-flash.com', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f44578114a8>: Failed to establish a new connection: [Errno -2] Name or service not known',))
	Trying: https://wet-flash.com/
HTTPSConnectionPool(host='wet-flash.com', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f4457811dd8>: Failed to establish a new connection: [Errno -5] No address associated with hostname',))
	Trying: https://www.wet-flash.com/
HTTPSConnectionPool(host='www.wet-flash.com', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f44578114e0>: Failed to establish a new connection: [Errno -2] Name or service not known',))
Checking url for ADMA Products
	Trying: http://admaproducts.com/#!
	Status code: 200
Checking url for Enginuity Worldwide
	Trying: http://enginuityww.webstarts.com/
	Status code: 200
Checking url for Morpho
	Trying: http://morpho.com/en
	Status code: 200
Checking url for Suganit Systems
	Trying: http://suganit.com/
	Status code: 200
Checking url for NOVA PANGAEA TECHNOLOGIES LIMITED
	Trying: http://novapangaea.com/
	Status code: 200
Checking url for Narsys
	Trying: http://narsystech.com/
	Status code: 200
Checking url for Pardev
	Trying: http://pardev.com/
	Status code: 200
Checking url for Lumenco
	Trying: http://en.lumenco.ca/
	Status code: 200
Checking url for Teal Sales Incorporated
	Trying: http://tsi-inc.net
HTTPConnectionPool(host='tsi-inc.net', port=80): Read timed out. (read timeout=5)
	Trying: http://www.tsi-inc.net
HTTPConnectionPool(host='tsi-inc.net', port=80): Read timed out. (read timeout=5)
	Trying: https://tsi-inc.net
HTTPSConnectionPool(host='tsi-inc.net', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError("bad handshake: Error([('SSL routines', 'ssl3_read_bytes', 'tlsv1 alert internal error')],)",),))
	Trying: https://www.tsi-inc.net
HTTPSConnectionPool(host='www.tsi-inc.net', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError("bad handshake: Error([('SSL routines', 'ssl3_read_bytes', 'tlsv1 alert internal error')],)",),))
Checking url for Polysar Corporation
	Trying: http://polystar.com/
	Status code: 200
Checking url for The Iams Company
	Trying: http://iams.com/
	Status code: 200
Checking url for TECHNOLOGY RESEARCH
	Trying: http://trci.net/
	Status code: 503
	Trying: http://www.trci.net/
	Status code: 503
	Trying: https://trci.net/
HTTPSConnectionPool(host='trci.net', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f4457824f98>: Failed to establish a new connection: [Errno 111] Connection refused',))
	Trying: https://www.trci.net/
HTTPSConnectionPool(host='www.trci.net', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f4457824860>: Failed to establish a new connection: [Errno 111] Connection refused',))
Checking url for GIGA SOLAR FPC
	Trying: http://gigasolarpv.com/
	Status code: 200
Checking url for Aurora Alage
	Trying: http://aurorabiofuels.com/
	Status code: 200
Checking url for INENTEC INC.
	Trying: http://inentec.com/
	Status code: 200
Checking url for PNM
	Trying: http://pnm.com/
	Status code: 200
Checking url for Tesla Nanocoatings
	Trying: http://teslanano.com/
	Status code: 200
Checking url for ESCAPE THERAPEUTICS
	Trying: http://escapetherapeutics.com/
	Status code: 200
Checking url for Cima NanoTech Israel Ltd.
	Trying: http://cimananotech.com/
	Status code: 200
Checking url for Tanaka Kikinzoku Kogyo K.K.
	Trying: http://gold.tanaka.co.jp/english/
	Status code: 200
Checking url for Pyrexar Medical Inc.
	Trying: http://pyrexar.com/
	Status code: 200
Checking url for Sirnaomics
	Trying: http://new.sirnaomics.com/
	Status code: 200
Checking url for Swagelok Company
	Trying: http://swagelok.com/
	Status code: 200
Checking url for PELLION TECHNOLOGIES
	Trying: http://pelliontech.com/
	Status code: 200
Checking url for Traex Corporation
	Trying: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm
	Status code: 200
Checking url for Yeda Research and Development Co.
	Trying: http://yedarnd.com/Overview
HTTPConnectionPool(host='yedarnd.com', port=80): Max retries exceeded with url: /Overview (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f44578116a0>, 'Connection to yedarnd.com timed out. (connect timeout=5)'))
	Trying: http://www.yedarnd.com/Overview
HTTPConnectionPool(host='www.yedarnd.com', port=80): Max retries exceeded with url: /Overview (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f44577c3470>, 'Connection to www.yedarnd.com timed out. (connect timeout=5)'))
	Trying: https://yedarnd.com/Overview
HTTPSConnectionPool(host='yedarnd.com', port=443): Max retries exceeded with url: /Overview (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x7f445781dbe0>, 'Connection to yedarnd.com timed out. (connect timeout=5)'))
	Trying: https://www.yedarnd.com/Overview
HTTPSConnectionPool(host='www.yedarnd.com', port=443): Max retries exceeded with url: /Overview (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x7f445781d860>, 'Connection to www.yedarnd.com timed out. (connect timeout=5)'))
Checking url for Bisco
	Trying: http://biscoind.com/
	Status code: 200
Checking url for QuantumScape Corporation
	Trying: http://quantumscape.com/
	Status code: 200
Checking url for Galemed Corporation
	Trying: http://galemed.com/manager/eService/RC08-all_0225.pdf
	Status code: 200
Checking url for Opel Solar
	Trying: http://opelinc.com/
	Status code: 200
Checking url for K.S. INTERNATIONAL CO.
	Trying: http://ksandco.com/
	Status code: 200
Checking url for Locus Energy
	Trying: http://locusenergy.com/
	Status code: 200
Checking url for SOLAZYME
	Trying: http://solazyme.com/
	Status code: 200
Checking url for Chipmos Technologies Inc.
	Trying: http://chipmos.com/index-en.aspx
HTTPConnectionPool(host='chipmos.com', port=80): Max retries exceeded with url: /index-en.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4457811940>: Failed to establish a new connection: [Errno -5] No address associated with hostname',))
	Trying: http://www.chipmos.com/index-en.aspx
	Status code: 200
Checking url for TheGreenCell
	Trying: http://thegreencell.com/
	Status code: 200
Checking url for Shimadzu Corporation
	Trying: http://shimadzu.com/
	Status code: 200
Checking url for Intellectual Keystone Technology LLC
	Trying: http://ikt.us.com/
	Status code: 200
Checking url for Ostendo Technologies
	Trying: http://ostendo.com/
	Status code: 200
Checking url for Nestec S. A.
	Trying: http://nestle.com/
	Status code: 200
Checking url for Cook Biotech Incorporated
	Trying: http://cookbiotech.com/
	Status code: 200
Checking url for Kajima Corporation
	Trying: http://kajimausa.com/
	Status code: 200
Checking url for Sysmex Corporation
	Trying: http://sysmex.com/Pages/default.aspx
	Status code: 200
Checking url for GlassPoint Solar
	Trying: http://glasspoint.com/
	Status code: 200
Checking url for Ajinomoto Althea
	Trying: http://ajibio-pharma.com/
	Status code: 200
Checking url for Achushnet Company
	Trying: http://acushnetgolf.com/
	Status code: 200
Checking url for Biogenic Reagents Ventures
	Trying: http://biogenicreagents.com/
	Status code: 200
Checking url for Clean Energy Labs
	Trying: http://cleanenergylabs.com/
	Status code: 200
Checking url for IntriEnergy Inc.
	Trying: http://intrienergy.com/#!
	Status code: 200
Checking url for NANOPHTHALMICS
	Trying: http://nanophthalmics.com/
	Status code: 200
Checking url for Novus Technology
	Trying: http://novustek.net/
	Status code: 200
Checking url for Chevron Oronitz Company LLC
	Trying: http://oronite.com/
	Status code: 200
[{'domain': 'nanogram.com',
  'firm_name': 'NanoGram Corporation',
  'url': 'http://nanogram.com/'},
 {'domain': 'pharmatrophix.com',
  'firm_name': 'Pharmatrophix',
  'url': 'http://pharmatrophix.com/'},
 {'domain': 'sunsynchrony.com',
  'firm_name': 'Sun Synchrony',
  'url': 'http://sunsynchrony.com/'},
 {'domain': 'www.fiatest.com',
  'firm_name': 'True-Safe Technologies',
  'url': 'https://www.fiatest.com/'},
 {'domain': 'www.admaproducts.com',
  'firm_name': 'ADMA Products',
  'url': 'https://www.admaproducts.com/#!'},
 {'domain': 'enginuityww.webstarts.com',
  'firm_name': 'Enginuity Worldwide',
  'url': 'http://enginuityww.webstarts.com/'},
 {'domain': 'morpho.com',
  'firm_name': 'Morpho',
  'url': 'https://morpho.com/en'},
 {'domain': 'suganit.com',
  'firm_name': 'Suganit Systems',
  'url': 'http://suganit.com/'},
 {'domain': 'www.novapangaea.com',
  'firm_name': 'NOVA PANGAEA TECHNOLOGIES LIMITED',
  'url': 'https://www.novapangaea.com/'},
 {'domain': 'narsystech.com',
  'firm_name': 'Narsys',
  'url': 'http://narsystech.com/'},
 {'domain': 'pardev.com', 'firm_name': 'Pardev', 'url': 'http://pardev.com/'},
 {'domain': 'en.lumenco.ca',
  'firm_name': 'Lumenco',
  'url': 'https://en.lumenco.ca/'},
 {'domain': 'www.polystar.com',
  'firm_name': 'Polysar Corporation',
  'url': 'https://www.polystar.com/'},
 {'domain': 'iams.com',
  'firm_name': 'The Iams Company',
  'url': 'http://iams.com/'},
 {'domain': 'www.gigasolarpv.com',
  'firm_name': 'GIGA SOLAR FPC',
  'url': 'https://www.gigasolarpv.com/'},
 {'domain': 'www.aurorabiofuels.com',
  'firm_name': 'Aurora Alage',
  'url': 'http://www.aurorabiofuels.com/'},
 {'domain': 'inentec.com',
  'firm_name': 'INENTEC INC.',
  'url': 'http://inentec.com/'},
 {'domain': 'WWW.PNM.COM', 'firm_name': 'PNM', 'url': 'https://WWW.PNM.COM/'},
 {'domain': 'www.teslanano.com',
  'firm_name': 'Tesla Nanocoatings',
  'url': 'https://www.teslanano.com/'},
 {'domain': 'escapetherapeutics.com',
  'firm_name': 'ESCAPE THERAPEUTICS',
  'url': 'http://escapetherapeutics.com/'},
 {'domain': 'cimananotech.com',
  'firm_name': 'Cima NanoTech Israel Ltd.',
  'url': 'http://cimananotech.com/en/closure/'},
 {'domain': 'gold.tanaka.co.jp',
  'firm_name': 'Tanaka Kikinzoku Kogyo K.K.',
  'url': 'https://gold.tanaka.co.jp/english/'},
 {'domain': 'pyrexar.com',
  'firm_name': 'Pyrexar Medical Inc.',
  'url': 'https://pyrexar.com/'},
 {'domain': 'new.sirnaomics.com',
  'firm_name': 'Sirnaomics',
  'url': 'http://new.sirnaomics.com/'},
 {'domain': 'www.swagelok.com:443',
  'firm_name': 'Swagelok Company',
  'url': 'https://www.swagelok.com:443/en'},
 {'domain': 'www.pelliontech.com',
  'firm_name': 'PELLION TECHNOLOGIES',
  'url': 'http://www.pelliontech.com/'},
 {'domain': 'vollrath.com',
  'firm_name': 'Traex Corporation',
  'url': 'http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm'},
 {'domain': 'www.biscoind.com',
  'firm_name': 'Bisco',
  'url': 'https://www.biscoind.com/'},
 {'domain': 'www.quantumscape.com',
  'firm_name': 'QuantumScape Corporation',
  'url': 'http://www.quantumscape.com/'},
 {'domain': 'galemed.com',
  'firm_name': 'Galemed Corporation',
  'url': 'http://galemed.com/manager/eService/RC08-all_0225.pdf'},
 {'domain': 'opelinc.com',
  'firm_name': 'Opel Solar',
  'url': 'http://opelinc.com/'},
 {'domain': 'ksandco.com',
  'firm_name': 'K.S. INTERNATIONAL CO.',
  'url': 'http://ksandco.com/'},
 {'domain': 'locusenergy.com:443',
  'firm_name': 'Locus Energy',
  'url': 'https://locusenergy.com:443/'},
 {'domain': 'solazyme.com',
  'firm_name': 'SOLAZYME',
  'url': 'http://solazyme.com/'},
 {'domain': 'www.chipmos.com',
  'firm_name': 'Chipmos Technologies Inc.',
  'url': 'https://www.chipmos.com/index-en.aspx'},
 {'domain': 'thegreencell.com',
  'firm_name': 'TheGreenCell',
  'url': 'http://thegreencell.com/'},
 {'domain': 'www.shimadzu.com',
  'firm_name': 'Shimadzu Corporation',
  'url': 'https://www.shimadzu.com/'},
 {'domain': 'ikt.us.com',
  'firm_name': 'Intellectual Keystone Technology LLC',
  'url': 'http://ikt.us.com/'},
 {'domain': 'www.ostendo.com',
  'firm_name': 'Ostendo Technologies',
  'url': 'https://www.ostendo.com/'},
 {'domain': 'www.nestle.com',
  'firm_name': 'Nestec S. A.',
  'url': 'http://www.nestle.com/'},
 {'domain': 'www.cookbiotech.com',
  'firm_name': 'Cook Biotech Incorporated',
  'url': 'https://www.cookbiotech.com/'},
 {'domain': 'kajimausa.com',
  'firm_name': 'Kajima Corporation',
  'url': 'https://kajimausa.com/'},
 {'domain': 'www.sysmex.com',
  'firm_name': 'Sysmex Corporation',
  'url': 'https://www.sysmex.com/Pages/default.aspx'},
 {'domain': 'glasspoint.com',
  'firm_name': 'GlassPoint Solar',
  'url': 'https://glasspoint.com/'},
 {'domain': 'ajibio-pharma.com',
  'firm_name': 'Ajinomoto Althea',
  'url': 'https://ajibio-pharma.com/'},
 {'domain': 'www.acushnetgolf.com',
  'firm_name': 'Achushnet Company',
  'url': 'https://www.acushnetgolf.com/'},
 {'domain': 'biogenicreagents.com',
  'firm_name': 'Biogenic Reagents Ventures',
  'url': 'http://biogenicreagents.com/'},
 {'domain': 'cleanenergylabs.com',
  'firm_name': 'Clean Energy Labs',
  'url': 'http://cleanenergylabs.com/'},
 {'domain': 'www.intrienergy.com',
  'firm_name': 'IntriEnergy Inc.',
  'url': 'http://www.intrienergy.com/#!'},
 {'domain': 'nanophthalmics.com',
  'firm_name': 'NANOPHTHALMICS',
  'url': 'http://nanophthalmics.com/'},
 {'domain': 'www.novustek.net',
  'firm_name': 'Novus Technology',
  'url': 'https://www.novustek.net'},
 {'domain': 'www.oronite.com',
  'firm_name': 'Chevron Oronitz Company LLC',
  'url': 'https://www.oronite.com/'}]
Missing King Electric Vehicles Inc., Yeda Research and Development Co., TECHNOLOGY RESEARCH, Building Envelope Innovations, Teal Sales Incorporated in the fixed urls list
Pages to scrape: ['http://nanogram.com/', 'http://pharmatrophix.com/', 'http://sunsynchrony.com/', 'https://www.fiatest.com/', 'https://www.admaproducts.com/#!', 'http://enginuityww.webstarts.com/', 'https://morpho.com/en', 'http://suganit.com/', 'https://www.novapangaea.com/', 'http://narsystech.com/', 'http://pardev.com/', 'https://en.lumenco.ca/', 'https://www.polystar.com/', 'http://iams.com/', 'https://www.gigasolarpv.com/', 'http://www.aurorabiofuels.com/', 'http://inentec.com/', 'https://WWW.PNM.COM/', 'https://www.teslanano.com/', 'http://escapetherapeutics.com/', 'http://cimananotech.com/en/closure/', 'https://gold.tanaka.co.jp/english/', 'https://pyrexar.com/', 'http://new.sirnaomics.com/', 'https://www.swagelok.com:443/en', 'http://www.pelliontech.com/', 'http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm', 'https://www.biscoind.com/', 'http://www.quantumscape.com/', 'http://galemed.com/manager/eService/RC08-all_0225.pdf', 'http://opelinc.com/', 'http://ksandco.com/', 'https://locusenergy.com:443/', 'http://solazyme.com/', 'https://www.chipmos.com/index-en.aspx', 'http://thegreencell.com/', 'https://www.shimadzu.com/', 'http://ikt.us.com/', 'https://www.ostendo.com/', 'http://www.nestle.com/', 'https://www.cookbiotech.com/', 'https://kajimausa.com/', 'https://www.sysmex.com/Pages/default.aspx', 'https://glasspoint.com/', 'https://ajibio-pharma.com/', 'https://www.acushnetgolf.com/', 'http://biogenicreagents.com/', 'http://cleanenergylabs.com/', 'http://www.intrienergy.com/#!', 'http://nanophthalmics.com/', 'https://www.novustek.net', 'https://www.oronite.com/']urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f4457811940>: Failed to establish a new connection: [Errno -5] No address associated with hostname

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/retry.py", line 398, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='chipmos.com', port=80): Max retries exceeded with url: /index-en.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4457811940>: Failed to establish a new connection: [Errno -5] No address associated with hostname',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/read_urls.py", line 22, in fix_urls
    resp = requests.get(http_url, headers=headers, timeout=timeout, verify=False)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/api.py", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py", line 637, in send
    r = adapter.send(request, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='chipmos.com', port=80): Max retries exceeded with url: /index-en.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4457811940>: Failed to establish a new connection: [Errno -5] No address associated with hostname',))
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
2018-11-14 19:18:54 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: FirmDB)
2018-11-14 19:18:54 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.6 (default, Sep 12 2018, 18:26:19) - [GCC 8.0.1 20180414 (experimental) [trunk revision 259383]], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-1027-aws-x86_64-with-Ubuntu-18.04-bionic
2018-11-14 19:18:54 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'FirmDB', 'CONCURRENT_REQUESTS_PER_DOMAIN': 4, 'DEPTH_LIMIT': '1', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'FirmDB.spiders', 'ROBOTSTXT_OBEY': 'False', 'SPIDER_MODULES': ['FirmDB.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2049.0 Safari/537.36'}
2018-11-14 19:18:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-11-14 19:18:54 [py.warnings] WARNING: /home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2018-11-14 19:18:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'random_useragent.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-11-14 19:18:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-11-14 19:18:54 [scrapy.middleware] INFO: Enabled item pipelines:
['FirmDB.pipelines.FirmDBPipeline']
2018-11-14 19:18:54 [scrapy.core.engine] INFO: Spider opened
2018-11-14 19:18:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
2018-11-14 19:24:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.admaproducts.com/> (referer: https://www.admaproducts.com/?_escaped_fragment_=)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 58, in parse_links
    browser.get(response.url)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: Timeout loading page after 300000ms

2018-11-14 19:24:14 [scrapy.extensions.logstats] INFO: Crawled 33 pages (at 33 pages/min), scraped 5 items (at 5 items/min)
2018-11-14 19:24:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.fiatest.com/environmental-allergy>: HTTP status code is not handled or not allowed
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:25:00 [scrapy.extensions.logstats] INFO: Crawled 44 pages (at 11 pages/min), scraped 17 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:26:15 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 5 pages/min), scraped 25 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:27:09 [scrapy.extensions.logstats] INFO: Crawled 55 pages (at 6 pages/min), scraped 31 items (at 6 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:27:54 [scrapy.extensions.logstats] INFO: Crawled 60 pages (at 5 pages/min), scraped 36 items (at 5 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:28:59 [scrapy.extensions.logstats] INFO: Crawled 72 pages (at 12 pages/min), scraped 44 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:30:23 [scrapy.extensions.logstats] INFO: Crawled 78 pages (at 6 pages/min), scraped 54 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:30:55 [scrapy.extensions.logstats] INFO: Crawled 89 pages (at 11 pages/min), scraped 57 items (at 3 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:32:08 [scrapy.extensions.logstats] INFO: Crawled 89 pages (at 0 pages/min), scraped 65 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:33:19 [scrapy.extensions.logstats] INFO: Crawled 98 pages (at 9 pages/min), scraped 72 items (at 7 items/min)
/bin/sh: 1: kill: No such process

2018-11-14 19:33:29 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.polystar.com/solutions/innovation/internet-of-things-iot/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:34:03 [scrapy.extensions.logstats] INFO: Crawled 106 pages (at 8 pages/min), scraped 76 items (at 4 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:35:08 [scrapy.extensions.logstats] INFO: Crawled 106 pages (at 0 pages/min), scraped 82 items (at 6 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:35:55 [scrapy.extensions.logstats] INFO: Crawled 115 pages (at 9 pages/min), scraped 87 items (at 5 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:36:59 [scrapy.extensions.logstats] INFO: Crawled 123 pages (at 8 pages/min), scraped 94 items (at 7 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:37:59 [scrapy.extensions.logstats] INFO: Crawled 131 pages (at 8 pages/min), scraped 101 items (at 7 items/min)

Crawling page: http://narsystech.com/Contact.html       valid
Crawling page: http://narsystech.com/Clients.html       valid
Crawling page: http://narsystech.com/Overview.html       valid
Crawling page: http://narsystech.com/News.html       valid
Crawling page: http://narsystech.com/index.html       valid
Crawling page: https://www.admaproducts.com/       Crawling page: http://escapetherapeutics.com/plurastem.html       valid
Crawling page: http://escapetherapeutics.com/pipeline.html       valid
Crawling page: http://escapetherapeutics.com/orthopedics.html       valid
Crawling page: http://escapetherapeutics.com/immustem.html       valid
Crawling page: http://escapetherapeutics.com/hematology.html       valid
Crawling page: http://escapetherapeutics.com/2010-november-2-1.html       valid
Crawling page: http://escapetherapeutics.com/2010-november-2-2.html       valid
Crawling page: http://escapetherapeutics.com/neuromuscular.html       valid
Crawling page: http://escapetherapeutics.com/contact.html       valid
Crawling page: http://escapetherapeutics.com/apply.html       valid
Crawling page: http://escapetherapeutics.com/2011-may-5.html       valid
Crawling page: https://en.lumenco.ca/fixtures/chandeliers/rod/       valid
Crawling page: https://en.lumenco.ca/fixtures/chandeliers/cord/       valid
Crawling page: https://en.lumenco.ca/fixtures/chandeliers/chain/       valid
Crawling page: https://en.lumenco.ca/fixtures/ceiling-lights/       valid
Crawling page: http://www.polystar.com/use-of-cookies/       valid
Crawling page: http://www.polystar.com/privacy-policy/       valid
Crawling page: https://www.polystar.com/2018/02/polystar-chosen-by-nos-to-deliver-end-to-end-network-monitoring-and-enhance-customer-care/       valid
Crawling page: https://www.polystar.com/2018/02/white-paper-unlock-new-insights-from-network-oss-data/       valid
Crawling page: https://en.lumenco.ca/commercial-lighting/light-bulbs/       valid
Crawling page: https://www.polystar.com/partners/       valid
Crawling page: https://www.polystar.com/category/awards/       valid
Crawling page: https://www.polystar.com/company/whistleblower/       valid
Crawling page: https://www.polystar.com/company/contact/       valid
Crawling page: https://www.polystar.com/company/upcoming-events/       valid
Crawling page: https://www.polystar.com/company/career/       valid
Crawling page: https://www.polystar.com/company/management/       valid
Crawling page: https://www.polystar.com/news/       valid
Crawling page: https://www.polystar.com/company/       valid
Crawling page: https://www.polystar.com/professional-services/       valid
Crawling page: https://www.polystar.com/company/our-approach-security-quality/       valid
Crawling page: https://www.polystar.com/products/solver/       valid
Crawling page: https://www.polystar.com/products/odin/       valid
Crawling page: https://www.polystar.com/products/service-enablement/       valid
Crawling page: https://www.polystar.com/products/jupiter/       valid
Crawling page: https://www.polystar.com/products/osix-export/       valid
Crawling page: https://www.polystar.com/products/osix-import/       valid
Crawling page: https://www.polystar.com/products/osix-monitoring/       valid
Crawling page: https://www.polystar.com/products/service-assurance/       valid
Crawling page: https://www.polystar.com/products/kalix/       valid
Crawling page: https://www.polystar.com/solutions/innovation/internet-of-things-iot/       valid
Crawling page: https://www.polystar.com/products/       valid
Crawling page: https://www.polystar.com/solutions/innovation/monitoring-in-the-ran/       valid
Crawling page: https://www.polystar.com/solutions/innovation/capitalising-on-machine-learning-and-predictive-analytics/       valid
Crawling page: https://en.lumenco.ca/contact-us/       valid
Crawling page: https://www.polystar.com/solutions/executive-insights/       valid
Crawling page: https://www.polystar.com/solutions/executive-insights/cxo-insights/       valid
Crawling page: https://www.polystar.com/solutions/innovation/noc-to-soc/       valid
Crawling page: https://www.polystar.com/solutions/network-performance-insights/roaming-insights/       valid
Crawling page: https://en.lumenco.ca/about-us/       valid
Crawling page: https://en.lumenco.ca/faq-lighting/       valid
Crawling page: https://en.lumenco.ca/blog/       valid
Crawling page: https://en.lumenco.ca/shipping-rates/       valid
Crawling page: https://www.polystar.com/solutions/network-performance-insights/       valid
Crawling page: https://www.polystar.com/solutions/business-insights/marketing-insights/       valid
Crawling page: https://en.lumenco.ca/canarm-iwf677a02bn-l.html       valid
Crawling page: https://en.lumenco.ca/actualites/recessed-lighting-calculator/       valid
Crawling page: https://www.polystar.com/solutions/business-insights/sales-insights/       valid
Crawling page: https://en.lumenco.ca/canarm-iwf673a01bk-l.html       valid
Crawling page: https://www.polystar.com/solutions/business-insights/customer-care-insights/       valid
Crawling page: https://www.polystar.com/       valid
Crawling page: https://www.polystar.com/solutions/       valid
Crawling page: https://en.lumenco.ca/nuvo-lighting-60-103.html       valid
Crawling page: https://en.lumenco.ca/nuvo-lighting-60-102.html       valid
Crawling page: https://www.polystar.com/solutions/business-insights/       valid
Crawling page: https://en.lumenco.ca/nuvo-lighting-60-043.html       valid
Crawling page: https://en.lumenco.ca/nuvo-lighting-60-1026.html       valid
Crawling page: https://en.lumenco.ca/canarm-iol320wh.html       valid
Crawling page: https://en.lumenco.ca/canarm-ipl454b01orb-l.html       valid
Crawling page: https://en.lumenco.ca/canarm-lcw134a01bn-l.html       valid
Crawling page: https://en.lumenco.ca/eiko-led11wbr30-830k-dim-g5.html       valid
Crawling page: https://en.lumenco.ca/canarm-icw580a01whc10.html       valid
Crawling page: https://en.lumenco.ca/canarm-led-sw3p-bn-c.html       valid
Crawling page: https://en.lumenco.ca/canarm-ich629a05bk22.html       valid
Crawling page: https://en.lumenco.ca/shop-by-room/dining-room-lighting/       valid
Crawling page: https://en.lumenco.ca/shop-by-room/bathroom-lighting/       valid
Crawling page: https://en.lumenco.ca/shop-by-room/bedroom-lighting/       valid
Crawling page: https://en.lumenco.ca/online-sales-policies/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/led-corn-bulbs/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/high-bay-low-bay-retrofit-lamps/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/recessed-lights/recessed-led/low-profile-models/       valid
Crawling page: https://en.lumenco.ca/shop-by-room/kitchen-lighting/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/grow-lighting/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/canopy-fixtures/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/solar-powered-lights-autonomous-area-lighting/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/shop-lights/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/security-lighting/post-light-bollards/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/security-lighting/flood-lights/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/wall-packs/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/security-lighting/wall-packs/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/vapor-tight-fixtures/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/security-lighting/led-barn-lights-dawn-to-dusk/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/security-lighting/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/flood-lights/led-floods-lights/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/flood-lights/rgb-led-flood-lights/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/flood-lights/low-profile-led-flood-lights/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/flood-lights//bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:39:01 [scrapy.extensions.logstats] INFO: Crawled 136 pages (at 5 pages/min), scraped 108 items (at 7 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:40:12 [scrapy.extensions.logstats] INFO: Crawled 140 pages (at 4 pages/min), scraped 116 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:41:13 [scrapy.extensions.logstats] INFO: Crawled 152 pages (at 12 pages/min), scraped 124 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:41:56 [scrapy.extensions.logstats] INFO: Crawled 152 pages (at 0 pages/min), scraped 128 items (at 4 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:42:57 [scrapy.extensions.logstats] INFO: Crawled 160 pages (at 8 pages/min), scraped 136 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:43:55 [scrapy.extensions.logstats] INFO: Crawled 167 pages (at 7 pages/min), scraped 142 items (at 6 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:45:09 [scrapy.extensions.logstats] INFO: Crawled 177 pages (at 10 pages/min), scraped 151 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:45:25 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://pyrexar.com/?Itemid=185>: HTTP status code is not handled or not allowed
2018-11-14 19:45:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/privacy-policy> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 19:45:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/pyrexar-signs-with-viet-nam> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:46:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/pyrexar-adds-thailand-distributor> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 19:46:21 [scrapy.extensions.logstats] INFO: Crawled 192 pages (at 15 pages/min), scraped 157 items (at 6 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:46:55 [scrapy.extensions.logstats] INFO: Crawled 192 pages (at 0 pages/min), scraped 161 items (at 4 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:47:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/about-us/how-to-buy> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 19:47:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/about-us/distributor-login> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 19:47:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/about-us/contact-us> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 19:47:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/about-us/in-the-news> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 19:47:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.pyrexar.com/hyperthermia/bsd-2000-3d-mr> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 19:47:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/about-us/featured-event> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:48:22 [scrapy.extensions.logstats] INFO: Crawled 201 pages (at 9 pages/min), scraped 167 items (at 6 items/min)
/bin/sh: 1: kill: No such process

2018-11-14 19:48:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/clinical/by-indication> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 19:48:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://morpho.com/en/block/press_release_block_instance_2x1>: HTTP status code is not handled or not allowed
2018-11-14 19:48:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/knowledgebase/hyperthermia-faq> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 19:48:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/knowledgebase/organizations> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 19:48:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/knowledgebase/pyrexar-video> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:49:19 [scrapy.extensions.logstats] INFO: Crawled 217 pages (at 16 pages/min), scraped 171 items (at 4 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:49:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/clinical/clinical-trials> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:50:06 [scrapy.extensions.logstats] INFO: Crawled 222 pages (at 5 pages/min), scraped 177 items (at 6 items/min)
2018-11-14 19:50:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/hyperthermia/bsd-2000-3d-mr> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 19:50:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/clinical/bsd-2000-clinical-studies> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 19:50:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/clinical> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/marine-lights/port-harbour/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/marine-lights/solar-lantern/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/marine-lights/lanterns/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/street-parking/efficiency/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/street-parking/voltage-12v-24v/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/street-parking/street-fixture/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/marine-lights/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/high-bay-lighting/high-bay-retrofit-lamps/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/high-bay-lighting/fluorescent-high-bay-fixtures/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/street-parking/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/high-bay-lighting/compact-led/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/high-bay-lighting/standard-led/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/high-bay-lighting/led-panel/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/high-bay-lighting/led-linear/       valid
Crawling page: https://en.lumenco.ca/commercial-lighting/commercial-ceiling-fans/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/       valid
Crawling page: https://morpho.com/en/legal-notes-privacy-policy       valid
Crawling page: https://morpho.com/en/sitemap       valid
Crawling page: https://en.lumenco.ca/commercial-lighting/architectural-lighting/led-extrusions/       valid
Crawling page: https://en.lumenco.ca/commercial-lighting/architectural-lighting/led-strip-lighting/       valid
Crawling page: https://en.lumenco.ca/commercial-lighting/architectural-lighting/sign-lighting/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/high-bay-lighting/       valid
Crawling page: https://en.lumenco.ca/commercial-lighting/architectural-lighting/led-flood-lights/       valid
Crawling page: https://en.lumenco.ca/commercial-lighting/architectural-lighting/wallwashers/       valid
Crawling page: https://en.lumenco.ca/commercial-lighting/architectural-lighting/       valid
Crawling page: https://en.lumenco.ca/commercial-lighting/decorative-lighting/       valid
Crawling page: https://en.lumenco.ca/commercial-lighting/high-bay-low-bay-lighting/       valid
Crawling page: https://en.lumenco.ca/commercial-lighting/led-panel-lights/       valid
Crawling page: https://en.lumenco.ca/commercial-lighting/wraparound/fluorescent-wraparound/       valid
Crawling page: https://en.lumenco.ca/commercial-lighting/wraparound/led-wraparound/       valid
Crawling page: https://en.lumenco.ca/commercial-lighting/wraparound/       valid
Crawling page: https://en.lumenco.ca/commercial-lighting/recessed-lights/       valid
Crawling page: https://en.lumenco.ca/commercial-lighting/suspended-linear-lights/       valid
Crawling page: https://en.lumenco.ca/commercial-lighting/troffers/       valid
Crawling page: https://en.lumenco.ca/commercial-lighting/       valid
Crawling page: https://en.lumenco.ca/fixtures/ceiling-fans/accessoires/       valid
Crawling page: https://en.lumenco.ca/fixtures/ceiling-fans/commercial-ceiling-fans/       valid
Crawling page: https://en.lumenco.ca/fixtures/ceiling-fans/52in/       valid
Crawling page: https://en.lumenco.ca/fixtures/ceiling-fans/48/       valid
Crawling page: https://en.lumenco.ca/fixtures/outdoor-lights/pendants/       valid
Crawling page: https://en.lumenco.ca/fixtures/outdoor-lights/mounts/       valid
Crawling page: https://en.lumenco.ca/fixtures/wall-lights/vertical-sconces/       valid
Crawling page: https://en.lumenco.ca/fixtures/wall-lights/ambient/       valid
Crawling page: https://en.lumenco.ca/fixtures/outdoor-lights/       valid
Crawling page: https://en.lumenco.ca/fixtures/outdoor-lights/ceiling-lights/       valid
Crawling page: https://en.lumenco.ca/fixtures/wall-lights/       valid
Crawling page: https://en.lumenco.ca/fixtures/lamps/decor/       valid
Crawling page: https://en.lumenco.ca/fixtures/wall-lights/bathroom-sconces/       valid
Crawling page: https://en.lumenco.ca/fixtures/wall-lights/indoor-sconces/       valid
Crawling page: https://en.lumenco.ca/fixtures/lamps/foot/       valid
Crawling page: https://en.lumenco.ca/fixtures/lamps/table/       valid
Crawling page: https://pyrexar.com/privacy-policy       Crawling page: https://pyrexar.com/pyrexar-signs-with-viet-nam       Crawling page: https://en.lumenco.ca/fixtures/pendant-lights/chain-dual-mount/       valid
Crawling page: https://en.lumenco.ca/fixtures/pendant-lights/rod/       valid
Crawling page: https://en.lumenco.ca/fixtures/lamps/       valid
Crawling page: https://en.lumenco.ca/fixtures/pendant-lights/cord/       valid
Crawling page: https://pyrexar.com/pyrexar-adds-thailand-distributor       Crawling page: https://en.lumenco.ca/fixtures/pendant-lights/lantern-pendant/       valid
Crawling page: https://en.lumenco.ca/fixtures/pendant-lights/bowl/       valid
Crawling page: https://en.lumenco.ca/fixtures/pendant-lights/drum/       valid
Crawling page: https://en.lumenco.ca/fixtures/pendant-lights/barn/       valid
Crawling page: https://en.lumenco.ca/fixtures/pendant-lights/       valid
Crawling page: https://en.lumenco.ca/fixtures/ceiling-lights/low-profile/       valid
Crawling page: https://en.lumenco.ca/fixtures/ceiling-lights/adjustable-track-lights/       valid
Crawling page: https://pyrexar.com/about-us/how-to-buy       Crawling page: https://pyrexar.com/about-us/distributor-login       Crawling page: https://pyrexar.com/about-us/contact-us       Crawling page: https://pyrexar.com/about-us/in-the-news       Crawling page: https://www.pyrexar.com/hyperthermia/bsd-2000-3d-mr       Crawling page: https://pyrexar.com/about-us/featured-event       Crawling page: https://en.lumenco.ca/fixtures/ceiling-lights/semi-flush-mounts/       valid
Crawling page: https://en.lumenco.ca/fixtures/ceiling-lights/track/       valid
Crawling page: https://en.lumenco.ca/fixtures/ceiling-lights/flush-mounts/       valid
Crawling page: https://morpho.com/en/press-releases/feed       valid
Crawling page: https://pyrexar.com/clinical/by-indication       Crawling page: https://pyrexar.com/knowledgebase/hyperthermia-faq       Crawling page: https://pyrexar.com/knowledgebase/organizations       Crawling page: https://pyrexar.com/knowledgebase/pyrexar-video       Crawling page: https://morpho.com/en/content/hub-our-identity-related-services       valid
Crawling page: https://www.morpho.com/en/implantation       valid
Crawling page: https://morpho.com/en/media/authentication-all-its-forms-20170717?shadowbox=1       valid
Crawling page: https://morpho.com/en/media/ot-morpho-joins-lora-alliance-20170719?shadowbox=1       valid
Crawling page: https://morpho.com/en/media/ot-morpho-receives-cspn-certification-its-embedded-secure-element-iot-20170720?shadowbox=1       valid
Crawling page: https://morpho.com/en/media/morpho-licence2go-digital-licence-platform-ot-morpho-20170725?shadowbox=1       valid
Crawling page: https://pyrexar.com/clinical/clinical-trials       Crawling page: https://morpho.com/en/media/changing-face-identity-verification-20170802?shadowbox=1       valid
Crawling page: https://morpho.com/en/media/licence2go-your-smartphone-becomes-your-secure-digital-id-wallet-20170803?shadowbox=1       valid
Crawling page: https://morpho.com/en/media/tsa-pre-and-twic-programs-be-offered-maryland-department-transportation-motor-vehicle-administration-mdot-mva-20170809?shadowbox=1       valid
Crawling page: https://pyrexar.com/hyperthermia/bsd-2000-3d-mr       Crawling page: https://pyrexar.com/clinical/bsd-2000-clinical-studies       Crawling page: https://pyrexar.com/clinical       Crawling page: https://morpho.com/en/media/majority-us-public-ready-mobile-driver-licenses-according-ot-morpho-radius-global-market-research-study-20170823?shadowbox=1       valid/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:51:08 [scrapy.extensions.logstats] INFO: Crawled 229 pages (at 7 pages/min), scraped 183 items (at 6 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:51:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/hyperthermia> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 19:51:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/hyperthermia/bsd-500> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:52:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/hyperthermia/bsd-2000> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 19:52:03 [scrapy.extensions.logstats] INFO: Crawled 240 pages (at 11 pages/min), scraped 189 items (at 6 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:52:51 [root] ERROR: Unable to find match for url: https://www.idemia.com/step-inside-world-idemia
/bin/sh: 1: kill: No such process

2018-11-14 19:52:56 [root] ERROR: Unable to find match for url: https://www.idemia.com/our-journey
2018-11-14 19:52:56 [scrapy.extensions.logstats] INFO: Crawled 249 pages (at 9 pages/min), scraped 197 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:54:26 [scrapy.extensions.logstats] INFO: Crawled 253 pages (at 4 pages/min), scraped 207 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:55:19 [scrapy.extensions.logstats] INFO: Crawled 258 pages (at 5 pages/min), scraped 212 items (at 5 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:56:09 [scrapy.extensions.logstats] INFO: Crawled 266 pages (at 8 pages/min), scraped 217 items (at 5 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:56:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

2018-11-14 19:56:55 [scrapy.extensions.logstats] INFO: Crawled 277 pages (at 11 pages/min), scraped 221 items (at 4 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:58:10 [scrapy.extensions.logstats] INFO: Crawled 279 pages (at 2 pages/min), scraped 231 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:59:19 [scrapy.extensions.logstats] INFO: Crawled 292 pages (at 13 pages/min), scraped 241 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 19:59:58 [scrapy.extensions.logstats] INFO: Crawled 297 pages (at 5 pages/min), scraped 247 items (at 6 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:00:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://morpho.com/en/../en/biometric-terminals-add-security-variety-processes>: HTTP status code is not handled or not allowed
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:01:04 [scrapy.extensions.logstats] INFO: Crawled 306 pages (at 9 pages/min), scraped 256 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:01:26 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.teslanano.com/?page_id=16>: HTTP status code is not handled or not allowed
2018-11-14 20:01:26 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.teslanano.com/markets/us-army-corps-of-engineers/>: HTTP status code is not handled or not allowed
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process


Crawling page: https://morpho.com/en/content/homepage-morpho-en?mini=2018-12       valid
Crawling page: https://morpho.com/en/node/5285?shadowbox=1       valid
Crawling page: https://www.morpho.com/iot       valid
Crawling page: https://morpho.com/en/media/almost-75-people-believe-cash-will-be-used-much-less-2022-20170914?shadowbox=1       valid
Crawling page: https://morpho.com/en/media/ot-morpho-unveils-morphotop-slim-its-new-convenient-and-innovative-solution-enrollment-20170912?shadowbox=1       valid
Crawling page: https://www.morpho.com/en/biometrics       valid
Crawling page: https://morpho.com/en/media/tokenization-demystified-20170919?shadowbox=1       valid
Crawling page: https://morpho.com/en/media/four-stages-payment-development-20170926?shadowbox=1       valid
Crawling page: https://morpho.com/en/content/publications-download       valid
Crawling page: https://pyrexar.com/hyperthermia       Crawling page: https://pyrexar.com/hyperthermia/bsd-500       Crawling page: https://morpho.com/en/cart       valid
Crawling page: https://morpho.com/en/media/ot-morpho-becomes-idemia-global-leader-trusted-identities-20170928?shadowbox=1       valid
Crawling page: https://pyrexar.com/hyperthermia/bsd-2000       Crawling page: https://morpho.com/en/contact       valid
Crawling page: https://morpho.com/en/implantation       valid
Crawling page: https://morpho.com/en/media/photos       valid
Crawling page: https://morpho.com/en/stay-in-touch-with-us       valid
Crawling page: https://morpho.com/en/media/press-releases-news       valid
Crawling page: https://morpho.com/cart       valid
Crawling page: https://www.idemia.com/step-inside-world-idemia       valid
Crawling page: https://www.idemia.com/our-journey       valid
Crawling page: https://morpho.com/en/media/ot-morpho-becomes-idemia-global-leader-trusted-identities-20170928       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/security/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/security/dawn-to-dusk-barn-lights/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/security/wall-packs/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/security/post-lights-bollards/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/floodlights/explosion-proof/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/floodlights/rgb-led/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/floodlights/low-voltage-12v-24v-led/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/floodlights/led-low-profile/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/floodlights/max-efficiency/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/landscape/solar-lights/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/landscape/pathlight/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/floodlights/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/floodlights/led/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/landscape/led-decor/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/landscape/in-ground-lights/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/outdoor-bulbs/halogen-incandescent-bulbs/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/landscape/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/landscape/strip-lights/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/outdoor-bulbs/outdoor-led-bulbs/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/outdoor-recessed/outdoor-halogen-incandescent/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/outdoor-recessed/outdoor-led-recessed/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/outdoor-recessed/       valid
Crawling page: https://pyrexar.com/       Crawling page: https://morpho.com/en/stay-in-touch-with-us/Stay%20in%20touch%20with%20us%20%21       valid
Crawling page: https://morpho.com/en/calendar?type=1065       valid
Crawling page: https://www.gigasolarpv.com/       valid
Crawling page: https://www.gigasolarpv.com/products/       valid
Crawling page: https://www.gigasolarpv.com/benefits-2/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/outdoor-fixtures/wall-mounts/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/outdoor-fixtures/garage-lights/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/outdoor-fixtures/outdoor-ceiling/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/outdoor-fixtures/outdoor-pendants/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/outdoor-fixtures/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/linear/troffers/       valid
Crawling page: https://morpho.com/en/media/case-studies-0/Case%20Studies       valid
Crawling page: https://morpho.com/en/biometric-terminals/mobile-terminals       valid
Crawling page: https://morpho.com/en/media/press-releases-news/Press%20Releases%20%26%20News       valid
Crawling page: https://morpho.com/en/media/publications-0/Publications       valid
Crawling page: https://morpho.com/en/biometric-terminals/oem-modules/fingerprint-modules       valid
Crawling page: https://morpho.com/en/biometric-terminals/oem-modules/vein-print-modules       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/lighting-controls/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/linear/suspended-linear-lights/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/linear/shop-lights/       valid
Crawling page: https://morpho.com/en/biometric-terminals/desktop-devices/fingerprint-devices       valid
Crawling page: https://morpho.com/en/biometric-terminals/desktop-devices/touchless-devices-0       valid
Crawling page: https://morpho.com/en/biometric-terminals/desktop-devices/vein-print-devices       valid
Crawling page: https://morpho.com/en/biometric-terminals/time-attendance-terminals/fingerprint-time-clocks-0       valid
Crawling page: https://morpho.com/en/biometric-terminals/time-attendance-terminals/touchless-time-clocks-0       valid
Crawling page: https://morpho.com/en/biometric-terminals/access-control-terminals/vein-print       valid
Crawling page: https://morpho.com/en/biometric-terminals/access-control-terminals/fingerprint       valid
Crawling page: https://morpho.com/en/biometric-terminals/access-control-terminals/facial       valid
Crawling page: https://morpho.com/en/biometric-terminals/access-control-terminals/touchless-terminals-0       valid
Crawling page: https://morpho.com/en/commercial-identity/solutions-terminals-gaming/cutting-edge-range-gaming-terminals       valid
Crawling page: https://morpho.com/en/commercial-identity/solutions-terminals-gaming/players-identification       valid
Crawling page: https://morpho.com/en/commercial-identity/solutions-terminals-gaming/comply-online-gaming-regulations-cost-efficiently-swiftly-and-openly       valid
Crawling page: https://morpho.com/en/commercial-identity/gaming-solutions       valid
Crawling page: https://morpho.com/en/commercial-identity/solutions-telecom/additional-solutions       valid
Crawling page: https://morpho.com/en/commercial-identity/solutions-telecom/biometrics-mobile-0       valid
Crawling page: https://morpho.com/en/commercial-identity/solutions-telecom/secure-elements       valid
Crawling page: https://morpho.com/en/commercial-identity/solutions-telecom/customer-registration-mobile-id       valid
Crawling page: https://morpho.com/en/commercial-identity/solutions-telecom/internet-things       valid
Crawling page: https://morpho.com/en/commercial-identity/solutions-financial-institutions/convenience-and-speed-secure-transit-payments       valid
Crawling page: https://morpho.com/en/commercial-identity/solutions-telecom       valid
Crawling page: https://morpho.com/en/commercial-identity/solutions-financial-institutions/biometrics-banking       valid
Crawling page: https://morpho.com/en/commercial-identity/solutions-financial-institutions/ebanking-digital-id/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:02:53 [scrapy.extensions.logstats] INFO: Crawled 328 pages (at 22 pages/min), scraped 270 items (at 14 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:03:51 [scrapy.extensions.logstats] INFO: Crawled 328 pages (at 0 pages/min), scraped 277 items (at 7 items/min)
/bin/sh: 1: kill: No such process

2018-11-14 20:03:57 [scrapy.extensions.logstats] INFO: Crawled 328 pages (at 0 pages/min), scraped 278 items (at 1 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:04:59 [scrapy.extensions.logstats] INFO: Crawled 334 pages (at 6 pages/min), scraped 284 items (at 6 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:06:06 [scrapy.extensions.logstats] INFO: Crawled 341 pages (at 7 pages/min), scraped 291 items (at 7 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:06:59 [scrapy.extensions.logstats] INFO: Crawled 352 pages (at 11 pages/min), scraped 297 items (at 6 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:07:56 [scrapy.extensions.logstats] INFO: Crawled 359 pages (at 7 pages/min), scraped 304 items (at 7 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:09:03 [scrapy.extensions.logstats] INFO: Crawled 363 pages (at 4 pages/min), scraped 313 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:09:57 [scrapy.extensions.logstats] INFO: Crawled 378 pages (at 15 pages/min), scraped 321 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:11:08 [scrapy.extensions.logstats] INFO: Crawled 383 pages (at 5 pages/min), scraped 331 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:11:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.fiatest.com/food-intolerance-test/>: HTTP status code is not handled or not allowed
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:12:10 [scrapy.extensions.logstats] INFO: Crawled 399 pages (at 16 pages/min), scraped 339 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

       valid
Crawling page: https://morpho.com/en/commercial-identity/solutions-financial-institutions/personalization       valid
Crawling page: https://morpho.com/en/commercial-identity/solutions-financial-institutions/insurance-econtracting       valid
Crawling page: https://www.fiatest.com/disclaimer/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/linear/wraparound/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/linear/led-extrusions/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/linear/linear-fluorescent-fixtures/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/linear/panel-lights/       valid
Crawling page: https://morpho.com/en/commercial-identity/solutions-financial-institutions/cards-payment       valid
Crawling page: https://morpho.com/en/commercial-identity/solutions-financial-institutions       valid
Crawling page: https://morpho.com/en/commercial-identity/solutions-financial-institutions/mobile-payment-solutions-empower-financial-world       valid
Crawling page: https://morpho.com/en/commercial-identity/provide-trust-services/digital-certificates       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/linear/led-tubes/drop-in-replacement-t8/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/linear/led-tubes/direct-wire-t8/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/linear/led-tubes/t5-replacement-led/       valid
Crawling page: https://morpho.com/en/commercial-identity/provide-trust-services/cloud-managed-services       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/undercabinet/accessoires-sous/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/linear/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/undercabinet/bar-light/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/linear/led-tubes/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/undercabinet/strip-lighting/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/bulbs/accessories/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/bulbs/traditional-lighting/incandescent-bulbs/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/bulbs/traditional-lighting/incandescent-bulbs/3-way/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/bulbs/vintage-bulbs/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/bulbs/high-bay-led-retrofit-lamp-hid-replacement/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/bulbs/traditional-lighting/halogen-bulbs/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/bulbs/led-bulbs/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/bulbs/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/fixtures/       valid
Crawling page: https://morpho.com/en/commercial-identity/provide-trust-services/legally-binding-archiving-data-confidentiality-and-traceability       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/recessed-lights/contractor-packs/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/recessed-lights/recessed-halogen-lights/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/recessed-lights/recessed-led/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/recessed-lights/       valid
Crawling page: https://en.lumenco.ca/customer/account/create/       valid
Crawling page: https://en.lumenco.ca/       valid
Crawling page: https://en.lumenco.ca/promos/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/       valid
Crawling page: https://morpho.com/en/commercial-identity/provide-trust-services/multichannel-electronic-contracting       valid
Crawling page: https://morpho.com/en/strong-customer-authentication-psd2       valid
Crawling page: https://morpho.com/en/commercial-identity/provide-trust-services       valid
Crawling page: https://en.lumenco.ca/customer/account/login/       valid
Crawling page: https://morpho.com/en/commercial-identity/register-customers-manage-identities/authenticate-documents       valid
Crawling page: https://morpho.com/en/commercial-identity/provide-trust-services/multi-factor-strong-authentication-federation-and-sso       valid
Crawling page: https://morpho.com/en/commercial-identity/provide-trust-services/digital-signatures-and-secure-transaction-confirmation       valid
Crawling page: https://morpho.com/en/commercial-identity/know-your-customer-kyc       valid
Crawling page: https://morpho.com/en/commercial-identity/register-customers-manage-identities/register-remotely-0       valid
Crawling page: https://morpho.com/en/ensuring-trusted-authentication-and-transactions-online       valid
Crawling page: https://morpho.com/en/public-security/guarantee-road-and-railways-safety/railways-safety       valid
Crawling page: https://morpho.com/en/public-security/guarantee-road-and-railways-safety/process-violations       valid
Crawling page: https://www.morpho.com/morpho-airport/index.html       valid
Crawling page: https://morpho.com/en/public-security/safer-road-and-railways       valid
Crawling page: https://www.morpho.com/en/atom/1094       valid
Crawling page: https://morpho.com/en/public-security/law-enforcement/biometric-scanners       valid
Crawling page: https://morpho.com/en/public-security/smart-borders-0       valid
Crawling page: https://morpho.com/en/public-security/smart-borders/semi-automated-solutions/e-counter       valid
Crawling page: https://morpho.com/en/public-security/guarantee-road-and-railways-safety/track-vehicles/anpr/mobile-plate-hunter       valid
Crawling page: https://morpho.com/en/public-security/guarantee-road-and-railways-safety/check-speed-red-light/detect-red-light-running-and-speed       valid
Crawling page: https://morpho.com/en/public-security/law-enforcement/morphobis-state-art-automated-biometric-identification-systems-abis-criminal-identification       valid
Crawling page: https://morpho.com/en/public-security/automated-biometric-tools-help-police-investigate-crimes-more-efficiently       valid
Crawling page: https://morpho.com/en/public-security/smart-borders/passenger-data-processing/morpho-traveler-analytics-suite       valid
Crawling page: https://morpho.com/en/public-security/smart-borders/automated-solutions/e-gate-1       valid
Crawling page: https://www.morpho.com/en/media/morpho-licence2go-digital-licence-platform-ot-morpho-20170725       valid
Crawling page: https://morpho.com/en/public-security/law-enforcement/video-analytics/morpho-video-and-image-analytic-platform       valid
Crawling page: https://morpho.com/en/mobile-and-automated-systems-improve-public-security       valid
Crawling page: https://morpho.com/en/civil-identity/added-value-services-and-expertise-delivery-civil-id-solutions       valid
Crawling page: https://morpho.com/en/public-security/law-enforcement/dna-identification/rapidhit       valid
Crawling page: https://morpho.com/en/public-security/law-enforcement/face-recognition/morpho-argus       valid
Crawling page: https://morpho.com/en/civil-identity/mastering-electronic-id-chain-secure-and-efficient-e-services       valid
Crawling page: https://morpho.com/en/civil-identity/secure-and-modular-healthcare-identification-solutions       valid
Crawling page: https://morpho.com/en/biometrics       valid
Crawling page: https://morpho.com/en       valid
Crawling page: https://www.fiatest.com/author/waqhz/       valid
Crawling page: https://www.fiatest.com/shop/       valid
Crawling page: https://morpho.com/en/innovation-0       valid
Crawling page: https://morpho.com/en/civil-identity/providing-governments-best-third-generation-travel-documents       valid
Crawling page: https://morpho.com/en/civil-identity/unrivalled-biometric-technology-ultra-secure-drivers-licenses       valid
Crawling page: https://morpho.com/en/government-id-solutions-facilitate-and-secure-identity-management       valid
Crawling page: https://morpho.com/en/civil-identity/population-and-voter-registration-creating-and-managing-reliable-ids-changing-world       valid
Crawling page: https://morpho.com/en/media       /bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:13:23 [scrapy.extensions.logstats] INFO: Crawled 399 pages (at 0 pages/min), scraped 348 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:14:15 [scrapy.extensions.logstats] INFO: Crawled 412 pages (at 13 pages/min), scraped 358 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:15:11 [scrapy.extensions.logstats] INFO: Crawled 437 pages (at 25 pages/min), scraped 373 items (at 15 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:16:10 [scrapy.extensions.logstats] INFO: Crawled 438 pages (at 1 pages/min), scraped 386 items (at 13 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:17:12 [scrapy.extensions.logstats] INFO: Crawled 455 pages (at 17 pages/min), scraped 394 items (at 8 items/min)
2018-11-14 20:17:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <406 https://www.gigasolarpv.com/about-us/>: HTTP status code is not handled or not allowed
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:18:20 [scrapy.extensions.logstats] INFO: Crawled 457 pages (at 2 pages/min), scraped 402 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:18:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/2018-website-update> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:18:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/wan-fang-opens-their-doors-to-share-hyperthermia> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:18:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/visit-us-at-astro-2018> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:18:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/new-study-supporting-focus-and-steering> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:19:09 [scrapy.extensions.logstats] INFO: Crawled 471 pages (at 14 pages/min), scraped 406 items (at 4 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:20:30 [scrapy.extensions.logstats] INFO: Crawled 471 pages (at 0 pages/min), scraped 414 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:21:44 [scrapy.extensions.logstats] INFO: Crawled 479 pages (at 8 pages/min), scraped 422 items (at 8 items/min)
2018-11-14 20:21:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <406 https://www.fiatest.com/privacy-policy/>: HTTP status code is not handled or not allowed
/bin/sh: 1: kill: No such process

2018-11-14 20:21:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.fiatest.com/food-allergy-test/>: HTTP status code is not handled or not allowed
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:22:48 [scrapy.extensions.logstats] INFO: Crawled 499 pages (at 20 pages/min), scraped 429 items (at 7 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:23:03 [scrapy.core.scraper] ERROR: Error downloading <GET https://pyrexar.com/hyperthermia-proton-beam>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-14 20:23:03 [scrapy.core.scraper] ERROR: Error downloading <GET https://pyrexar.com/sarcoma-patients-living-longer-with-hyperthermia>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-14 20:23:03 [scrapy.extensions.logstats] INFO: Crawled 499 pages (at 0 pages/min), scraped 431 items (at 2 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:23:48 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.polystar.com/2018/09/polystar-announces-iso-9001-and-27001-certifications/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-14 20:23:48 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.polystar.com/2018/03/polystar-chosen-by-european-tier-1-operator-to-provide-virtual-solver-test-solution/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
/bin/sh: 1: kill: No such process

2018-11-14 20:23:56 [scrapy.extensions.logstats] INFO: Crawled 509 pages (at 10 pages/min), scraped 438 items (at 7 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

valid
Crawling page: https://morpho.com/en/civil-identity/secure-and-convenient-id-solutions-governments-and-citizens-cover-entire-civil-identity-lifecycle       valid
Crawling page: https://www.fiatest.com/food-allergy/       valid
Crawling page: https://www.fiatest.com/food-intolerance/       valid
Crawling page: https://www.fiatest.com/about/       valid
Crawling page: https://www.fiatest.com/faqs/       valid
Crawling page: http://escapetherapeutics.com/eoe.html       valid
Crawling page: http://escapetherapeutics.com/benefits.html       valid
Crawling page: http://escapetherapeutics.com/openings.html       valid
Crawling page: http://escapetherapeutics.com/press.html       valid
Crawling page: https://www.teslanano.com/markets/oil-and-gas/       valid
Crawling page: https://www.teslanano.com/markets/civil-military/       valid
Crawling page: https://www.teslanano.com/markets/trucking-construction-agriculture/       valid
Crawling page: https://www.teslanano.com/markets/infrastructure/       valid
Crawling page: https://morpho.com/fr       valid
Crawling page: https://www.fiatest.com/       valid
Crawling page: https://www.teslanano.com/markets/       valid
Crawling page: https://www.teslanano.com/products/       valid
Crawling page: http://escapetherapeutics.com/news.html       valid
Crawling page: http://escapetherapeutics.com/publications.html       valid
Crawling page: http://escapetherapeutics.com/osteostem.html       valid
Crawling page: http://escapetherapeutics.com/intellectualproperty.html       valid
Crawling page: http://escapetherapeutics.com/neurox.html       valid
Crawling page: http://escapetherapeutics.com/fibrostem.html       valid
Crawling page: http://escapetherapeutics.com/allograf.html       valid
Crawling page: http://escapetherapeutics.com/morphogel.html       valid
Crawling page: http://escapetherapeutics.com/glucostem.html       valid
Crawling page: http://escapetherapeutics.com/mesostem.html       valid
Crawling page: http://escapetherapeutics.com/dermatology.html       valid
Crawling page: http://escapetherapeutics.com/endocrinology.html       valid
Crawling page: http://escapetherapeutics.com/focus.html       valid
Crawling page: http://escapetherapeutics.com/investors.html       valid
Crawling page: http://escapetherapeutics.com/board.html       valid
Crawling page: https://www.teslanano.com/about/       valid
Crawling page: https://www.teslanano.com/about/history/       valid
Crawling page: https://www.teslanano.com/       valid
Crawling page: https://www.teslanano.com/about/management-team/       valid
Crawling page: http://escapetherapeutics.com/management.html       valid
Crawling page: http://escapetherapeutics.com/overview.html       valid
Crawling page: http://escapetherapeutics.com/mission.html       valid
Crawling page: http://escapetherapeutics.com/index.html       valid
Crawling page: https://www.teslanano.com/contact-us/       valid
Crawling page: https://www.teslanano.com/product-category/corrosion-resistant-primers/       valid
Crawling page: https://www.teslanano.com/product-category/corrosion-resistant-paints/       valid
Crawling page: https://morpho.com/en/newsletters-list/2326-Newsletters       valid
Crawling page: https://morpho.com/en/media/ot-morpho-will-present-its-first-semester-2017-financial-results-investors-october-6th-2017-20170915?shadowbox=1       valid
Crawling page: https://www.gigasolarpv.com/comp/       valid
Crawling page: https://www.gigasolarpv.com/about/       valid
Crawling page: https://www.gigasolarpv.com/faq/       valid
Crawling page: https://www.gigasolarpv.com/gallery/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/linear/led-tubes/4ft-over-2000-lumens/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/linear/led-tubes/u-bend-pl-pll-led/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/linear/led-tubes/4ft-1800-to-2000-lumens/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/linear/led-tubes/t8-replacement-led/       valid
Crawling page: https://morpho.com/en/media/ibia-welcomes-bob-eckel-its-board-directors-20170801?shadowbox=1       valid
Crawling page: https://www.gigasolarpv.com/news/       valid
Crawling page: https://en.lumenco.ca/fixtures/       valid
Crawling page: https://en.lumenco.ca/fixtures/chandeliers/       valid
Crawling page: https://en.lumenco.ca/fixtures/chandeliers/chandeliers-1-etage/       valid
Crawling page: https://en.lumenco.ca/outdoor-lighting/outdoor-bulbs/       valid
Crawling page: https://morpho.com/en/cookie-policy       valid
Crawling page: https://morpho.com/en/contact?shadowbox=1       valid
Crawling page: https://pyrexar.com/2018-website-update       Crawling page: https://pyrexar.com/wan-fang-opens-their-doors-to-share-hyperthermia       Crawling page: https://pyrexar.com/visit-us-at-astro-2018       Crawling page: https://pyrexar.com/new-study-supporting-focus-and-steering       Crawling page: https://en.lumenco.ca/industrial-lighting/street-parking/extreme-weather/       valid
Crawling page: https://en.lumenco.ca/fixtures/ceiling-fans/36/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/led-tubes/       valid
Crawling page: https://en.lumenco.ca/industrial-lighting/light-bulbs/       valid
Crawling page: https://en.lumenco.ca/fixtures/ceiling-fans/       valid
Crawling page: https://en.lumenco.ca/fixtures/ceiling-fans/42/       valid
Crawling page: https://en.lumenco.ca/shop-by-room/garage-lighting/       valid
Crawling page: https://en.lumenco.ca/eiko-led15wt8f-48-840k-g7dm.html       valid
Crawling page: https://en.lumenco.ca/canarm-icw492a01ch9.html       valid
Crawling page: https://en.lumenco.ca/canarm-icw582a01bn10-l.html       valid
Crawling page: https://www.polystar.com/products/solver/solver-for-csps/       valid
Crawling page: https://en.lumenco.ca/nuvo-lighting-60-104.html       valid
Crawling page: https://en.lumenco.ca/canarm-lch116a20bn-l.html       valid
Crawling page: https://en.lumenco.ca/canarm-ich677a05orb16-l.html       valid
Crawling page: https://www.polystar.com/2017/02/white-paper-monitoring-customer-experience-partially-virtualised-network/       valid
Crawling page: https://www.polystar.com/products/solver/solver-for-nems/       valid
Crawling page: https://en.lumenco.ca/nuvo-lighting-60-105.html       valid
Crawling page: https://www.polystar.com/solutions/innovation/big-data-analytics/       valid
Crawling page: https://www.polystar.com/2017/02/white-paper-monitoring-customer-experience-partially-virtualised-network/       valid
Crawling page: https://www.polystar.com/?s=       valid
Crawling page: https://www.polystar.com/partners/oem-partnerships/       valid
Crawling page: https://en.lumenco.ca/commercial-lighting/wall-packs/       valid
Crawling page: https://en.lumenco.ca/commercial-lighting/security-lighting/       valid
Crawling page: https://www.polystar.com/resource-library/       valid
Crawling page: https://www.polystar.com/partners/channel-partnerships/       valid
Crawling page: https://en.lumenco.ca/fixtures/chandeliers/drum/       valid
Crawling page: https://en.lumenco.ca/fixtures/chandeliers/crystal/       valid
Crawling page: https://en.lumenco.ca/terms-of-use/       valid
Crawling page: https://en.lumenco.ca/subventions/       valid
Crawling page: https://en.lumenco.ca/security-privacy-policy/       valid
Crawling page: https://en.lumenco.ca/eclairage/       valid
Crawling page: https://www.polystar.com/solutions/innovation/network-oss-data/       valid
Crawling page: https://www.polystar.com/solutions/network-performance-insights/noc-insights/       valid
Crawling page: https://gold.tanaka.co.jp/english/       valid
Crawling page: https://gold.tanaka.co.jp/english/activities/index.html       valid
Crawling page: https://www.novapangaea.com/contact/       valid
Crawling page: https://www.novapangaea.com/team/       valid
Crawling page: https://www.novapangaea.com/news-blog/       valid
Crawling page: https://www.novapangaea.com/sustainable-commercial-approach/       valid
Crawling page: https://www.polystar.com/solutions/network-performance-insights/device-insights/       valid
Crawling page:/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:24:58 [scrapy.extensions.logstats] INFO: Crawled 513 pages (at 4 pages/min), scraped 447 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:25:22 [scrapy.core.scraper] ERROR: Error downloading <GET https://pyrexar.com/astro-2018-in-review>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-14 20:25:22 [scrapy.core.scraper] ERROR: Error downloading <GET https://marketing.pyrexar.com/acton/form/15060/0027:d-0001/1/-/-/-/-/index.htm>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-14 20:25:22 [scrapy.core.scraper] ERROR: Error downloading <GET https://pyrexar.com/about-us/our-team>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-14 20:25:22 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.pyrexar.com/clinical/clinical-trials>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-14 20:25:22 [scrapy.core.scraper] ERROR: Error downloading <GET https://pyrexar.com/about-us>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-14 20:25:22 [scrapy.core.scraper] ERROR: Error downloading <GET https://pyrexar.com/hyperthermia-blog>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
/bin/sh: 1: kill: No such process

2018-11-14 20:25:27 [scrapy.core.scraper] ERROR: Error downloading <GET http://sunsynchrony.com/contact/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-14 20:25:27 [scrapy.core.scraper] ERROR: Error downloading <GET http://sunsynchrony.com/products/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
/bin/sh: 1: kill: No such process

2018-11-14 20:25:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/hyperthermia/bsd-2000-3d> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

2018-11-14 20:25:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://sunsynchrony.com/press/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-14 20:25:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://sunsynchrony.com/technology/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-14 20:25:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://sunsynchrony.com/about/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-14 20:25:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://sunsynchrony.com/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-14 20:25:47 [scrapy.core.scraper] ERROR: Error downloading <GET https://gold.tanaka.co.jp/english/contact/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-14 20:25:47 [scrapy.core.scraper] ERROR: Error downloading <GET https://gold.tanaka.co.jp/index.php>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-14 20:25:47 [scrapy.core.scraper] ERROR: Error downloading <GET https://gold.tanaka.co.jp/english/products/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:26:14 [scrapy.extensions.logstats] INFO: Crawled 523 pages (at 10 pages/min), scraped 457 items (at 10 items/min)
2018-11-14 20:26:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://pyrexar.com/knowledgebase/treatment-centers> (referer: https://pyrexar.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:26:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <406 http://opelinc.com/>: HTTP status code is not handled or not allowed
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:26:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Foodservice> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:27:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Parts-Support.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:27:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/calprop65> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:27:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Parts-Support/TSR-and-CSR-Contact.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:27:24 [scrapy.extensions.logstats] INFO: Crawled 545 pages (at 22 pages/min), scraped 461 items (at 4 items/min)
/bin/sh: 1: kill: No such process

2018-11-14 20:27:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/vollrathUniversity/Culinary-Support/Guides-Reference.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

2018-11-14 20:27:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Parts-Support/Service-Agencies.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:28:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Resource-Library/Price-Lists.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:28:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Parts-Support/Parts-Search.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:28:12 [scrapy.extensions.logstats] INFO: Crawled 547 pages (at 2 pages/min), scraped 463 items (at 2 items/min)
/bin/sh: 1: kill: No such process

2018-11-14 20:28:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Resource-Library/Catalogs.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:28:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Resource-Library/New-Product-Brochure.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:28:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/vollrathUniversity/Product-Training.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:28:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Terms-of-Use.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

2018-11-14 20:29:04 [scrapy.extensions.logstats] INFO: Crawled 549 pages (at 2 pages/min), scraped 465 items (at 2 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:30:06 [scrapy.extensions.logstats] INFO: Crawled 556 pages (at 7 pages/min), scraped 473 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:30:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Privacy-Policy.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:30:58 [scrapy.extensions.logstats] INFO: Crawled 567 pages (at 11 pages/min), scraped 479 items (at 6 items/min)
2018-11-14 20:31:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/customers/dealers/terms-conditions-sale> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:31:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Contact-Us/Contact-Lookup> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:31:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Contact-Us/Website-Feedback> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:31:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Customers/Dealers/Customer-Self-Service.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:31:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Parts-Support/Product-Registration.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:32:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/warewashingconfigurator> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

2018-11-14 20:32:12 [scrapy.extensions.logstats] INFO: Crawled 575 pages (at 8 pages/min), scraped 482 items (at 3 items/min)
/bin/sh: 1: kill: No such process

2018-11-14 20:32:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Warewashing-Handling-and-Dispensers/How-To-Order-Traex-Imprinted-Racks.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

2018-11-14 20:32:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Warewashing-Handling-and-Dispensers/How-To-Order-Traex-Color-Racks.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

2018-11-14 20:33:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Contact-Us/Vollrath-Email-Signup> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:33:05 [scrapy.extensions.logstats] INFO: Crawled 578 pages (at 3 pages/min), scraped 485 items (at 3 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:33:55 [scrapy.extensions.logstats] INFO: Crawled 585 pages (at 7 pages/min), scraped 492 items (at 7 items/min)
/bin/sh: 1: kill: No such process

2018-11-14 20:34:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/VollrathUniversity/About.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:34:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Contact-Us> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:34:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Products/Warewashing-Handling-and-Dispensers/Signature-Dishrack-Component-Parts-Guide.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:34:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/About-Us/Markets/Supermarket.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:35:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/About-Us/Markets/Senior-Living.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:35:02 [scrapy.extensions.logstats] INFO: Crawled 594 pages (at 9 pages/min), scraped 495 items (at 3 items/min)
2018-11-14 20:35:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/About-Us/Markets/Stadium-Arena.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:35:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/About-Us/Markets/Retail.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:35:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/About-Us/Markets/Restaurant.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:35:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/About-Us/Markets/Recreation.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:35:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Contact-Us/Contact-Sales> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:35:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/About-Us/Markets/Hotel-Lodging-Casino.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:36:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/About-Us/Markets/Medical.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:36:09 [scrapy.extensions.logstats] INFO: Crawled 600 pages (at 6 pages/min), scraped 495 items (at 0 items/min)
2018-11-14 20:36:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/About-Us/Markets/Government-Military.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:36:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/About-Us/Markets/Fabricator.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:36:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Convenience-Stores.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:36:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/About-Us/Markets/Corrections.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:36:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/About-Us/Markets/Convention-Center.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:36:57 [scrapy.extensions.logstats] INFO: Crawled 609 pages (at 9 pages/min), scraped 495 items (at 0 items/min)
/bin/sh: 1: kill: No such process

2018-11-14 20:37:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/About-Us/Markets/Catering.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:37:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/About-Us/Markets/college-university.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:37:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/About-Us/Markets/Culinary-School.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:37:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/About-Us/Markets.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:37:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/About-Us/Markets/Airport-Travel.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:37:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/About-Us/Markets/Business-Industry.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:38:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/About-Us/Markets/Bar-Tavern.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:38:07 [scrapy.extensions.logstats] INFO: Crawled 616 pages (at 7 pages/min), scraped 496 items (at 1 items/min)
2018-11-14 20:38:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/About-Us.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:38:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/About-Us/Philosophy-and-Services.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:38:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/About-Us/Case-Studies/Paddock-Club.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:38:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Parts-Support/FAQ-and-Reference.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:38:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://vollrathcompany.com/about-us/locations> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:38:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/VollrathUniversity.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:39:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Products/Serving-Systems--Components.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:39:03 [scrapy.extensions.logstats] INFO: Crawled 626 pages (at 10 pages/min), scraped 496 items (at 0 items/min)
 https://www.polystar.com/solutions/network-performance-insights/network-management-insights/       valid
Crawling page: https://en.lumenco.ca/fixtures/pendant-lights/mini/       valid
Crawling page: https://en.lumenco.ca/fixtures/chandeliers/chandeliers-2-etages/       valid
Crawling page: https://www.novapangaea.com/privacy-cookie-policy/       valid
Crawling page: https://www.novapangaea.com/refnova-products-services/       valid
Crawling page: https://www.novapangaea.com/novapangaea-fractionation-technology/       valid
Crawling page: https://www.novapangaea.com/       valid
Crawling page: https://www.novapangaea.com/invest/       valid
Crawling page: https://www.novapangaea.com/feed/       valid
Crawling page: https://pyrexar.com/hyperthermia/bsd-2000-3d       Crawling page: https://en.lumenco.ca/indoor-lighting/undercabinet/stick-lights/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/undercabinet/led-pucks/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/undercabinet/       valid
Crawling page: https://en.lumenco.ca/indoor-lighting/bulbs/traditional-lighting/       valid
Crawling page: https://pyrexar.com/knowledgebase/treatment-centers       Crawling page: https://www.biscoind.com/       valid
Crawling page: https://www.biscoind.com/store?p=privacy-policy.html       valid
Crawling page: https://vollrath.com/Foodservice       Crawling page: https://www.biscoind.com/store?p=terms.html       valid
Crawling page: https://www.novapangaea.com/terms-use/       valid
Crawling page: https://vollrath.com/Vollrath/Parts-Support.htm       Crawling page: https://vollrath.com/calprop65       Crawling page: https://vollrath.com/Vollrath/Parts-Support/TSR-and-CSR-Contact.htm       Crawling page: https://www.biscoind.com/store?o=BiscoCareer       valid
Crawling page: https://vollrath.com/vollrathUniversity/Culinary-Support/Guides-Reference.htm       Crawling page: https://www.biscoind.com/MicroPlastics       valid
Crawling page: https://vollrath.com/Vollrath/Parts-Support/Service-Agencies.htm       Crawling page: https://vollrath.com/Vollrath/Resource-Library/Price-Lists.htm       Crawling page: https://vollrath.com/Vollrath/Parts-Support/Parts-Search.htm       Crawling page: https://www.biscoind.com/zago-mfg       valid
Crawling page: https://vollrath.com/Vollrath/Resource-Library/Catalogs.htm       Crawling page: https://vollrath.com/Vollrath/Resource-Library/New-Product-Brochure.htm       Crawling page: https://vollrath.com/vollrathUniversity/Product-Training.htm       Crawling page: https://vollrath.com/Vollrath/Terms-of-Use.htm       Crawling page: https://www.biscoind.com/Panduit-cableties-acces       valid
Crawling page: https://www.biscoind.com/AlphaWire       valid
Crawling page: https://www.biscoind.com/Optifuse       valid
Crawling page: https://www.biscoind.com/Conta-Clip       valid
Crawling page: https://www.biscoind.com/Accuride       valid
Crawling page: https://www.biscoind.com/bisco-clinchfastener       valid
Crawling page: https://www.biscoind.com/AdvancedInterconnections       valid
Crawling page: https://www.biscoind.com/Hantronix       valid
Crawling page: https://www.biscoind.com/Richco/5200004/Fiber--Wire---Cable-Management       valid
Crawling page: https://www.biscoind.com/Tyton       valid
Crawling page: https://www.biscoind.com/PSM-CaptiveHardware       valid
Crawling page: https://www.biscoind.com/Richco       valid
Crawling page: https://www.biscoind.com/3M       valid
Crawling page: https://vollrath.com/Vollrath/Privacy-Policy.htm       Crawling page: https://www.biscoind.com/Schroff-Pentair       valid
Crawling page: https://www.biscoind.com/KATO       valid
Crawling page: https://vollrath.com/customers/dealers/terms-conditions-sale       Crawling page: https://vollrath.com/Contact-Us/Contact-Lookup       Crawling page: https://vollrath.com/Contact-Us/Website-Feedback       Crawling page: https://www.biscoind.com/Acme-Industrial       valid
Crawling page: https://www.biscoind.com/Bivar       valid
Crawling page: https://vollrath.com/Vollrath/Customers/Dealers/Customer-Self-Service.htm       Crawling page: https://vollrath.com/Vollrath/Parts-Support/Product-Registration.htm       Crawling page: https://vollrath.com/warewashingconfigurator       Crawling page: https://www.biscoind.com/Hoffman-Enclosures       valid
Crawling page: https://www.biscoind.com/Altech       valid
Crawling page: https://vollrath.com/Warewashing-Handling-and-Dispensers/How-To-Order-Traex-Imprinted-Racks.htm       Crawling page: https://www.biscoind.com/RAF       valid
Crawling page: https://vollrath.com/Warewashing-Handling-and-Dispensers/How-To-Order-Traex-Color-Racks.htm       Crawling page: https://www.biscoind.com/Pomona-Electronics       valid
Crawling page: https://vollrath.com/Contact-Us/Vollrath-Email-Signup       Crawling page: https://www.biscoind.com/Hammond       valid
Crawling page: https://www.biscoind.com/store?goto=1       valid
Crawling page: https://www.biscoind.com/store?body=basket&typ=B       valid
Crawling page: https://www.biscoind.com/store?o=BiscoProfile       valid
Crawling page: https://www.biscoind.com/store?o=BiscoLocation       valid
Crawling page: https://www.biscoind.com/store?o=BiscoValueAdded       valid
Crawling page: https://www.biscoind.com/store?p=aog_info.html       valid
Crawling page: https://www.biscoind.com/store?o=BiscoContact       valid
Crawling page: https://vollrath.com/VollrathUniversity/About.htm       Crawling page: https://www.biscoind.com/store       valid
Crawling page: https://www.biscoind.com/store?forgot=1       valid
Crawling page: https://vollrath.com/Contact-Us       Crawling page: https://vollrath.com/Vollrath/Products/Warewashing-Handling-and-Dispensers/Signature-Dishrack-Component-Parts-Guide.htm       Crawling page: https://vollrath.com/Vollrath/About-Us/Markets/Supermarket.htm       Crawling page: https://vollrath.com/Vollrath/About-Us/Markets/Senior-Living.htm       Crawling page: https://vollrath.com/Vollrath/About-Us/Markets/Stadium-Arena.htm       Crawling page: https://vollrath.com/Vollrath/About-Us/Markets/Retail.htm       Crawling page: https://vollrath.com/Vollrath/About-Us/Markets/Restaurant.htm       Crawling page: https://vollrath.com/Vollrath/About-Us/Markets/Recreation.htm       Crawling page: https://vollrath.com/Contact-Us/Contact-Sales       Crawling page: https://vollrath.com/Vollrath/About-Us/Markets/Hotel-Lodging-Casino.htm       Crawling page: https://vollrath.com/Vollrath/About-Us/Markets/Medical.htm       Crawling page: https://vollrath.com/Vollrath/About-Us/Markets/Government-Military.htm       Crawling page: https://vollrath.com/Vollrath/About-Us/Markets/Fabricator.htm       Crawling page: https://vollrath.com/Convenience-Stores.htm       Crawling page: https://vollrath.com/Vollrath/About-Us/Markets/Corrections.htm       Crawling page: https://vollrath.com/Vollrath/About-Us/Markets/Convention-Center.htm       Crawling page: https://www.biscoind.com/store?register=1       valid
Crawling page: https://vollrath.com/Vollrath/About-Us/Markets/Catering.htm       Crawling page: https://vollrath.com/Vollrath/About-Us/Markets/college-university.htm       Crawling page: https://vollrath.com/Vollrath/About-Us/Markets/Culinary-School.htm       Crawling page: https://vollrath.com/Vollrath/About-Us/Markets.htm       Crawling page: https://vollrath.com/Vollrath/About-Us/Markets/Airport-Travel.htm       Crawling page: https://vollrath.com/Vollrath/About-Us/Markets/Business-Industry.htm       Crawling page: https://vollrath.com/Vollrath/About-Us/Markets/Bar-Tavern.htm       Crawling page: https://vollrath.com/Vollrath/About-Us.htm       Crawling page: https://vollrath.com/Vollrath/About-Us/Philosophy-and-Services.htm       Crawling page: https://vollrath.com/Vollrath/About-Us/Case-Studies/Paddock-Club.htm       Crawling page: https://vollrath.com/Vollrath/Parts-Support/FAQ-and-Reference.htm       Crawling page: http://vollrathcompany.com/about-us/locations       Crawling page: https://vollrath.com/VollrathUniversity.htm       Crawling page: https://vollrath.com/Vollrath/Products/Serving-Systems--Components.htm       Crawling page: https://vollrath.com/Vollrath/Parts-Support/Warranty-Information.htm2018-11-14 20:39:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Parts-Support/Warranty-Information.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:39:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Resource-Library.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:39:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath-Serving-Systems> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:39:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Fabricator-Components> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:39:49 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wheretobuy.vollrath.com> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:39:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Warming-Equipment> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:40:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Merchandising-Display-Solutions> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:40:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Frozen-Treat-Equipment> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:40:18 [scrapy.extensions.logstats] INFO: Crawled 633 pages (at 7 pages/min), scraped 496 items (at 0 items/min)
2018-11-14 20:40:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Cooking-Equipment> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:40:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Food-Prep-Equipment> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:40:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Products/Countertop-Equipment.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:40:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Dispensing-Solutions> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:40:56 [scrapy.extensions.logstats] INFO: Crawled 633 pages (at 0 pages/min), scraped 496 items (at 0 items/min)
2018-11-14 20:41:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Warewashing-Handling> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:41:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Buffet-Tabletop-Service> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:41:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Kitchen-Essentials> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:41:54 [scrapy.core.scraper] ERROR: Spider error processing <GET http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:42:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Cookware-Bakeware> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:42:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Steam-Table-Pans> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:42:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Events-Calendar.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:42:22 [scrapy.extensions.logstats] INFO: Crawled 645 pages (at 12 pages/min), scraped 499 items (at 3 items/min)
2018-11-14 20:42:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Products/Smallwares> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:42:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Customers/Consultants.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:43:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Press-Room.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:43:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Customers/Chains.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:43:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Suppliers> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:43:33 [scrapy.extensions.logstats] INFO: Crawled 654 pages (at 9 pages/min), scraped 503 items (at 4 items/min)
2018-11-14 20:43:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Customers.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

2018-11-14 20:43:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/OffNav/Wish-List/Your-Wish-List.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:44:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Customers/Dealers.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:44:06 [scrapy.extensions.logstats] INFO: Crawled 659 pages (at 5 pages/min), scraped 504 items (at 1 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:44:35 [scrapy.core.scraper] ERROR: Spider error processing <GET http://vollrathcompany.com/careers> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

2018-11-14 20:44:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://vollrath.com/Vollrath/Products.htm> (referer: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=(browser.page_source).encode('utf-8'))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 241, in check_response
    raise exception_class(message, screen, stacktrace, alert_text)
selenium.common.exceptions.UnexpectedAlertPresentException: Alert Text: None
Message: 

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:44:59 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.biscoind.com/Sunon>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-14 20:44:59 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.biscoind.com/LynTronInc>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-14 20:44:59 [scrapy.extensions.logstats] INFO: Crawled 663 pages (at 4 pages/min), scraped 508 items (at 4 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:45:42 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.biscoind.com/Unicorp>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2018-11-14 20:45:42 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.biscoind.com/Nord-Lock>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:45:59 [scrapy.extensions.logstats] INFO: Crawled 685 pages (at 22 pages/min), scraped 513 items (at 5 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:47:14 [scrapy.extensions.logstats] INFO: Crawled 698 pages (at 13 pages/min), scraped 525 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:48:24 [scrapy.extensions.logstats] INFO: Crawled 724 pages (at 26 pages/min), scraped 541 items (at 16 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:48:57 [scrapy.extensions.logstats] INFO: Crawled 729 pages (at 5 pages/min), scraped 549 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:49:58 [scrapy.extensions.logstats] INFO: Crawled 738 pages (at 9 pages/min), scraped 563 items (at 14 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:50:56 [scrapy.extensions.logstats] INFO: Crawled 746 pages (at 8 pages/min), scraped 575 items (at 12 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

       Crawling page: https://vollrath.com/Vollrath/Resource-Library.htm       Crawling page: https://vollrath.com/Vollrath-Serving-Systems       Crawling page: https://vollrath.com/Fabricator-Components       Crawling page: http://wheretobuy.vollrath.com       Crawling page: https://vollrath.com/Warming-Equipment       Crawling page: https://vollrath.com/Merchandising-Display-Solutions       Crawling page: https://vollrath.com/Frozen-Treat-Equipment       Crawling page: https://vollrath.com/Cooking-Equipment       Crawling page: https://vollrath.com/Food-Prep-Equipment       Crawling page: https://vollrath.com/Vollrath/Products/Countertop-Equipment.htm       Crawling page: https://vollrath.com/Dispensing-Solutions       Crawling page: https://vollrath.com/Warewashing-Handling       Crawling page: https://vollrath.com/Buffet-Tabletop-Service       Crawling page: https://vollrath.com/Kitchen-Essentials       Crawling page: https://www.biscoind.com/Manufacturers       valid
Crawling page: https://www.biscoind.com/store?o=BiscoProducts       valid
Crawling page: https://www.biscoind.com/store?body=basket       valid
Crawling page: http://vollrath.com/Traex-Full-Size-Compartment-Racks-1766.htm       Crawling page: https://vollrath.com/Cookware-Bakeware       Crawling page: https://vollrath.com/Steam-Table-Pans       Crawling page: https://vollrath.com/Vollrath/Events-Calendar.htm       Crawling page: https://vollrath.com/Vollrath/Products/Smallwares       Crawling page: https://vollrath.com/Vollrath/Customers/Consultants.htm       Crawling page: https://www.biscoind.com/store?body=address       valid
Crawling page: https://www.biscoind.com/Southco       valid
Crawling page: https://vollrath.com/Vollrath/Press-Room.htm       Crawling page: https://vollrath.com/Vollrath/Customers/Chains.htm       Crawling page: https://www.biscoind.com/Marathon       valid
Crawling page: https://www.biscoind.com/Keystone       valid
Crawling page: https://vollrath.com/Suppliers       Crawling page: https://vollrath.com/Vollrath/Customers.htm       Crawling page: https://www.biscoind.com/Birtcher-Pentair       valid
Crawling page: https://vollrath.com/Vollrath/OffNav/Wish-List/Your-Wish-List.htm       Crawling page: https://vollrath.com/Vollrath/Customers/Dealers.htm       Crawling page: https://www.biscoind.com/MolexConnectors       valid
Crawling page: https://www.biscoind.com/MillMaxPins       valid
Crawling page: http://vollrathcompany.com/careers       Crawling page: https://vollrath.com/Vollrath/Products.htm       Crawling page: https://www.biscoind.com/CaptiveFastenerCorp       valid
Crawling page: https://www.biscoind.com/Heyco       valid
Crawling page: https://www.biscoind.com/Panduit       valid
Crawling page: https://www.biscoind.com/Bivar/1000001/LED+Indication       valid
Crawling page: https://www.chipmos.com/english/hr/overview.aspx       valid
Crawling page: https://www.chipmos.com/english/contact/contact.aspx       valid
Crawling page: https://ajibio-pharma.com/privacy-policy/       valid
Crawling page: https://www.chipmos.com/english/news/list.aspx?MID=1       valid
Crawling page: https://www.chipmos.com/english/csr/editor.aspx?CID=2       valid
Crawling page: https://www.chipmos.com/english/ir/overview.aspx       valid
Crawling page: https://www.chipmos.com/english/product/overview.aspx       valid
Crawling page: https://www.chipmos.com/english/about/detail.aspx       valid
Crawling page: https://ajibio-pharma.com/cdmo-services/overview/       valid
Crawling page: https://ajibio-pharma.com/terms-of-use/       valid
Crawling page: https://ajibio-pharma.com/resources/       valid
Crawling page: https://www.chipmos.com/english/csr/editor.aspx?CID=3       valid
Crawling page: https://www.chipmos.com/english/csr/editor.aspx?CID=12       valid
Crawling page: https://www.chipmos.com/english/csr/editor.aspx?CID=7       valid
Crawling page: https://www.chipmos.com/english/csr/editor.aspx?CID=15       valid
Crawling page: https://www.chipmos.com/english/csr/editor.aspx?CID=10       valid
Crawling page: https://kajimausa.com/ja/%e3%83%9b%e3%83%bc%e3%83%a0/       valid
Crawling page: https://kajimausa.com/kajima-development-corporation/       valid
Crawling page: https://kajimausa.com/development-ventures-group-inc/       valid
Crawling page: https://kajimausa.com/anglebrook-golf-club/       valid
Crawling page: https://kajimausa.com/flournoy-development-company/       valid
Crawling page: https://kajimausa.com/core5-industrial-partners/       valid
Crawling page: https://kajimausa.com/contact-us/       valid
Crawling page: https://ajibio-pharma.com/cdmo-services/large-molecule-manufacturing/       valid
Crawling page: https://ajibio-pharma.com/cdmo-services/development-services/       valid
Crawling page: https://ajibio-pharma.com/cdmo-services/oligo-peptide-synthesis/       valid
Crawling page: https://ajibio-pharma.com/contact-us/overview/       valid
Crawling page: https://kajimausa.com/kcs-west/       valid
Crawling page: https://kajimausa.com/kbd-group-inc/       valid
Crawling page: https://kajimausa.com/batson-cook-development-company/       valid
Crawling page: https://kajimausa.com/real-estate-development-companies/       valid
Crawling page: https://ajibio-pharma.com/cdmo-services/highly-potent-adc-services/       valid
Crawling page: https://ajibio-pharma.com/cdmo-services/product-quality-analytical-services/       valid
Crawling page: https://ajibio-pharma.com/cdmo-services/drug-product-manufacturing/       valid
Crawling page: https://ajibio-pharma.com/cdmo-services/small-molecule-manufacturing/       valid
Crawling page: https://kajimausa.com/hawaiian-dredging-construction-company/       valid
Crawling page: https://kajimausa.com/flournoy-construction-company/       valid
Crawling page: https://kajimausa.com/batson-cook-company/       valid
Crawling page: https://kajimausa.com/the-austin-company/       valid
Crawling page: https://kajimausa.com/engineering-and-construction-companies/       valid
Crawling page: https://kajimausa.com/leisure-development/       valid
Crawling page: https://kajimausa.com/retail-development/       valid
Crawling page: https://kajimausa.com/residential-development/       valid
Crawling page: https://kajimausa.com/industrial-development/       valid
Crawling page: https://kajimausa.com/commercial-development/       valid
Crawling page: https://kajimausa.com/real-estate-development-services/       valid
Crawling page: https://kajimausa.com/design-build/       valid
Crawling page: https://www.sysmex.com/la/es/Pages/default.aspx       valid
Crawling page: https://www.sysmex.com/us/en/Pages/default.aspx       valid
Crawling page: https://kajimausa.com/consulting/       valid
Crawling page: https://kajimausa.com/general-contracting/       valid
Crawling page: https://kajimausa.com/architecture/       valid
Crawling page: https://kajimausa.com/engineering/       valid
Crawling page: https://kajimausa.com/first-50-years-in-the-usa/       valid
Crawling page: https://kajimausa.com/who-we-are/       valid
Crawling page: https://kajimausa.com/our-leadership-team/       valid
Crawling page: https://kajimausa.com/engineering-and-construction-services/       valid
Crawling page: https://kajimausa.com/       valid
Crawling page: https://www.sysmex.com/Pages/default.aspx       valid
Crawling page: https://www.cookbiotech.com/contact/       valid
Crawling page: https://www.cookbiotech.com/about/       valid
Crawling page: https://www.sysmex.com/la/pt/Pages/default.aspx       valid
Crawling page: https://www.cookbiotech.com/product_cat/feet-and-legs/       valid
Crawling page: https://www.cookbiotech.com/news/       valid
Crawling page: https://ajibio-pharma.com/news-events/overview/       valid
Crawling page: https://www.cookbiotech.com/product_cat/arms-and-hands/       valid
Crawling page: https://www.cookbiotech.com/product_cat/pelvis/       valid
Crawling page: https://www.cookbiotech.com/product_cat/torso/       valid
Crawling page: https://www.cookbiotech.com/product_cat/abdomen/       valid
Crawling page: https://www.cookbiotech.com/product_cat/head-and-neck/       valid
Crawling page: https://www.cookbiotech.com/products//bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:52:13 [scrapy.extensions.logstats] INFO: Crawled 766 pages (at 20 pages/min), scraped 588 items (at 13 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:52:57 [scrapy.extensions.logstats] INFO: Crawled 766 pages (at 0 pages/min), scraped 595 items (at 7 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:54:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.chipmos.com/.english/product/detail.aspx?MID=5>: HTTP status code is not handled or not allowed
2018-11-14 20:54:09 [scrapy.extensions.logstats] INFO: Crawled 785 pages (at 19 pages/min), scraped 605 items (at 10 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:54:25 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.chipmos.com/english/produc/overview.aspx>: HTTP status code is not handled or not allowed
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:55:09 [scrapy.extensions.logstats] INFO: Crawled 792 pages (at 7 pages/min), scraped 616 items (at 11 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:56:06 [scrapy.extensions.logstats] INFO: Crawled 801 pages (at 9 pages/min), scraped 625 items (at 9 items/min)
/bin/sh: 1: kill: No such process

2018-11-14 20:56:13 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/news/mbxon4wrz-ms5e2_.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:56:42 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/news/sxun_u_ax_0cow3u.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 20:56:42 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/about/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 20:56:42 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/news/c-od0gjn000000gld5.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:57:05 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/news/9fejfmd4je8r8dzf.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 20:57:05 [scrapy.extensions.logstats] INFO: Crawled 811 pages (at 10 pages/min), scraped 633 items (at 8 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:57:53 [root] ERROR: Unable to find match for url: https://oronite.force.com/OroniteConnect/s/login/SelfRegister
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:58:08 [scrapy.extensions.logstats] INFO: Crawled 823 pages (at 12 pages/min), scraped 642 items (at 9 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 20:59:25 [scrapy.extensions.logstats] INFO: Crawled 831 pages (at 8 pages/min), scraped 653 items (at 11 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 21:00:09 [scrapy.extensions.logstats] INFO: Crawled 839 pages (at 8 pages/min), scraped 660 items (at 7 items/min)
/bin/sh: 1: kill: No such process

2018-11-14 21:00:14 [root] ERROR: Unable to find match for url: https://oronite.force.com/OroniteConnect/s/
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 21:00:31 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/news/d6gnypbtww5fli9u.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-14 21:00:31 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/about/business.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-14 21:00:31 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/sitemap/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 21:00:53 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/privacy/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-14 21:01:09 [scrapy.extensions.logstats] INFO: Crawled 844 pages (at 5 pages/min), scraped 670 items (at 10 items/min)
2018-11-14 21:01:09 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/terms/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-14 21:01:09 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/about/history.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-14 21:01:09 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/ir/shareholder.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-14 21:01:09 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/news/qaohknbupxzsv2xb.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:09 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/an/lcms/nexera-mikros/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:09 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/an/ndi/ct/xdimensus.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:09 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/med/products/icg/od0gjn000000hpk7.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:09 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/news/c-od0gjn000000i3cs.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:09 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/news/c-od0gjn000000h1if.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:09 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/news/65j7shbqwzsuy39p.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:09 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/news/qpydd5689d817qam.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/med/products/angio/06.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/an/molecular_spectro/uv/uv-1900/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/ir/governance.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/ir/factbook.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/about/pressrelease/5iqj1d0000021xfl.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/ir/earnings.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/news/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/news/gguyzh2jmz42i0iq.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/opt/products/ref/kpr300.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/news/od0gjn00000055su.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/news/f2a3i7hbyfbh_ajm.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/ir/presentation.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/an/lifescience/imaging/reveal.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/news/96hl2-94myujbo5i.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/an/lcms/9030/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/news/9l8iu7d7jeclwubp.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/an/balance/analytical/ap.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/an/gcms/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/news/3951dl7sfmacb1d9.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/opt/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/ir/library/finance.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/ir/annual.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/news/18wrmslcrgs2a7k_.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/ecms/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/about/momentum/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/about/imagine/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/industry/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/med/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/about/corporate_advertising.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/ferromagnetic/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/aircraft/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/hydraulic/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/an/hplc/i-series/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/about/magazine/tec_news/srv74_12/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/research_and_development/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/visionary/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/med/rsna/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/med/minimallyinvasive/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/contact/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
/bin/sh: 1: kill: No such process

2018-11-14 21:01:15 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/links/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:15 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.shimadzu.com/brand/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: Too many Content-Length headers; response is invalid>]
2018-11-14 21:01:16 [scrapy.core.engine] INFO: Closing spider (finished)
2018-11-14 21:01:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 388,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 2,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 4,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 166,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 216,
 'downloader/request_bytes': 639601,
 'downloader/request_count': 1461,
 'downloader/request_method_count/GET': 1461,
 'downloader/response_bytes': 19009729,
 'downloader/response_count': 1073,
 'downloader/response_status_count/200': 833,
 'downloader/response_status_count/301': 219,
 'downloader/response_status_count/302': 8,
 'downloader/response_status_count/403': 1,
 'downloader/response_status_count/404': 9,
 'downloader/response_status_count/406': 3,
 'dupefilter/filtered': 67,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 11, 14, 21, 1, 16, 1498),
 'httperror/response_ignored_count': 13,
 'httperror/response_ignored_status_count/403': 1,
 'httperror/response_ignored_status_count/404': 9,
 'httperror/response_ignored_status_count/406': 3,
 'item_scraped_count': 671,
 'log_count/ERROR': 199,
 'log_count/INFO': 118,
 'log_count/WARNING': 1,
 'memusage/max': 195162112,
 'memusage/startup': 100610048,
 'offsite/domains': 110,
 'offsite/filtered': 528,
 'request_depth_max': 1,
 'response_received_count': 845,
 'retry/count': 303,
 'retry/max_reached': 85,
 'retry/reason_count/twisted.internet.error.ConnectError': 2,
 'retry/reason_count/twisted.internet.error.TimeoutError': 4,
 'retry/reason_count/twisted.web._newclient.ResponseFailed': 113,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 184,
 'scheduler/dequeued': 1461,
 'scheduler/dequeued/memory': 1461,
 'scheduler/enqueued': 1461,
 'scheduler/enqueued/memory': 1461,
 'spider_exceptions/TimeoutException': 1,
 'spider_exceptions/UnexpectedAlertPresentException': 109,
 'start_time': datetime.datetime(2018, 11, 14, 19, 18, 54, 285850)}
2018-11-14 21:01:16 [scrapy.core.engine] INFO: Spider closed (finished)
       valid
Crawling page: https://www.cookbiotech.com       valid
Crawling page: https://www.cookbiotech.com/technology/       valid
Crawling page: https://www.novustek.net/contact       valid
Crawling page: https://www.novustek.net/microsoft-skype-for-business       valid
Crawling page: https://www.novustek.net/support       valid
Crawling page: https://www.novustek.net/autotask-work-place       valid
Crawling page: https://www.novustek.net/microsoft-office-365       valid
Crawling page: https://www.acushnetgolf.com/?@USER.LanguageCode=gb&@USER.LocaleID=3       valid
Crawling page: https://www.acushnetgolf.com/?@USER.LanguageCode=us&@USER.LocaleID=12       valid
Crawling page: https://www.novustek.net/about-nti       valid
Crawling page: https://www.novustek.net/managed-it-support       valid
Crawling page: https://www.novustek.net/blog       valid
Crawling page: https://www.novustek.net/by-industry       valid
Crawling page: https://www.novustek.net/       valid
Crawling page: https://www.chipmos.com/english/hr/detail.aspx?HID=3       valid
Crawling page: https://www.chipmos.com/english/csr/editor.aspx?CID=11       valid
Crawling page: https://www.chipmos.com/english/ir/calendar.aspx       valid
Crawling page: https://www.chipmos.com/english/ir/industry.aspx       valid
Crawling page: https://www.chipmos.com/english/hr/detail.aspx?HID=5       valid
Crawling page: https://www.chipmos.com/english/hr/detail.aspx?HID=8       valid
Crawling page: https://www.chipmos.com/english/ir/info.aspx       valid
Crawling page: https://www.chipmos.com/english/ir/editor01.aspx       valid
Crawling page: https://www.chipmos.com/english/ir/financial.aspx       valid
Crawling page: https://www.chipmos.com/english/ir/company.aspx       valid
Crawling page: https://www.chipmos.com/english/product/detail.aspx?MID=15       valid
Crawling page: https://www.chipmos.com/english/product/detail.aspx?MID=12       valid
Crawling page: https://www.chipmos.com/english/product/detail.aspx?MID=6       valid
Crawling page: https://www.chipmos.com/english/about/detail.aspx?AID=10       valid
Crawling page: https://www.chipmos.com/english/about/detail.aspx?AID=9       valid
Crawling page: https://www.chipmos.com/english/about/detail.aspx?AID=8       valid
Crawling page: https://www.acushnetgolf.com/?@USER.LanguageCode=en&@USER.LocaleID=6       valid
Crawling page: https://www.chipmos.com/english/about/detail.aspx?AID=5       valid
Crawling page: https://www.chipmos.com/english/about/detail.aspx?AID=1       valid
Crawling page: https://www.chipmos.com/chinese/product/detail.aspx?MID=6       valid
Crawling page: https://www.chipmos.com/english/product/detail.aspx?MID=1       valid
Crawling page: https://www.chipmos.com/english/product/detail.aspx?MID=5       valid
Crawling page: https://www.chipmos.com/index.aspx       valid
Crawling page: https://www.chipmos.com/english/csr/overview.aspx       valid
Crawling page: https://www.shimadzu.com/an/index.html       valid
Crawling page: https://ajibio-pharma.com/dedication-to-quality/overview/       valid
Crawling page: https://www.oronite.com/sitemap.aspx       valid
Crawling page: https://ajibio-pharma.com/about/overview/       valid
Crawling page: https://www.oronite.com/about/news/latest-news/2018/oronite-ready-for-IMO-2020.aspx       valid
Crawling page: https://ajibio-pharma.com/careers/overview/       valid
Crawling page: https://www.oronite.com/about/news/latest-news/2018/OLOA-61011-is-MAN-M3677-approved.aspx       valid
Crawling page: https://www.oronite.com/about/news/latest-news/2018/introduces-OLOA-54720.aspx       valid
Crawling page: https://www.oronite.com/about/news/latest-news/2018/breaks-ground-on-china-expansion.aspx       valid
Crawling page: https://www.oronite.com/additive-industry-leader/industry-involvement.aspx       valid
Crawling page: https://www.oronite.com/additive-industry-leader/investing-future.aspx       valid
Crawling page: https://www.oronite.com/additive-industry-leader/default.aspx       valid
Crawling page: https://www.oronite.com/how-we-add-value/innovative-technology.aspx       valid
Crawling page: https://www.oronite.com/how-we-add-value/enduring-relationships.aspx       valid
Crawling page: https://www.oronite.com/how-we-add-value/exceptional-reliability.aspx       valid
Crawling page: https://www.oronite.com/how-we-add-value/default.aspx       valid
Crawling page: https://www.oronite.com/products-technology/small-engines/watercraft.aspx       valid
Crawling page: https://www.oronite.com/default.aspx       valid
Crawling page: https://www.oronite.com/products-technology/small-engines/motorcycle.aspx       valid
Crawling page: https://www.oronite.com/products-technology/small-engines/default.aspx       valid
Crawling page: https://oronite.force.com/OroniteConnect/s/login/SelfRegister       valid
Crawling page: https://www.oronite.com/products-technology/gas-engine-oils/stationary-power-mobile.aspx       valid
Crawling page: https://www.oronite.com/products-technology/gas-engine-oils/default.aspx       valid
Crawling page: https://www.oronite.com/products-technology/viscosity-modifiers/paratone.aspx       valid
Crawling page: https://www.oronite.com/products-technology/viscosity-modifiers/default.aspx       valid
Crawling page: https://www.oronite.com/products-technology/automotive/hdeo/ACEA-HDEO.aspx       valid
Crawling page: https://www.oronite.com/products-technology/automotive/hdeo/API-HDEO.aspx       valid
Crawling page: https://www.oronite.com/products-technology/automotive/heavy-duty-engine-oils.aspx       valid
Crawling page: https://www.oronite.com/products-technology/automotive/pcmo/API-ILSAC-PCMO.aspx       valid
Crawling page: https://www.oronite.com/products-technology/automotive/pcmo/ACEA_PCMO.aspx       valid
Crawling page: https://www.oronite.com/products-technology/automotive/passenger-car-motor-oils.aspx       valid
Crawling page: https://www.oronite.com/products-technology/automotive/default.aspx       valid
Crawling page: https://www.oronite.com/products-technology/fuel-additives.aspx       valid
Crawling page: https://www.oronite.com/products-technology/chemicals.aspx       valid
Crawling page: https://www.oronite.com/products-technology/driveline-fluids.aspx       valid
Crawling page: https://www.oronite.com/products-technology/railroad-inland-marine.aspx       valid
Crawling page: https://www.oronite.com/products-technology/marine.aspx       valid
Crawling page: https://www.oronite.com/products-technology/hydraulic-oils.aspx       valid
Crawling page: https://www.oronite.com/products-technology/default.aspx       valid
Crawling page: https://www.oronite.com/about/news/API-SN-plus.aspx       valid
Crawling page: https://www.oronite.com/about/news/low-speed-pre-ignition.aspx       valid
Crawling page: https://oronite.force.com/OroniteConnect/s/       valid
Crawling page: https://www.oronite.com/about/news/default.aspx       valid
Crawling page: https://www.oronite.com/about/china-overview.aspx       valid
Crawling page: https://www.oronite.com/about/contact.aspx       valid
Crawling page: https://www.oronite.com/about/careers.aspx       valid
Crawling page: https://www.oronite.com/about/leadership/default.aspx       valid
Crawling page: https://www.oronite.com/about/history.aspx       valid
Crawling page: https://www.oronite.com/about/vision.aspx       valid
Crawling page: https://www.oronite.com/about/default.aspx       valid
Crawling page: https://www.oronite.com/       valid
Crawling page: https://www.shimadzu.com/       valid
