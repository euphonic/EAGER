nohup: ignoring input
2018-11-12 01:16:59 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: FirmDB)
2018-11-12 01:16:59 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.6 (default, Sep 12 2018, 18:26:19) - [GCC 8.0.1 20180414 (experimental) [trunk revision 259383]], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.15.0-1023-aws-x86_64-with-Ubuntu-18.04-bionic
2018-11-12 01:16:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'FirmDB', 'CONCURRENT_REQUESTS_PER_DOMAIN': 4, 'DEPTH_LIMIT': '1', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'FirmDB.spiders', 'ROBOTSTXT_OBEY': 'False', 'SPIDER_MODULES': ['FirmDB.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2049.0 Safari/537.36'}
2018-11-12 01:16:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2018-11-12 01:16:59 [py.warnings] WARNING: /home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2018-11-12 01:16:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'random_useragent.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-11-12 01:16:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-11-12 01:16:59 [scrapy.middleware] INFO: Enabled item pipelines:
['FirmDB.pipelines.FirmDBPipeline']
2018-11-12 01:16:59 [scrapy.core.engine] INFO: Spider opened
2018-11-12 01:16:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
/bin/sh: 1: kill: Illegal number: 
2018-11-12 01:18:12 [scrapy.extensions.logstats] INFO: Crawled 44 pages (at 44 pages/min), scraped 15 items (at 15 items/min)
Checking url for Uni-Charm Corporation
	Trying http://unicharm.co.jp/english/index.html
	Trying http://www.unicharm.co.jp/english/index.html
Checking url for FPInnovations
	Trying http://fpinnovations.ca/
	Trying http://www.fpinnovations.ca/
	Trying https://fpinnovations.ca/
	Trying https://www.fpinnovations.ca/
Checking url for Kerr Corporation
	Trying http://kerrdental.com/
Checking url for Rockwell Automation Technologies
	Trying http://rockwellautomation.com/site-selection.html
Checking url for Sarepta Therapeutics
	Trying http://sarepta.com/
Checking url for Pacific Biosciences of California
	Trying http://pacb.com/
Checking url for Sima Therapeutics
	Trying http://semma-tx.com/
Checking url for Universal Leaf Tobacco Co.
	Trying http://universalcorp.com/
	Trying http://www.universalcorp.com/
Checking url for Babcock Power Services
	Trying http://babcockpower.com/
Checking url for Fike Corporation
	Trying http://fike.com/
Checking url for Carbon3D
	Trying http://carbon3d.com/
Checking url for Chromalox
	Trying http://chromalox.com/
Checking url for Meyer Tool
	Trying http://meyertool.com/
Checking url for Thoratec Corporation
	Trying http://thoratec.com/
Checking url for CSEM Centre Suisse d'Electronique et de Microtechnique SA-Recherche et Developpement
	Trying http://csem.ch/
Checking url for Piksel
	Trying http://piksel.com/
Checking url for PDF Solutions
	Trying http://pdf.com
Checking url for Vizio Inc.
	Trying http://vizio.com/
Checking url for Amicus Therapeutics
	Trying http://amicusrx.com/
	Trying http://www.amicusrx.com/
	Trying https://amicusrx.com/
	Trying https://www.amicusrx.com/
Checking url for HRL Laboratories
	Trying http://hrl.com/
Checking url for New England Biolabs
	Trying http://neb.com/
Checking url for Praxair Technology
	Trying http://praxair.com/
Checking url for Praxair S.T. Technology
	Trying http://praxair.com/
Checking url for Shin-Etsu Chemical Co.
	Trying http://shinetsu.co.jp/en/
	Trying http://www.shinetsu.co.jp/en/
Checking url for Unifrax I LLC
	Trying http://unifrax.com/
	Trying http://www.unifrax.com/
Checking url for EXOS LLC
	Trying http://teamexos.com/
Checking url for Medicis Pharmaceutical Corporation
	Trying http://medicis.com/
Checking url for AG ENERGY SOLUTIONS
	Trying http://ag.energy/
Checking url for Richtek Technology Corporation
	Trying http://richtek.com/
	Trying http://www.richtek.com/
Checking url for VERILY LIFE SCIENCES LLC
	Trying http://verily.com/
[{'domain': 'www.unicharm.co.jp',
  'firm_name': 'Uni-Charm Corporation',
  'url': 'http://www.unicharm.co.jp/english/index.html'},
 {'domain': 'www.kerrdental.com',
  'firm_name': 'Kerr Corporation',
  'url': 'https://www.kerrdental.com/'},
 {'domain': 'www.rockwellautomation.com',
  'firm_name': 'Rockwell Automation Technologies',
  'url': 'https://www.rockwellautomation.com/site-selection.html'},
 {'domain': 'www.sarepta.com',
  'firm_name': 'Sarepta Therapeutics',
  'url': 'https://www.sarepta.com/'},
 {'domain': 'www.pacb.com',
  'firm_name': 'Pacific Biosciences of California',
  'url': 'https://www.pacb.com/'},
 {'domain': 'www.semma-tx.com',
  'firm_name': 'Sima Therapeutics',
  'url': 'http://www.semma-tx.com/'},
 {'domain': 'www.universalcorp.com',
  'firm_name': 'Universal Leaf Tobacco Co.',
  'url': 'http://www.universalcorp.com/'},
 {'domain': 'www.babcockpower.com',
  'firm_name': 'Babcock Power Services',
  'url': 'https://www.babcockpower.com/'},
 {'domain': 'www.fike.com',
  'firm_name': 'Fike Corporation',
  'url': 'https://www.fike.com/'},
 {'domain': 'www.carbon3d.com',
  'firm_name': 'Carbon3D',
  'url': 'https://www.carbon3d.com/'},
 {'domain': 'www.chromalox.com',
  'firm_name': 'Chromalox',
  'url': 'https://www.chromalox.com/'},
 {'domain': 'www.meyertool.com',
  'firm_name': 'Meyer Tool',
  'url': 'https://www.meyertool.com/'},
 {'domain': 'www.thoratec.com',
  'firm_name': 'Thoratec Corporation',
  'url': 'http://www.thoratec.com/'},
 {'domain': 'www.csem.ch',
  'firm_name': "CSEM Centre Suisse d'Electronique et de Microtechnique "
               'SA-Recherche et Developpement',
  'url': 'https://www.csem.ch/home'},
 {'domain': 'piksel.com', 'firm_name': 'Piksel', 'url': 'https://piksel.com/'},
 {'domain': 'pdf.com',
  'firm_name': 'PDF Solutions',
  'url': 'http://pdf.com/Home'},
 {'domain': 'www.vizio.com',
  'firm_name': 'Vizio Inc.',
  'url': 'https://www.vizio.com/'},
 {'domain': 'www.hrl.com',
  'firm_name': 'HRL Laboratories',
  'url': 'http://www.hrl.com/'},
 {'domain': 'www.neb.com',
  'firm_name': 'New England Biolabs',
  'url': 'https://www.neb.com/'},
 {'domain': 'www.praxair.com',
  'firm_name': 'Praxair Technology',
  'url': 'https://www.praxair.com/'},
 {'domain': 'www.praxair.com',
  'firm_name': 'Praxair S.T. Technology',
  'url': 'https://www.praxair.com/'},
 {'domain': 'www.shinetsu.co.jp',
  'firm_name': 'Shin-Etsu Chemical Co.',
  'url': 'http://www.shinetsu.co.jp/en/'},
 {'domain': 'www.unifrax.com',
  'firm_name': 'Unifrax I LLC',
  'url': 'http://www.unifrax.com/'},
 {'domain': 'www.teamexos.com',
  'firm_name': 'EXOS LLC',
  'url': 'https://www.teamexos.com/'},
 {'domain': 'www.bauschhealth.com',
  'firm_name': 'Medicis Pharmaceutical Corporation',
  'url': 'https://www.bauschhealth.com/'},
 {'domain': 'ag.energy',
  'firm_name': 'AG ENERGY SOLUTIONS',
  'url': 'https://ag.energy/'},
 {'domain': 'www.richtek.com',
  'firm_name': 'Richtek Technology Corporation',
  'url': 'https://www.richtek.com/'},
 {'domain': 'verily.com',
  'firm_name': 'VERILY LIFE SCIENCES LLC',
  'url': 'https://verily.com/'}]
Missing Amicus Therapeutics, FPInnovations in the fixed urls list
Pages to scrape: ['http://www.unicharm.co.jp/english/index.html', 'https://www.kerrdental.com/', 'https://www.rockwellautomation.com/site-selection.html', 'https://www.sarepta.com/', 'https://www.pacb.com/', 'http://www.semma-tx.com/', 'http://www.universalcorp.com/', 'https://www.babcockpower.com/', 'https://www.fike.com/', 'https://www.carbon3d.com/', 'https://www.chromalox.com/', 'https://www.meyertool.com/', 'http://www.thoratec.com/', 'https://www.csem.ch/home', 'https://piksel.com/', 'http://pdf.com/Home', 'https://www.vizio.com/', 'http://www.hrl.com/', 'https://www.neb.com/', 'https://www.praxair.com/', 'https://www.praxair.com/', 'http://www.shinetsu.co.jp/en/', 'http://www.unifrax.com/', 'https://www.teamexos.com/', 'https://www.bauschhealth.com/', 'https://ag.energy/', 'https://www.richtek.com/', 'https://verily.com/']
Crawling page: https://www.babcockpower.com/       valid
Crawling page: https://www.fike.com/       valid
Crawling page: http://www.thoratec.com/about-us/contact-us.aspx       valid
Crawling page: http://pdf.com/Home       valid
Crawling page: https://www.fike.com/terms-conditions-old/       valid
Crawling page: http://www.semma-tx.com/contact       valid
Crawling page: http://www.thoratec.com/index.aspx       valid
Crawling page: http://www.semma-tx.com/careers       valid
Crawling page: https://www.praxair.com/investor-relations/stock-information-and-price       valid
Crawling page: http://www.thoratec.com/about-us/media-room/index.aspx       valid
Crawling page: http://www.thoratec.com/site-map/index.aspx       valid
Crawling page: http://www.thoratec.com/legal/index.aspx       valid
Crawling page: https://www.praxair.com/resource-library/sds       valid
Crawling page: http://www.thoratec.com/vad-trials-outcomes/ongoing-clinical-trials/shieldii.aspx       valid
Crawling page: http://www.unifrax.com/contact-us/       valid
Crawling page: https://www.praxair.com/industries/water-and-wastewater-treatment       valid
Crawling page: http://www.thoratec.com/medical-professionals/resource-library/bibliography.aspx       valid
Crawling page: https://www.praxair.com/our-company/safety-and-environment/distribution-safety       valid
Crawling page: http://www.thoratec.com/media/youtube-player.aspx?clip=5Zqabp4xKSE&KeepThis=true&TB_iframe=true&height=429&width=675       valid
Crawling page: https://www.praxair.com/ie-compatibility-view       valid
Crawling page: https://www.praxair.com/site-map       valid
Crawling page: http://www.thoratec.com/select-location       valid
Crawling page: https://www.praxair.com/resource-library/videos2018-11-12 01:19:01 [scrapy.extensions.logstats] INFO: Crawled 60 pages (at 16 pages/min), scraped 26 items (at 11 items/min)
2018-11-12 01:20:27 [scrapy.extensions.logstats] INFO: Crawled 64 pages (at 4 pages/min), scraped 39 items (at 13 items/min)
2018-11-12 01:21:17 [scrapy.extensions.logstats] INFO: Crawled 77 pages (at 13 pages/min), scraped 48 items (at 9 items/min)
2018-11-12 01:22:04 [scrapy.extensions.logstats] INFO: Crawled 86 pages (at 9 pages/min), scraped 54 items (at 6 items/min)
/bin/sh: 1: kill: Illegal number: 
2018-11-12 01:23:04 [scrapy.extensions.logstats] INFO: Crawled 96 pages (at 10 pages/min), scraped 64 items (at 10 items/min)
2018-11-12 01:24:13 [scrapy.extensions.logstats] INFO: Crawled 100 pages (at 4 pages/min), scraped 75 items (at 11 items/min)
2018-11-12 01:25:00 [scrapy.extensions.logstats] INFO: Crawled 109 pages (at 9 pages/min), scraped 81 items (at 6 items/min)
2018-11-12 01:26:16 [scrapy.extensions.logstats] INFO: Crawled 120 pages (at 11 pages/min), scraped 89 items (at 8 items/min)
2018-11-12 01:27:09 [scrapy.extensions.logstats] INFO: Crawled 123 pages (at 3 pages/min), scraped 95 items (at 6 items/min)
2018-11-12 01:28:23 [scrapy.extensions.logstats] INFO: Crawled 129 pages (at 6 pages/min), scraped 102 items (at 7 items/min)
2018-11-12 01:30:09 [scrapy.extensions.logstats] INFO: Crawled 130 pages (at 1 pages/min), scraped 104 items (at 2 items/min)
2018-11-12 01:31:28 [scrapy.extensions.logstats] INFO: Crawled 138 pages (at 8 pages/min), scraped 113 items (at 9 items/min)
2018-11-12 01:32:08 [scrapy.extensions.logstats] INFO: Crawled 145 pages (at 7 pages/min), scraped 117 items (at 4 items/min)
2018-11-12 01:33:07 [scrapy.extensions.logstats] INFO: Crawled 150 pages (at 5 pages/min), scraped 123 items (at 6 items/min)
2018-11-12 01:34:09 [scrapy.extensions.logstats] INFO: Crawled 157 pages (at 7 pages/min), scraped 129 items (at 6 items/min)
2018-11-12 01:35:04 [scrapy.extensions.logstats] INFO: Crawled 159 pages (at 2 pages/min), scraped 134 items (at 5 items/min)
       valid
Crawling page: https://www.praxair.com/resource-library/specification-sheets-and-brochures       valid
Crawling page: https://www.csem.ch/EnergySystems       valid
Crawling page: https://www.csem.ch/Semiconductors       valid
Crawling page: https://www.praxair.com/privacy-statement       valid
Crawling page: https://www.praxair.com/legal-notice       valid
Crawling page: https://www.csem.ch/Watchmaking       valid
Crawling page: https://www.csem.ch/Space       valid
Crawling page: https://www.csem.ch/Food       valid
Crawling page: https://www.csem.ch/Environment       valid
Crawling page: https://www.csem.ch/Security       valid
Crawling page: https://www.csem.ch/IndustrialControl       valid
Crawling page: https://www.csem.ch/Healthcare       valid
Crawling page: https://www.csem.ch/Energy       valid
Crawling page: https://www.csem.ch/Communications       valid
Crawling page: https://www.csem.ch/LifeSciences       valid
Crawling page: https://www.csem.ch/Aeronautics       valid
Crawling page: https://www.csem.ch/Vision       valid
Crawling page: https://www.csem.ch/Wireless       valid
Crawling page: https://www.csem.ch/Module-PV       valid
Crawling page: https://www.csem.ch/ThinFilm-PV       valid
Crawling page: https://www.csem.ch/SystemOnChip       valid
Crawling page: https://www.csem.ch/PrintedElectronics       valid
Crawling page: https://www.csem.ch/Automation       valid
Crawling page: https://www.csem.ch/Instrumentation       valid
Crawling page: https://www.csem.ch/MedicalTechnologies       valid
Crawling page: https://www.csem.ch/Biosurface       valid
Crawling page: https://www.csem.ch/Nanosurface       valid
Crawling page: https://www.csem.ch/AdvancedMicroManufacturing       valid
Crawling page: https://www.csem.ch/MEMSIntegration       valid
Crawling page: https://www.csem.ch/digitaljourney       valid
Crawling page: https://www.csem.ch/Press       valid
Crawling page: https://www.csem.ch/Events       valid
Crawling page: https://www.csem.ch/home       valid
Crawling page: https://www.csem.ch/MEMSDesign       valid
Crawling page: https://www.csem.ch/Contact       valid
Crawling page: https://www.csem.ch/Publications       valid
Crawling page: https://www.csem.ch/Trends/FoodAgriculture       valid
Crawling page: https://www.csem.ch/Jobs       valid
Crawling page: https://www.csem.ch/Trends/Smartwatches       valid
Crawling page: https://www.csem.ch/Trends/ProstheticsImplants       valid
Crawling page: https://www.csem.ch/Trends/Industry40       valid
Crawling page: https://www.csem.ch/Trends/TheInternetofThings       valid
Crawling page: https://www.csem.ch/Trends/PersonalizedHealth       valid
Crawling page: https://www.csem.ch/Trends/TheFutureofEnergy       valid
Crawling page: https://www.csem.ch/Trends       valid
Crawling page: https://www.csem.ch/Solutions/WorkingwithCSEM       valid
Crawling page: https://www.csem.ch/Solutions/Services       valid
Crawling page: https://www.csem.ch/Solutions/ByIndustry       valid
Crawling page: https://www.csem.ch/Solutions/ByExpertise       valid
Crawling page: https://www.csem.ch/Solutions       valid
Crawling page: https://www.csem.ch/About/Start-ups       valid
Crawling page: https://www.csem.ch/About/Socialresponsibility       valid
Crawling page: https://www.neb.com/terms-of-use       valid
Crawling page: https://www.csem.ch/About/Certifications       valid
Crawling page: https://www.csem.ch/About/Governance       valid
Crawling page: https://www.csem.ch/About/Partnerships       valid
Crawling page: https://www.csem.ch/About/Awards       valid
Crawling page: https://www.csem.ch/About/MissionVisionStrategy       valid
Crawling page: https://www.csem.ch/About/History       valid
Crawling page: https://www.csem.ch/About       valid
Crawling page: https://www.csem.ch/Home       valid
Crawling page: https://www.neb.com/support/terms-of-sale       valid
Crawling page: https://www.neb.com/privacy-policy       valid
Crawling page: https://www.neb.com/trademarks       valid
Crawling page: https://www.neb.com/site-map       valid
Crawling page: https://www.neb.com/tools-and-resources/video-library/nebtv-episode-21       valid
Crawling page: https://www.neb.com/tools-and-resources/search?type=Protocol       valid
Crawling page: https://www.neb.com/tools-and-resources/neb-mobile-applications       valid
Crawling page: https://www.neb.com/tools-and-resources/video-library?device=modal&videoid=%7B1e69a0ed-6c16-4e77-87cc-6cd4ed6b2b50%7D       valid
Crawling page: https://www.neb.com/tools-and-resources/video-library/neb-tv-ep-22-technical-support-at-neb       valid
Crawling page: https://www.neb.com/tools-and-resources/video-library?device=modal&videoid=%7Bdfdf28e2-d90d-4fc3-8c87-602494f349f7%7D       valid
Crawling page: https://www.neb.com/tools-and-resources/video-library/behind-the-paper-nice-seq-high-resolution-open-chromatin-profiling       valid
Crawling page: https://www.neb.com/tools-and-resources/video-library?device=modal&videoid=%7B6b2eefa1-a055-455b-a723-40b2e78456a4%7D       valid
Crawling page: https://www.neb.com/tools-and-resources/video-library/colorimetric-lamp-in-point-of-care-diagnostics       valid
Crawling page: https://www.neb.com/podcasts/podcasts/podcast-4       valid
Crawling page: https://www.neb.com/tools-and-resources/video-library/base-modifications-affecting-rna-polymerase-and-reverse-transcriptase-fidelity       valid
Crawling page: https://www.neb.com/new-to-cloning/new-to-cloning       valid
Crawling page: https://www.neb.com/products/e6420-nebnext-single-cell-low-input-rna-library-prep-kit-for-illumina       valid
Crawling page: https://www.neb.com/products/e6631-nebnext-direct-custom-ready-panels       valid
Crawling page: https://www.neb.com/products/competent-cells/cloning-competent-cell-strains/cloning-competent-cell-strains       valid
Crawling page: https://www.neb.com/account/create-account       valid
Crawling page: https://www.neb.com/cart/view-cart       valid
Crawling page: https://www.neb.com/quick-order       valid
Crawling page: https://www.neb.com/about-neb/careers       valid
Crawling page: https://www.neb.com/about-neb/certifications       valid
Crawling page: https://www.neb.com/about-neb/passion-in-science-awards       valid
Crawling page: https://www.neb.com/about-neb/leadership       valid
Crawling page: https://www.neb.com/about-neb/business-development-opportunities       valid
Crawling page: https://www.neb.com/new-student-starter-pack       valid
Crawling page: https://www.neb.com/about-neb/environmental-commitment       valid
Crawling page: https://www.neb.com/about-neb       valid
Crawling page: https://www.neb.com/about-neb/news-and-press-releases       valid
Crawling page: https://www.neb.com/about-neb/contact-us       valid
Crawling page: https://www.neb.com/about-neb/neb-overview       valid
Crawling page: https://www.neb.com/nebsolutions       valid
Crawling page: https://www.neb.com/support/support-for-the-biotech-and-biopharma-industry       valid
Crawling page: https://www.neb.com/about-neb/quality-at-neb       valid
Crawling page: https://www.neb.com/research       valid
Crawling page: https://www.neb.com/support/new-lab-discount       valid
Crawling page: https://www.neb.com/freezer-programs       valid
Crawling page: https://www.neb.com/support/customer-feedback       valid
Crawling page: https://www.neb.com/support/contact-information-for-us-sales-inquiries-and-support       valid
Crawling page: https://www.neb.com/support/international-ordering-support       valid
Crawling page: https://www.neb.com/support/us-customer-and-order-support       valid
Crawling page: https://www.neb.com/support/neb-technical-support       valid
Crawling page: https://www.neb.com/support       valid
Crawling page: https://www.neb.com/promoting-science-education       valid
Crawling page: https://www.neb.com/support/packaging-shipping-and-delivery       valid
Crawling page: https://www.neb.com/support/catalog-and-literature-request       valid
Crawling page: https://www.neb.com/webinar/neb-tv-webinar-series       valid
Crawling page: https://www.neb.com/tools-and-resources/interactive-tools       valid
Crawling page: https://www.neb.com/tools-and-resources/video-library2018-11-12 01:36:14 [scrapy.extensions.logstats] INFO: Crawled 176 pages (at 17 pages/min), scraped 142 items (at 8 items/min)
2018-11-12 01:37:55 [scrapy.extensions.logstats] INFO: Crawled 179 pages (at 3 pages/min), scraped 153 items (at 11 items/min)
2018-11-12 01:38:00 [scrapy.extensions.logstats] INFO: Crawled 179 pages (at 0 pages/min), scraped 154 items (at 1 items/min)
2018-11-12 01:38:03 [root] ERROR: Unable to find match for url: https://careers.vizio.com?utm_source=careersite
2018-11-12 01:38:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.neb.com/-/media/nebus/campaign/golden-gate/breakingthroughlimitsgoldengate_tn.pdf?la=en> (referer: https://www.neb.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 59, in parse_links
    res = response.replace(body=browser.page_source)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 81, in replace
    return cls(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 22, in __init__
    self._set_body(body)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py", line 57, in _set_body
    "Response body must be bytes. "
TypeError: Response body must be bytes. If you want to pass unicode body use TextResponse or HtmlResponse.
2018-11-12 01:39:12 [scrapy.extensions.logstats] INFO: Crawled 201 pages (at 22 pages/min), scraped 171 items (at 17 items/min)
2018-11-12 01:40:05 [scrapy.extensions.logstats] INFO: Crawled 215 pages (at 14 pages/min), scraped 183 items (at 12 items/min)
2018-11-12 01:41:13 [scrapy.extensions.logstats] INFO: Crawled 227 pages (at 12 pages/min), scraped 198 items (at 15 items/min)
2018-11-12 01:42:12 [scrapy.extensions.logstats] INFO: Crawled 238 pages (at 11 pages/min), scraped 208 items (at 10 items/min)
2018-11-12 01:42:38 [scrapy.core.scraper] ERROR: Spider error processing <GET http://nebasechanger.neb.com> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 01:43:05 [scrapy.core.scraper] ERROR: Spider error processing <GET http://pcrfidelityestimator.neb.com/?_escaped_fragment_=%2F> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 01:43:05 [scrapy.extensions.logstats] INFO: Crawled 252 pages (at 14 pages/min), scraped 216 items (at 8 items/min)
2018-11-12 01:44:10 [root] ERROR: Unable to find match for url: https://goldengate.neb.com/editor
2018-11-12 01:44:10 [scrapy.extensions.logstats] INFO: Crawled 255 pages (at 3 pages/min), scraped 224 items (at 8 items/min)
2018-11-12 01:44:14 [scrapy.core.scraper] ERROR: Spider error processing <GET http://tmcalculator.neb.com/> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 01:45:03 [scrapy.core.scraper] ERROR: Spider error processing <GET http://rebase.neb.com/rebase/> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 01:45:03 [scrapy.extensions.logstats] INFO: Crawled 263 pages (at 8 pages/min), scraped 229 items (at 5 items/min)
2018-11-12 01:45:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://polbase.neb.com/> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
       valid
Crawling page: https://www.chromalox.com/en/international-sites       valid
Crawling page: https://www.vizio.com/content-partners       valid
Crawling page: https://www.vizio.com/accessibility       valid
Crawling page: https://www.vizio.com/order-lookup/       valid
Crawling page: https://www.chromalox.com/de-DE       valid
Crawling page: https://www.chromalox.com/fr-FR       valid
Crawling page: https://www.chromalox.com/es-MX       valid
Crawling page: https://www.neb.com/tools-and-resources/search?type=ApplicationNotes       valid
Crawling page: https://www.neb.com/tools-and-resources/search?type=UsageGuidelines       valid
Crawling page: https://www.neb.com/tools-and-resources/search?type=Chart       valid
Crawling page: https://www.chromalox.com/zh-CN       valid
Crawling page: https://www.vizio.com/environment       valid
Crawling page: https://www.vizio.com/product-registration/login/       valid
Crawling page: https://www.chromalox.com/en/sitemap       valid
Crawling page: https://www.chromalox.com/pt-BR       valid
Crawling page: https://www.chromalox.com/fr-CA       valid
Crawling page: https://www.neb.com/tools-and-resources/search?type=FAQ       valid
Crawling page: https://www.neb.com/tools-and-resources/search?type=TroubleshootingGuide       valid
Crawling page: https://www.chromalox.com/en/global/events/2018/december/powergen       valid
Crawling page: https://careers.vizio.com?utm_source=careersite       valid
Crawling page: https://www.praxair.com/our-company/suppliers/supply-chain-transparency       valid
Crawling page: https://www.praxair.com/our-company/our-people/our-executives       valid
Crawling page: https://www.praxair.com/our-company/our-people       valid
Crawling page: https://www.praxair.com/our-company/suppliers/supplier-diversity       valid
Crawling page: https://www.neb.com/-/media/nebus/campaign/golden-gate/breakingthroughlimitsgoldengate_tn.pdf?la=en       Crawling page: https://www.praxair.com/our-company/suppliers/expectations       valid
Crawling page: https://www.praxair.com/our-company/suppliers       valid
Crawling page: https://www.praxair.com/our-company/suppliers/what-we-buy       valid
Crawling page: https://www.praxair.com/our-company/suppliers/pre-qualification       valid
Crawling page: https://www.praxair.com/our-company/corporate-responsibility/policies-and-position-statements       valid
Crawling page: https://www.praxair.com/our-company/corporate-responsibility/government-affairs       valid
Crawling page: https://www.praxair.com/our-company/corporate-responsibility       valid
Crawling page: https://www.praxair.com/our-company/vision-and-values       valid
Crawling page: https://www.praxair.com/our-company       valid
Crawling page: https://www.praxair.com/careers/jobs-for-veterans       valid
Crawling page: https://www.praxair.com/careers/students-and-graduates-jobs/tips-for-success       valid
Crawling page: https://www.praxair.com/careers/students-and-graduates-jobs/our-hiring-process       valid
Crawling page: https://www.praxair.com/careers/full-time-opportunities       valid
Crawling page: https://www.praxair.com/careers/students-and-graduates-jobs/college-recruiting       valid
Crawling page: https://www.praxair.com/careers/students-and-graduates-jobs/leadership-development-programs       valid
Crawling page: https://www.praxair.com/careers/students-and-graduates-jobs/internships       valid
Crawling page: https://www.praxair.com/careers/why-work-at-praxair/training-and-development       valid
Crawling page: https://www.praxair.com/careers/students-and-graduates-jobs       valid
Crawling page: https://www.praxair.com/careers/why-work-at-praxair/benefits       valid
Crawling page: https://www.praxair.com/careers/why-work-at-praxair/culture       valid
Crawling page: https://www.praxair.com/careers       valid
Crawling page: https://www.praxair.com/careers/why-work-at-praxair       valid
Crawling page: https://www.praxair.com/careers/why-work-at-praxair/career-areas       valid
Crawling page: https://www.praxair.com/news       valid
Crawling page: http://www.shinetsu.co.jp/en/policy.html       valid
Crawling page: http://www.shinetsu.co.jp/en/privacy.html       valid
Crawling page: https://www.praxair.com/calendar       valid
Crawling page: https://www.praxair.com/investor-relations/reports-filings-and-presentations       valid
Crawling page: https://www.praxair.com/investor-relations/reports-filings-and-presentations/investor-presentations       valid
Crawling page: https://www.praxair.com/investor-relations/reports-filings-and-presentations/disclosure-practices       valid
Crawling page: http://www.shinetsu.co.jp/en/company/supplier_hotline.html       valid
Crawling page: http://www.shinetsu.co.jp/en/calendar/       valid
Crawling page: https://www.praxair.com/investor-relations/praxair-earnings       valid
Crawling page: https://www.praxair.com/investor-relations/quarterly-earnings       valid
Crawling page: https://www.praxair.com/investor-relations/praxair-earnings/earnings-guidance       valid
Crawling page: https://www.praxair.com/investor-relations/stock-information-and-price/quarterly-stock-price       valid
Crawling page: http://www.shinetsu.co.jp/en/materials/       valid
Crawling page: https://www.praxair.com/investor-relations/stock-information-and-price/dividends-stock-split       valid
Crawling page: https://www.praxair.com/investor-relations/stock-information-and-price/shareholder-services       valid
Crawling page: http://www.shinetsu.co.jp/en/news/schedule.html       valid
Crawling page: http://www.shinetsu.co.jp/en/company/movie.html       valid
Crawling page: http://www.shinetsu.co.jp/en/news/archive.php?id=487       valid
Crawling page: http://www.shinetsu.co.jp/en/ir/ir_briefing.html       valid
Crawling page: http://www.shinetsu.co.jp/en/products/semicon.html       valid
Crawling page: http://www.shinetsu.co.jp/en/news/archive.php?id=489       valid
Crawling page: http://www.shinetsu.co.jp/en/ir/ir_calendar.html       valid
Crawling page: http://www.shinetsu.co.jp/en/ir/ir_general_meeting.html       valid
Crawling page: http://www.shinetsu.co.jp/en/ir/stock_info.html       valid
Crawling page: http://www.shinetsu.co.jp/en/ir/ir_results.html       valid
Crawling page: http://www.shinetsu.co.jp/en/ir/ir_data.html       valid
Crawling page: http://www.shinetsu.co.jp/en/ir/policy.html       valid
Crawling page: http://www.shinetsu.co.jp/en/company/r_d.html       valid
Crawling page: http://www.shinetsu.co.jp/en/company/quality.html       valid
Crawling page: http://nebasechanger.neb.com       Crawling page: http://www.shinetsu.co.jp/en/inquiry.html       valid
Crawling page: http://www.shinetsu.co.jp/en/company/profile.html       valid
Crawling page: http://www.shinetsu.co.jp/en/news/       valid
Crawling page: http://www.shinetsu.co.jp/en/company/history.html       valid
Crawling page: http://pcrfidelityestimator.neb.com/?_escaped_fragment_=%2F       Crawling page: http://www.shinetsu.co.jp/en/csr/       valid
Crawling page: http://www.shinetsu.co.jp/en/products/       valid
Crawling page: http://www.shinetsu.co.jp/en/ir/       valid
Crawling page: http://www.shinetsu.co.jp/en/company/network.html       valid
Crawling page: http://www.shinetsu.co.jp/en/company/       valid
Crawling page: http://www.shinetsu.co.jp/en/sitemap.html       valid
Crawling page: http://www.shinetsu.co.jp/cn/       valid
Crawling page: https://goldengate.neb.com/editor       valid
Crawling page: http://tmcalculator.neb.com/       Crawling page: https://www.neb.com/freezer-programs/find-your-freezer       valid
Crawling page: https://www.neb.com/tools-and-resources/interactive-tools/dna-sequences-and-maps-tool       valid
Crawling page: http://www.shinetsu.co.jp/       valid
Crawling page: https://www.neb.com/tools-and-resources       valid
Crawling page: https://www.neb.com/nebnext-ultra-ii-fns-dna/nebnext-ultra-ii-for-dna-library-prep       valid
Crawling page: http://rebase.neb.com/rebase/       Crawling page: https://polbase.neb.com/       Crawling page: https://www.neb.com/monarch/monarch-nucleic-acid-purification-kits       valid
Crawling page: https://www.neb.com/products/special-offers2018-11-12 01:45:40 [scrapy.core.scraper] ERROR: Spider error processing <GET http://nc2.neb.com/NEBcutter2/> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 01:46:00 [scrapy.extensions.logstats] INFO: Crawled 269 pages (at 6 pages/min), scraped 234 items (at 5 items/min)
2018-11-12 01:46:06 [root] ERROR: Unable to find match for url: https://enzymefinder.neb.com
2018-11-12 01:46:08 [root] ERROR: Unable to find match for url: https://nebcloner.neb.com/?_escaped_fragment_=%2Fredigest
2018-11-12 01:46:11 [scrapy.core.scraper] ERROR: Spider error processing <GET http://nebiocalculator.neb.com/> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 01:46:14 [scrapy.core.scraper] ERROR: Spider error processing <GET http://nebuilder.neb.com/> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 01:46:17 [scrapy.core.scraper] ERROR: Spider error processing <GET http://nebnextselector.neb.com> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 01:46:30 [scrapy.core.scraper] ERROR: Spider error processing <GET http://nebcloner.neb.com/> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 01:47:10 [scrapy.extensions.logstats] INFO: Crawled 277 pages (at 8 pages/min), scraped 241 items (at 7 items/min)
2018-11-12 01:48:31 [scrapy.extensions.logstats] INFO: Crawled 285 pages (at 8 pages/min), scraped 249 items (at 8 items/min)
2018-11-12 01:49:06 [scrapy.extensions.logstats] INFO: Crawled 294 pages (at 9 pages/min), scraped 253 items (at 4 items/min)
2018-11-12 01:50:23 [scrapy.extensions.logstats] INFO: Crawled 309 pages (at 15 pages/min), scraped 262 items (at 9 items/min)
2018-11-12 01:51:25 [scrapy.extensions.logstats] INFO: Crawled 309 pages (at 0 pages/min), scraped 270 items (at 8 items/min)
2018-11-12 01:52:04 [scrapy.extensions.logstats] INFO: Crawled 319 pages (at 10 pages/min), scraped 278 items (at 8 items/min)
2018-11-12 01:53:03 [scrapy.extensions.logstats] INFO: Crawled 331 pages (at 12 pages/min), scraped 289 items (at 11 items/min)
2018-11-12 01:53:59 [scrapy.extensions.logstats] INFO: Crawled 331 pages (at 0 pages/min), scraped 295 items (at 6 items/min)
/bin/sh: 1: kill: No such process

2018-11-12 01:54:59 [scrapy.extensions.logstats] INFO: Crawled 335 pages (at 4 pages/min), scraped 299 items (at 4 items/min)
2018-11-12 01:55:59 [scrapy.extensions.logstats] INFO: Crawled 335 pages (at 0 pages/min), scraped 299 items (at 0 items/min)
2018-11-12 02:02:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.vizio.com/p65f1.html> (referer: https://www.vizio.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 58, in parse_links
    browser.get(response.url)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: Timeout loading page after 300000ms

2018-11-12 02:02:18 [scrapy.extensions.logstats] INFO: Crawled 346 pages (at 11 pages/min), scraped 306 items (at 7 items/min)
2018-11-12 02:03:03 [scrapy.extensions.logstats] INFO: Crawled 348 pages (at 2 pages/min), scraped 309 items (at 3 items/min)
2018-11-12 02:04:16 [scrapy.extensions.logstats] INFO: Crawled 355 pages (at 7 pages/min), scraped 315 items (at 6 items/min)
2018-11-12 02:10:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.vizio.com/pq65f1.html> (referer: https://www.vizio.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 58, in parse_links
    browser.get(response.url)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: Timeout loading page after 300000ms

2018-11-12 02:10:02 [scrapy.extensions.logstats] INFO: Crawled 357 pages (at 2 pages/min), scraped 318 items (at 3 items/min)
2018-11-12 02:11:27 [scrapy.extensions.logstats] INFO: Crawled 370 pages (at 13 pages/min), scraped 325 items (at 7 items/min)
2018-11-12 02:16:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.vizio.com/sound-bars> (referer: https://www.vizio.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 58, in parse_links
    browser.get(response.url)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: Timeout loading page after 300000ms

2018-11-12 02:21:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.vizio.com/dolby-atmos> (referer: https://www.vizio.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 58, in parse_links
    browser.get(response.url)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: Timeout loading page after 300000ms

       valid
Crawling page: https://www.neb.com/products/new-products       valid
Crawling page: http://nc2.neb.com/NEBcutter2/       Crawling page: https://www.neb.com/products/discontinued       valid
Crawling page: https://www.neb.com/products/strains       valid
Crawling page: https://enzymefinder.neb.com       valid
Crawling page: https://nebcloner.neb.com/?_escaped_fragment_=%2Fredigest       valid
Crawling page: http://nebiocalculator.neb.com/       Crawling page: http://nebuilder.neb.com/       Crawling page: http://nebnextselector.neb.com       Crawling page: https://www.neb.com/products/buffers/buffers       valid
Crawling page: http://nebcloner.neb.com/       Crawling page: https://www.neb.com/products/competent-cells/competent-cells       valid
Crawling page: https://www.neb.com/products/dna-plasmids-and-substrates       valid
Crawling page: https://www.neb.com/products/glycobiology/glycobiology       valid
Crawling page: https://www.neb.com/products/protein-tools/protein-tools       valid
Crawling page: https://www.neb.com/products/cellular-analysis/cellular-analysis       valid
Crawling page: https://www.neb.com/products/genome-editing/genome-editing       valid
Crawling page: https://www.neb.com/products/epigenetics/epigenetics       valid
Crawling page: https://www.neb.com/products/protein-expression-and-purification-technologies/protein-expression-and-purification-technologies       valid
Crawling page: https://www.neb.com/products/dna-assembly-cloning-and-mutagenesis-kits/dna-assembly-cloning-and-mutagenesis-kits       valid
Crawling page: https://www.neb.com/products/nucleic-acid-purification/nucleic-acid-purification       valid
Crawling page: https://www.neb.com/products/markers-and-ladders/markers-and-ladders       valid
Crawling page: https://www.neb.com/products/rna-reagents/rna-reagents       valid
Crawling page: https://www.neb.com/products/pcr-qpcr-and-amplification-technologies/pcr-qpcr-and-amplification-technologies       valid
Crawling page: https://www.neb.com/products/restriction-endonucleases/restriction-endonucleases       valid
Crawling page: https://www.neb.com/products/dna-modifying-enzymes-and-cloning-technologies/dna-modifying-enzymes-and-cloning-technologies       valid
Crawling page: https://www.neb.com/products/sample-preparation-for-next-generation-sequencing       valid
Crawling page: http://www.unifrax.com/sitemap/       valid
Crawling page: https://www.neb.com/products       valid
Crawling page: https://www.neb.com/applications/glycobiology-and-proteomics       valid
Crawling page: https://www.neb.com/applications/cellular-analysis       valid
Crawling page: https://www.neb.com/applications/protein-analysis-and-tools       valid
Crawling page: https://www.neb.com/applications/epigenetics       valid
Crawling page: https://www.neb.com/applications/library-preparation-for-next-generation-sequencing       valid
Crawling page: https://www.neb.com/applications/rna-analysis       valid
Crawling page: https://www.neb.com/applications/protein-expression-and-purification       valid
Crawling page: http://www.unifrax.com/uk-modern-slavery-statement/       valid
Crawling page: http://www.unifrax.com/business-conduct-and-code-of-business-ethics/       valid
Crawling page: http://www.unifrax.com/california-transparency-in-supply-chains-act-disclosure/       valid
Crawling page: http://www.unifrax.com/terms-of-use/       valid
Crawling page: https://www.neb.com/applications       valid
Crawling page: https://www.neb.com/applications/cloning-and-synthetic-biology       valid
Crawling page: https://www.neb.com/applications/genome-editing       valid
Crawling page: https://www.neb.com/applications/dna-amplification-pcr-and-qpcr       valid
Crawling page: http://www.unifrax.com/news-events/       valid
Crawling page: http://www.unifrax.com/about-us/career-opportunities/       valid
Crawling page: http://www.unifrax.com/about-us/locations/       valid
Crawling page: http://www.unifrax.com/privacy-and-cookie-statement/       valid
Crawling page: https://www.neb.com/account/sign-in       valid
Crawling page: http://www.unifrax.com/about-us/luyang-partnership/       valid
Crawling page: http://www.unifrax.com/about-us/product-stewardship-program/       valid
Crawling page: http://www.unifrax.com/about-us/       valid
Crawling page: http://www.unifrax.com/product-resources/training-education/       valid
Crawling page: http://www.unifrax.com/product-resources/international-safety-data-sheets/       valid
Crawling page: http://www.unifrax.com/product-resources/material-estimator/       valid
Crawling page: http://www.unifrax.com/product-resources/videos/       valid
Crawling page: http://www.unifrax.com/product-resources/technical-bulletins/       valid
Crawling page: http://www.unifrax.com/product-resources/specifications/       valid
Crawling page: http://www.unifrax.com/product-resources/installation-manuals/       valid
Crawling page: http://www.unifrax.com/product-resources/health-and-safety-publications/       valid
Crawling page: https://www.teamexos.com/idea-list/corporate-wellness-myths/       valid
Crawling page: http://www.unifrax.com/product-resources/submittal-sheets/       valid
Crawling page: http://www.unifrax.com/product-resources/drawings/       valid
Crawling page: https://www.teamexos.com/idea-list/should-young-athletes-take-supplements/       valid
Crawling page: http://www.unifrax.com/product-resources/safety-data-sheets/       valid
Crawling page: http://www.unifrax.com/product-resources/certifications/       valid
Crawling page: http://www.unifrax.com/product-resources/application-stories/       valid
Crawling page: http://www.unifrax.com/product-resources/brochures/       valid
Crawling page: http://www.unifrax.com/product-resources/product-information-sheets/       valid
Crawling page: https://www.teamexos.com/fortune-magazine-explores-how-exos-is-changing-the-corporate-wellness-model/       valid
Crawling page: https://www.teamexos.com/industries/       valid
Crawling page: https://www.teamexos.com/       valid
Crawling page: https://www.teamexos.com/locations/       valid
Crawling page: https://www.teamexos.com/contact-us/       valid
Crawling page: https://www.vizio.com/why-vizio       valid
Crawling page: https://www.vizio.com/privacy       valid
Crawling page: https://www.teamexos.com/about-2/       valid
Crawling page: https://www.vizio.com/about       valid
Crawling page: https://www.teamexos.com/casestudy/       valid
Crawling page: https://www.vizio.com/p65f1.html       Crawling page: https://www.teamexos.com/industries/education/       valid
Crawling page: https://www.vizio.com/m55f0.html       valid
Crawling page: https://www.vizio.com/d70f3.html       valid
Crawling page: https://www.vizio.com/m65f0.html       valid
Crawling page: https://www.vizio.com/smart-tv-apps       valid
Crawling page: https://www.vizio.com/sp30e0.html       valid
Crawling page: https://www.vizio.com/sb362anf6.html       valid
Crawling page: https://www.vizio.com/audio/home-theater.html?filter_soundbar_configurations=834       valid
Crawling page: https://www.vizio.com/dseries.html       valid
Crawling page: https://www.vizio.com/audio/atmos.html       valid
Crawling page: https://www.vizio.com/eseries.html       valid
Crawling page: https://www.vizio.com/mseries.html       valid
Crawling page: https://www.vizio.com/pseries.html       valid
Crawling page: https://www.vizio.com/pq65f1.html       Crawling page: https://www.vizio.com/locator       valid
Crawling page: https://www.vizio.com/myvizio/account/login       valid
Crawling page: https://www.vizio.com/multi-room-audio-overview       valid
Crawling page: https://www.vizio.com/audio/home-theater.html       valid
Crawling page: https://www.vizio.com/smartcast-portable-speaker-crave-go       valid
Crawling page: https://www.vizio.com/smartcast-portable-speaker-crave-360       valid
Crawling page: https://www.vizio.com/smartcast-stationary-speaker-crave-pro       valid
Crawling page: https://www.vizio.com/sound-bars       Crawling page: https://www.vizio.com/dolby-atmos       Crawling page: https://www.vizio.com/sound-bar-comparison       valid
Crawling page:2018-11-12 02:21:52 [scrapy.extensions.logstats] INFO: Crawled 371 pages (at 1 pages/min), scraped 327 items (at 2 items/min)
2018-11-12 02:26:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.vizio.com/smartcast> (referer: https://www.vizio.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 58, in parse_links
    browser.get(response.url)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: Timeout loading page after 300000ms

2018-11-12 02:27:34 [scrapy.extensions.logstats] INFO: Crawled 371 pages (at 0 pages/min), scraped 330 items (at 3 items/min)
2018-11-12 02:28:18 [scrapy.extensions.logstats] INFO: Crawled 384 pages (at 13 pages/min), scraped 336 items (at 6 items/min)
2018-11-12 02:29:06 [scrapy.extensions.logstats] INFO: Crawled 393 pages (at 9 pages/min), scraped 348 items (at 12 items/min)
2018-11-12 02:30:13 [scrapy.extensions.logstats] INFO: Crawled 408 pages (at 15 pages/min), scraped 363 items (at 15 items/min)
2018-11-12 02:31:02 [scrapy.extensions.logstats] INFO: Crawled 414 pages (at 6 pages/min), scraped 372 items (at 9 items/min)
2018-11-12 02:32:15 [scrapy.extensions.logstats] INFO: Crawled 431 pages (at 17 pages/min), scraped 390 items (at 18 items/min)
/bin/sh: 1: kill: No such process

2018-11-12 02:33:06 [scrapy.extensions.logstats] INFO: Crawled 451 pages (at 20 pages/min), scraped 403 items (at 13 items/min)
/bin/sh: 1: kill: No such process

 https://www.vizio.com/sound-bar-audio-overview       valid
Crawling page: https://www.vizio.com/smartcast       Crawling page: https://www.vizio.com/myvizio/account/login/referer/aHR0cHM6Ly93d3cudml6aW8uY29tL215dml6aW8vYWNjb3VudC9pbmRleC8,/       valid
Crawling page: https://www.vizio.com/tv-comparison       valid
Crawling page: https://www.vizio.com/tvs.html       valid
Crawling page: https://www.chromalox.com/en/catalog/component-technologies/drimeg-cartridge-heaters       valid
Crawling page: https://www.chromalox.com/en/catalog/industrial-heaters-and-systems/c2i-immersion-heaters       valid
Crawling page: https://www.chromalox.com/en/catalog/component-technologies/c2i-temperature-controllers       valid
Crawling page: https://www.vizio.com/checkout/cart       valid
Crawling page: https://www.chromalox.com/en/catalog/industrial-heaters-and-systems/powerv-circulation-heaters       valid
Crawling page: https://www.chromalox.com/en/catalog/component-technologies/drimeg-tubular-heaters       valid
Crawling page: https://www.chromalox.com/en/global/case-studies/chromalox-products-protect-the-environment       valid
Crawling page: https://www.chromalox.com/en/global/case-studies/chromalox-case-study-chlor-alkali       valid
Crawling page: https://www.chromalox.com/en/catalog/industrial-heaters-and-systems/power-v-power-control-systems       valid
Crawling page: https://www.chromalox.com/en/global/case-studies/chromalox-saves-with-digital-thermostats       valid
Crawling page: https://www.chromalox.com/en/about-us/policies       valid
Crawling page: https://www.chromalox.com/en/about-us/our-history       valid
Crawling page: https://www.chromalox.com/en/about-us/news-and-events       valid
Crawling page: https://www.chromalox.com/en/100       valid
Crawling page: https://www.chromalox.com/en/about-us/careers       valid
Crawling page: https://www.chromalox.com/en/about-us/our-people       valid
Crawling page: https://www.chromalox.com/en/about-us/our-capabilities/manufacturing-quality       valid
Crawling page: https://www.chromalox.com/en/about-us/our-capabilities/facilities       valid
Crawling page: https://www.chromalox.com/en/about-us/our-capabilities       valid
Crawling page: https://www.chromalox.com/en/about-us/our-approach/our-vision       valid
Crawling page: https://www.chromalox.com/en/about-us/our-approach/our-mission       valid
Crawling page: https://www.chromalox.com/en/about-us/our-approach/our-passion       valid
Crawling page: https://www.chromalox.com/en/about-us/our-approach/our-values       valid
Crawling page: https://www.chromalox.com/en/about-us/our-approach/our-brand       valid
Crawling page: https://www.chromalox.com/en/about-us/our-approach       valid
Crawling page: https://www.chromalox.com/en/resources-and-support/support/order-literature       valid
Crawling page: https://www.chromalox.com/en/resources-and-support/support/customer-service       valid
Crawling page: https://www.chromalox.com/en/resources-and-support/support/about-service-contracts       valid
Crawling page: https://www.chromalox.com/en/resources-and-support/support       valid
Crawling page: https://www.chromalox.com/en/resources-and-support/technical-resources/engineering-technical-data       valid
Crawling page: https://www.chromalox.com/en/resources-and-support/tools       valid
Crawling page: https://www.chromalox.com/en/resources-and-support/software       valid
Crawling page: https://www.chromalox.com/en/resources-and-support/design-tools/designguidelines       valid
Crawling page: https://www.chromalox.com/en/resources-and-support/design-tools       valid
Crawling page: https://www.chromalox.com/en/resources-and-support/technical-resources/white-papers       valid
Crawling page: https://www.chromalox.com/en/resources-and-support/technical-resources/training-manuals       valid
Crawling page: https://www.chromalox.com/en/resources-and-support/technical-resources/thermal-system-technical-glossary       valid
Crawling page: https://www.chromalox.com/en/resources-and-support/technical-resources/specification-data-sheets       valid
Crawling page: https://www.chromalox.com/en/resources-and-support/technical-resources/product-data-sheets       valid
Crawling page: https://www.chromalox.com/en/resources-and-support/technical-resources/video-library       valid
Crawling page: https://www.chromalox.com/en/resources-and-support/technical-resources/industry-standards       valid
Crawling page: https://www.chromalox.com/en/resources-and-support/technical-resources/installation-manuals       valid
Crawling page: https://www.chromalox.com/en/solutions/all-case-studies/case-studies       valid
Crawling page: https://www.praxair.com/industries/food-and-beverage       valid
Crawling page: https://www.praxair.com/industries/glass       valid
Crawling page: https://www.praxair.com/industries/healthcare-and-medical       valid
Crawling page: https://www.praxair.com/industries/electronics       valid
Crawling page: https://www.praxair.com/industries/energy       valid
Crawling page: https://www.praxair.com/industries/automotive-and-transportation       valid
Crawling page: https://www.praxair.com/industries/aerospace-and-aircraft       valid
Crawling page: https://www.praxair.com/industries/chemicals       valid
Crawling page: https://www.praxair.com/industries/diving       valid
Crawling page: https://www.praxair.com/industries/additive-manufacturing-3d-printing       valid
Crawling page: https://www.praxair.com/industries       valid
Crawling page: https://www.praxair.com/industries/oil-and-gas?tab=services       valid
Crawling page: https://www.praxair.com/services/gas-tanks-and-piping-industrial-services       valid
Crawling page: https://www.praxair.com/services/industrial-gas-supply-and-management       valid
Crawling page: https://www.praxair.com/services       valid
Crawling page: https://www.praxair.com/gases/specialty-gases       valid
Crawling page: https://www.praxair.com/gases/gas-mixtures       valid
Crawling page: https://www.praxair.com/gases/buy-liquid-oxygen-or-compressed-oxygen-gas       valid
Crawling page: https://www.praxair.com/gases/gas-handling-equipment       valid
Crawling page: https://www.praxair.com/gases/buy-liquid-nitrogen-or-compressed-nitrogen-gas       valid
Crawling page: https://www.praxair.com/gases/buy-xenon-gas       valid
Crawling page: https://www.praxair.com/gases/buy-compressed-hydrogen-gas-or-liquid-hydrogen       valid
Crawling page: https://www.praxair.com/gases/buy-neon-gas-or-liquid-neon       valid
Crawling page: https://www.praxair.com/gases/buy-krypton-gas       valid
Crawling page: https://www.praxair.com/gases/buy-helium-gas-or-liquid-helium       valid
Crawling page: http://www.shinetsu.co.jp/en/       valid
Crawling page: https://www.praxair.com/gases/buy-compressed-argon-gas-or-liquid-argon       valid
Crawling page: https://www.praxair.com/gases/buy-acetylene-gas-or-chemical-acetylene       valid
Crawling page: https://www.praxair.com/gases/buy-dry-ice-solid-carbon-dioxide       valid
Crawling page: https://www.praxair.com/gases/buy-liquid-or-compressed-carbon-dioxide-gas       valid
Crawling page: https://www.praxair.com/gases       valid
Crawling page: http://www.unifrax.com/product-category/vacuum-formed-components/?productcategory=243       valid
Crawling page: https://www.praxair.com/international-locations       valid
Crawling page: http://www.unifrax.com/product-category/textiles/?productcategory=233       valid
Crawling page: https://www.praxair.com/contact-us       valid
Crawling page: http://www.unifrax.com/product-resources/       valid
Crawling page: https://www.praxair.com/       valid
Crawling page: https://www.praxair.com/store-locator       valid
Crawling page: http://www.unifrax.com/product-category/papers-and-felts/?productcategory=235       valid
Crawling page: https://www.praxair.com/buy-online       valid
Crawling page: https://www.praxair.com/manage-your-account       valid
Crawling page: http://www.unifrax.com/product-category/microporous-insulation/?productcategory=230       valid
Crawling page: http://www.unifrax.com/product-category/modules/?productcategory=207       valid2018-11-12 02:34:02 [scrapy.extensions.logstats] INFO: Crawled 460 pages (at 9 pages/min), scraped 417 items (at 14 items/min)
/bin/sh: 1: kill: No such process

2018-11-12 02:35:04 [scrapy.extensions.logstats] INFO: Crawled 475 pages (at 15 pages/min), scraped 430 items (at 13 items/min)
2018-11-12 02:36:13 [scrapy.extensions.logstats] INFO: Crawled 488 pages (at 13 pages/min), scraped 447 items (at 17 items/min)
2018-11-12 02:37:20 [scrapy.extensions.logstats] INFO: Crawled 506 pages (at 18 pages/min), scraped 462 items (at 15 items/min)
/bin/sh: 1: kill: No such process

2018-11-12 02:37:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://blog.piksel.com> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:37:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://blog.piksel.com/press-release> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:38:06 [scrapy.extensions.logstats] INFO: Crawled 515 pages (at 9 pages/min), scraped 466 items (at 4 items/min)
2018-11-12 02:38:58 [scrapy.core.scraper] ERROR: Spider error processing <GET http://www.hrl.com/news/2018/07/19/stellar-system-will-enable-autonomous-systems-to-learn-for-life> (referer: http://www.hrl.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 58, in parse_links
    browser.get(response.url)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: Reached error page: about:neterror?e=connectionFailure&u=http%3A//www.hrl.com/news/2018/07/19/stellar-system-will-enable-autonomous-systems-to-learn-for-life&c=UTF-8&f=regular&d=Firefox%20can%E2%80%99t%20establish%20a%20connection%20to%20the%20server%20at%20www.hrl.com.

2018-11-12 02:39:05 [scrapy.extensions.logstats] INFO: Crawled 537 pages (at 22 pages/min), scraped 477 items (at 11 items/min)
2018-11-12 02:39:29 [scrapy.core.scraper] ERROR: Spider error processing <GET http://www.hrl.com/news/2018/10/08/new-radio-frequency-switch-enables-faster-video-and-audio-5g-streaming> (referer: http://www.hrl.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 58, in parse_links
    browser.get(response.url)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: Reached error page: about:neterror?e=connectionFailure&u=http%3A//www.hrl.com/news/2018/10/08/new-radio-frequency-switch-enables-faster-video-and-audio-5g-streaming&c=UTF-8&f=regular&d=Firefox%20can%E2%80%99t%20establish%20a%20connection%20to%20the%20server%20at%20www.hrl.com.

2018-11-12 02:40:25 [scrapy.extensions.logstats] INFO: Crawled 537 pages (at 0 pages/min), scraped 492 items (at 15 items/min)
2018-11-12 02:41:02 [scrapy.extensions.logstats] INFO: Crawled 550 pages (at 13 pages/min), scraped 501 items (at 9 items/min)
2018-11-12 02:42:14 [scrapy.extensions.logstats] INFO: Crawled 565 pages (at 15 pages/min), scraped 518 items (at 17 items/min)
/bin/sh: 1: kill: No such process

2018-11-12 02:42:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/careers> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:42:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/careers/our-u-s-businesses> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'

Crawling page: http://www.unifrax.com/product-category/microfine-glass-fibers/?productcategory=217       valid
Crawling page: https://www.neb.com/       valid
Crawling page: http://www.unifrax.com/product-category/hot-gas-filtration/?productcategory=248       valid
Crawling page: http://www.unifrax.com/product-category/fire-protection/?productcategory=241       valid
Crawling page: http://www.unifrax.com/product-category/furnace-lining-products/?productcategory=240       valid
Crawling page: http://www.unifrax.com/product-category/furnace-hardware/?productcategory=239       valid
Crawling page: http://www.unifrax.com/product-category/fibers/?productcategory=231       valid
Crawling page: http://www.unifrax.com/product-category/emission-control-mounting-mats/?productcategory=215       valid
Crawling page: http://www.unifrax.com/product-category/fabricated-products/?productcategory=238       valid
Crawling page: http://www.unifrax.com/product-category/custom-mat-products/?productcategory=247       valid
Crawling page: http://www.unifrax.com/product-category/boards/?productcategory=211       valid
Crawling page: http://www.unifrax.com/products/?ind=0       valid
Crawling page: http://www.unifrax.com/product-category/coatings-mixes/?productcategory=237       valid
Crawling page: http://www.unifrax.com/product-category/blankets/?productcategory=236       valid
Crawling page: http://www.unifrax.com/products/       valid
Crawling page: http://www.unifrax.com/industry/transportation/       valid
Crawling page: http://www.unifrax.com/industry/power-generation/       valid
Crawling page: http://www.unifrax.com/industry/petrochemical/       valid
Crawling page: http://www.unifrax.com/industry/iron-and-steel/       valid
Crawling page: http://www.unifrax.com/industry/hvac/       valid
Crawling page: http://www.unifrax.com/industry/fire-protection-industrial/       valid
Crawling page: http://www.unifrax.com/industry/hearth/       valid
Crawling page: http://www.unifrax.com/industry/foundry/       valid
Crawling page: http://www.unifrax.com/industry/battery-and-energy-storage/       valid
Crawling page: http://www.unifrax.com/industry/filtration-and-separation/       valid
Crawling page: http://www.unifrax.com/industry/ceramic-and-glass/       valid
Crawling page: http://www.unifrax.com/industry/fire-protection-commercial/       valid
Crawling page: http://www.unifrax.com/industry/automotive/       valid
Crawling page: http://www.unifrax.com/industry/appliance/       valid
Crawling page: http://www.unifrax.com/industry/aluminum-non-ferrous-metals/       valid
Crawling page: http://www.unifrax.com/industry/aerospace/       valid
Crawling page: https://blog.piksel.com/press-release/piksel-group-selected-for-aws-well-architected-partner-programme       valid
Crawling page: https://blog.piksel.com/press-release/simon-fell-joins-piksel-board-of-directors       valid
Crawling page: https://piksel.com/SAAS-PRIVACY-POLICY/       valid
Crawling page: https://piksel.com/partners/       valid
Crawling page: http://www.unifrax.com/es-mx/       valid
Crawling page: http://www.unifrax.com/industry/additives-and-reinforcements/       valid
Crawling page: http://www.unifrax.com/pt-br/       valid
Crawling page: http://www.unifrax.com/industries/       valid
Crawling page: https://blog.piksel.com/press-release/piksel-announces-vcms2       valid
Crawling page: https://piksel.com/casestudy/channel4/       valid
Crawling page: https://info.piksel.com/microsoft-qmth-program       valid
Crawling page: http://www.unifrax.com/it/       valid
Crawling page: http://www.unifrax.com/es/       valid
Crawling page: http://www.unifrax.com/de/       valid
Crawling page: http://www.unifrax.com/fr/       valid
Crawling page: http://www.unifrax.com/en-uk/       valid
Crawling page: http://www.unifrax.com/       valid
Crawling page: https://blog.piksel.com       Crawling page: https://blog.piksel.com/press-release       Crawling page: http://www.unifrax.com/ko/       valid
Crawling page: http://www.unifrax.com/zh-hans/       valid
Crawling page: https://piksel.com/contact/       valid
Crawling page: https://piksel.com/about/       valid
Crawling page: https://piksel.com/white-papers/       valid
Crawling page: https://piksel.com/video-content/       valid
Crawling page: https://piksel.com/resource-centre/       valid
Crawling page: https://piksel.com/customers/       valid
Crawling page: http://www.hrl.com/sitemap       valid
Crawling page: http://www.hrl.com/terms       valid
Crawling page: http://www.hrl.com/privacy       valid
Crawling page: http://www.hrl.com/horizons/003       valid
Crawling page: http://www.hrl.com/news/2018/07/19/stellar-system-will-enable-autonomous-systems-to-learn-for-life       Crawling page: http://www.hrl.com/news/2018/07/16/hrl-employees-experience-touches-down-on-mars       valid
Crawling page: https://piksel.com/services/       valid
Crawling page: https://piksel.com/product/piksel-palette/       valid
Crawling page: https://piksel.com/product/digital-enterprise/       valid
Crawling page: https://piksel.com/product/ds/       valid
Crawling page: http://www.hrl.com/news/2018/10/08/new-radio-frequency-switch-enables-faster-video-and-audio-5g-streaming       Crawling page: http://www.hrl.com/news/2018/08/23/hrls-history-of-the-future-podcast-episode-003-dan-sievenpiper       valid
Crawling page: http://www.hrl.com/news/2018/08/01/skills-and-learning-improved-by-closed-loop-electrical-brain-stimulation-during-sleep       valid
Crawling page: http://www.hrl.com/news/2018/07/24/all-memristor-architecture-could-enable-brain-like-computers       valid
Crawling page: https://piksel.com/product/digital-showcase/       valid
Crawling page: https://piksel.com/product/fuse-publisher/       valid
Crawling page: https://piksel.com/product/fuse-metadata/       valid
Crawling page: https://piksel.com/video-products/       valid
Crawling page: http://www.hrl.com/news/2018/11/07/novel-memristor-neuron-circuits-offer-building-blocks-for-mimicking-the-brain       valid
Crawling page: http://www.hrl.com/products-services/machine-shop       valid
Crawling page: http://www.hrl.com/products-services/foundry       valid
Crawling page: http://www.hrl.com/products-services       valid
Crawling page: http://www.hrl.com/laboratories/sel       valid
Crawling page: http://www.hrl.com/laboratories/issl       valid
Crawling page: http://www.hrl.com/laboratories/mml       valid
Crawling page: http://www.hrl.com/laboratories/mtl       valid
Crawling page: http://www.hrl.com/laboratories       valid
Crawling page: http://www.hrl.com/careers/communities       valid
Crawling page: http://www.hrl.com/careers/diversity       valid
Crawling page: http://www.hrl.com/careers/benefits       valid
Crawling page: http://www.hrl.com/careers/internships       valid
Crawling page: http://www.hrl.com/careers/current-openings       valid
Crawling page: http://www.hrl.com/careers       valid
Crawling page: http://www.hrl.com/video       valid
Crawling page: http://www.hrl.com/news/archive       valid
Crawling page: http://www.hrl.com/about/contact       valid
Crawling page: http://www.hrl.com/about/directions       valid
Crawling page: http://www.hrl.com/about/laser       valid
Crawling page: http://www.hrl.com/about/mission       valid
Crawling page: http://www.hrl.com/about/staff       valid
Crawling page: http://www.hrl.com/about/quality       valid
Crawling page: http://www.hrl.com/about/history       valid
Crawling page: http://www.hrl.com/about/board       valid
Crawling page: http://www.hrl.com/about/tours       valid
Crawling page: http://www.hrl.com/about       valid
Crawling page: https://www.bauschhealth.com/terms       valid
Crawling page: https://www.bauschhealth.com/privacy       valid
Crawling page: http://www.hrl.com/news       valid
Crawling page: https://www.bauschhealth.com/       valid
Crawling page: https://www.bauschhealth.com/r-d       valid
Crawling page: https://www.bauschhealth.com/careers       Crawling page: https://www.bauschhealth.com/careers/our-u-s-businesses       Crawling page: https://www.bauschhealth.com/news-room/our-perspective/artmid/714/articleid/12/bausch-health-launches-csr-report2018-11-12 02:42:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/news-room/our-perspective/artmid/714/articleid/12/bausch-health-launches-csr-report> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:42:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/news-room/our-perspective/artmid/714/articleid/11/welcome-to-bausch-health> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:42:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/careers/working-at-bausch-health> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:42:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/news-room/news-releases> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:42:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/careers/benefits> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:43:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/news-room/our-perspective> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:43:00 [scrapy.extensions.logstats] INFO: Crawled 581 pages (at 16 pages/min), scraped 520 items (at 2 items/min)
2018-11-12 02:43:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/responsibility/submitting-a-supplier-diversity-profile> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:43:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/responsibility/safety-data-sheets> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:43:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/news-room> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:43:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/responsibility/u-s-supplier-diversity> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:43:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/responsibility/patient-assistance-programs> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:43:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/responsibility/grants-and-investigator-initiated-studies> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:43:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/responsibility/u-s-assistance-programs> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:43:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/responsibility/u-s-health-care-compliance-policy> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:43:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/responsibility/payments> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:43:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/responsibility/u-s-grants> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:43:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/responsibility/u-s-health-care-compliance-program> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:43:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/responsibility> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:43:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/responsibility/the-bausch-foundation> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:43:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/responsibility/declaration-of-compliance> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:44:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/products/us-product-list> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:44:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/products/consumer-health> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:44:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/r-d/our-pipeline> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:44:09 [scrapy.extensions.logstats] INFO: Crawled 596 pages (at 15 pages/min), scraped 520 items (at 0 items/min)
2018-11-12 02:44:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/products/dentistry> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:44:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/products/generics-neurology-and-other> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:44:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/products/eye-health> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:44:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/products/gastrointestinal> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:44:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/products> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:44:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/products/dermatology-products-aesthetics-devices> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:44:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/about-us/who-we-are> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:44:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/about-us/our-mission-vision-and-values> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:44:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/about-us/contact-us> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:44:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/about-us/our-locations> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:45:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/about-us/executive-management-team> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:45:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bauschhealth.com/about-us> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 02:45:18 [scrapy.extensions.logstats] INFO: Crawled 614 pages (at 18 pages/min), scraped 524 items (at 4 items/min)
2018-11-12 02:46:02 [scrapy.extensions.logstats] INFO: Crawled 622 pages (at 8 pages/min), scraped 536 items (at 12 items/min)
2018-11-12 02:47:03 [scrapy.extensions.logstats] INFO: Crawled 642 pages (at 20 pages/min), scraped 553 items (at 17 items/min)
2018-11-12 02:48:39 [scrapy.extensions.logstats] INFO: Crawled 655 pages (at 13 pages/min), scraped 568 items (at 15 items/min)
2018-11-12 02:49:10 [scrapy.extensions.logstats] INFO: Crawled 659 pages (at 4 pages/min), scraped 573 items (at 5 items/min)
       Crawling page: https://www.bauschhealth.com/news-room/our-perspective/artmid/714/articleid/11/welcome-to-bausch-health       Crawling page: https://www.bauschhealth.com/careers/working-at-bausch-health       Crawling page: https://www.bauschhealth.com/news-room/news-releases       Crawling page: https://www.bauschhealth.com/careers/benefits       Crawling page: https://www.bauschhealth.com/news-room/our-perspective       Crawling page: https://www.bauschhealth.com/responsibility/submitting-a-supplier-diversity-profile       Crawling page: https://www.bauschhealth.com/responsibility/safety-data-sheets       Crawling page: https://www.bauschhealth.com/news-room       Crawling page: https://www.bauschhealth.com/responsibility/u-s-supplier-diversity       Crawling page: https://www.bauschhealth.com/responsibility/patient-assistance-programs       Crawling page: https://www.bauschhealth.com/responsibility/grants-and-investigator-initiated-studies       Crawling page: https://www.bauschhealth.com/responsibility/u-s-assistance-programs       Crawling page: https://www.bauschhealth.com/responsibility/u-s-health-care-compliance-policy       Crawling page: https://www.bauschhealth.com/responsibility/payments       Crawling page: https://www.bauschhealth.com/responsibility/u-s-grants       Crawling page: https://www.bauschhealth.com/responsibility/u-s-health-care-compliance-program       Crawling page: https://www.bauschhealth.com/responsibility       Crawling page: https://www.bauschhealth.com/responsibility/the-bausch-foundation       Crawling page: https://www.bauschhealth.com/responsibility/declaration-of-compliance       Crawling page: https://www.bauschhealth.com/products/us-product-list       Crawling page: https://www.bauschhealth.com/products/consumer-health       Crawling page: https://www.bauschhealth.com/r-d/our-pipeline       Crawling page: https://www.bauschhealth.com/products/dentistry       Crawling page: https://www.bauschhealth.com/products/generics-neurology-and-other       Crawling page: https://www.bauschhealth.com/products/eye-health       Crawling page: https://www.bauschhealth.com/products/gastrointestinal       Crawling page: https://www.bauschhealth.com/products       Crawling page: https://www.bauschhealth.com/products/dermatology-products-aesthetics-devices       Crawling page: https://www.bauschhealth.com/about-us/who-we-are       Crawling page: https://www.bauschhealth.com/about-us/our-mission-vision-and-values       Crawling page: https://www.bauschhealth.com/about-us/contact-us       Crawling page: https://www.bauschhealth.com/about-us/our-locations       Crawling page: https://www.chromalox.com/en/resources-and-support/technical-resources       valid
Crawling page: https://www.bauschhealth.com/about-us/executive-management-team       Crawling page: https://www.bauschhealth.com/about-us       Crawling page: https://www.chromalox.com/en/industries/building-and-construction       valid
Crawling page: https://www.chromalox.com/en/industries/commercial-food-equipment       valid
Crawling page: https://www.chromalox.com/en/industries/water-and-waste-water       valid
Crawling page: https://www.chromalox.com/en/industries/military-and-defense       valid
Crawling page: https://www.chromalox.com/en/industries/medical-equipment       valid
Crawling page: https://www.chromalox.com/en/industries/analytical-equipment       valid
Crawling page: https://www.chromalox.com/en/industries/transportation       valid
Crawling page: https://www.chromalox.com/en/industries/industrial-equipment       valid
Crawling page: https://www.chromalox.com/en/industries/bio-pharmaceutical       valid
Crawling page: https://www.chromalox.com/en/industries/industries-we-support       valid
Crawling page: https://www.chromalox.com/en/industries/power-generation       valid
Crawling page: https://www.chromalox.com/en/industries/petrochemical       valid
Crawling page: https://www.chromalox.com/en/industries/oil-and-gas       valid
Crawling page: https://www.chromalox.com/en/industries/mining-and-mineral-processing       valid
Crawling page: https://www.chromalox.com/en/industries/chemical-processing       valid
Crawling page: https://www.chromalox.com/en/technologies/redsage       valid
Crawling page: https://www.chromalox.com/en/industries/alternative-fuels       valid
Crawling page: https://www.chromalox.com/en/technologies/c2i       valid
Crawling page: https://www.chromalox.com/en/about-us/our-capabilities/technology-platforms       valid
Crawling page: https://www.chromalox.com/en/technologies/drimeg       valid
Crawling page: https://www.chromalox.com/en/technologies/powerv       valid
Crawling page: https://www.chromalox.com/en/solutions/component-heating-solutions/technical-resources       valid
Crawling page: https://www.chromalox.com/en/solutions/component-heating-solutions/site-services       valid
Crawling page: https://www.chromalox.com/en/solutions/component-heating-solutions/engineering-services       valid
Crawling page: https://www.chromalox.com/en/solutions/component-heating-solutions/case-studies       valid
Crawling page: https://www.chromalox.com/en/solutions/component-heating-solutions/applications       valid
Crawling page: https://www.chromalox.com/en/solutions/component-heating-solutions       valid
Crawling page: https://www.chromalox.com/en/solutions/process-heating-solutions/custom-designed-solutions       valid
Crawling page: https://www.chromalox.com/en/solutions/process-heating-solutions/technical-resources       valid
Crawling page: https://www.chromalox.com/en/solutions/process-heating-solutions/site-services       valid
Crawling page: https://www.chromalox.com/en/solutions/process-heating-solutions/engineering-services       valid
Crawling page: https://www.chromalox.com/en/solutions/process-heating-solutions/case-studies       valid
Crawling page: https://www.chromalox.com/en/solutions/process-heating-solutions/applications       valid
Crawling page: https://www.chromalox.com/en/solutions/process-heating-solutions       valid
Crawling page: https://www.chromalox.com/en/solutions/temperature-management-solutions/technical-resources       valid
Crawling page: https://www.chromalox.com/en/solutions/temperature-management-solutions/site-services       valid
Crawling page: https://www.chromalox.com/en/solutions/temperature-management-solutions/engineering-services       valid
Crawling page: https://www.chromalox.com/en/solutions/temperature-management-solutions/case-studies       valid
Crawling page: https://www.chromalox.com/en/solutions/temperature-management-solutions/applications       valid
Crawling page: https://www.chromalox.com/en/solutions/temperature-management-solutions       valid
Crawling page: http://www.unicharm.co.jp/english/news/20171027.html       valid
Crawling page: http://www.unicharm.co.jp/english/ir/library/earnings/index.html       valid
Crawling page: http://www.unicharm.co.jp/english/ir/library/investors/index.html       valid
Crawling page: https://www.chromalox.com/en/technologies/xtremeduty       valid
Crawling page: https://www.chromalox.com/en/technologies/directconnect       valid
Crawling page: https://www.chromalox.com/en/solutions/cold-weather-solutions       valid
Crawling page: http://www.unicharm.co.jp/products_en/index.html       valid
Crawling page: https://piksel.com       valid
Crawling page: https://www.chromalox.com/en/catalog/component-technologies/xtremeduty       valid
Crawling page: https://www.chromalox.com/en/catalog/component-technologies/control-and-sensor-accessories       valid
Crawling page: https://www.chromalox.com/en/catalog/component-technologies/electromechanical-controls-and-thermostats       valid
Crawling page: https://www.chromalox.com/en/catalog/component-technologies/temperature-sensors       valid
Crawling page: https://www.chromalox.com/en/catalog/component-technologies/enclosure-heaters       valid
Crawling page: https://www.chromalox.com/en/catalog/component-technologies/power-controllers       valid
Crawling page: https://www.chromalox.com/en/catalog/component-technologies/surface-tank-and-drum-heaters       valid
Crawling page: https://www.chromalox.com/en/catalog/component-technologies/temperature-controllers/bin/sh: 1: kill: No such process

2018-11-12 02:50:20 [scrapy.extensions.logstats] INFO: Crawled 674 pages (at 15 pages/min), scraped 588 items (at 15 items/min)
2018-11-12 02:51:04 [scrapy.extensions.logstats] INFO: Crawled 686 pages (at 12 pages/min), scraped 600 items (at 12 items/min)
2018-11-12 02:52:04 [scrapy.extensions.logstats] INFO: Crawled 700 pages (at 14 pages/min), scraped 616 items (at 16 items/min)
/bin/sh: 1: kill: No such process

2018-11-12 02:53:00 [scrapy.extensions.logstats] INFO: Crawled 719 pages (at 19 pages/min), scraped 632 items (at 16 items/min)
/bin/sh: 1: kill: No such process

2018-11-12 02:54:09 [scrapy.extensions.logstats] INFO: Crawled 742 pages (at 23 pages/min), scraped 654 items (at 22 items/min)
2018-11-12 02:54:29 [root] ERROR: Unable to find match for url: https://www.heartmate.com
       valid
Crawling page: https://www.chromalox.com/en/catalog/component-technologies/band-and-nozzle-heaters       valid
Crawling page: https://www.chromalox.com/en/catalog/component-technologies/high-temperature-multiple-zone-heaters       valid
Crawling page: https://www.chromalox.com/en/catalog/component-technologies/strip-and-ring-heaters       valid
Crawling page: https://www.chromalox.com/en/catalog/component-technologies/flexible-heaters       valid
Crawling page: https://www.chromalox.com/en/catalog/component-technologies/cartridge-heaters       valid
Crawling page: http://www.unicharm.co.jp/english/index.html       valid
Crawling page: https://www.chromalox.com/en/catalog/component-technologies/tubular-heaters       valid
Crawling page: https://www.chromalox.com/en/catalog/component-technologies       valid
Crawling page: https://www.chromalox.com/en/catalog/industrial-heaters-and-systems/xtremeduty       valid
Crawling page: https://www.chromalox.com/en/catalog/industrial-heaters-and-systems/directconnect       valid
Crawling page: http://www.unicharm.co.jp/global/index.html       valid
Crawling page: https://www.chromalox.com/en/catalog/industrial-heaters-and-systems/industrial-air-and-infrared-radiant-heaters       valid
Crawling page: https://www.chromalox.com/en/catalog/industrial-heaters-and-systems/hazardous-location-and-corrosive-environment-air-heaters       valid
Crawling page: https://www.chromalox.com/en/catalog/industrial-heaters-and-systems/storage-tank-heaters       valid
Crawling page: https://www.chromalox.com/en/catalog/industrial-heaters-and-systems/circulation-and-immersion-heaters       valid
Crawling page: https://www.chromalox.com/en/catalog/industrial-heaters-and-systems/loadbanks-and-air-handlers       valid
Crawling page: https://www.chromalox.com/en/catalog/industrial-heaters-and-systems/hot-oil-and-process-liquid-vaporizers       valid
Crawling page: https://www.chromalox.com/en/catalog/industrial-heaters-and-systems/heat-transfer-system       valid
Crawling page: https://www.chromalox.com/en/catalog/industrial-heaters-and-systems/steam-superheaters       valid
Crawling page: https://www.chromalox.com/en/catalog/industrial-heaters-and-systems/electric-steam-boilers       valid
Crawling page: https://www.chromalox.com/en/catalog/industrial-heaters-and-systems/power-control-systems       valid
Crawling page: https://www.chromalox.com/en/catalog/industrial-heaters-and-systems/fuel-gas-conditioning-systems       valid
Crawling page: https://www.chromalox.com/en/catalog/industrial-heaters-and-systems/packaged-process-skids       valid
Crawling page: https://www.chromalox.com/en/catalog/industrial-heaters-and-systems/process-heaters       valid
Crawling page: https://www.chromalox.com/en/catalog/industrial-heaters-and-systems       valid
Crawling page: https://www.chromalox.com/en/catalog/heat-trace/xtremeduty       valid
Crawling page: https://www.chromalox.com/en/catalog/heat-trace/tube-bundle-and-instrument-enclosures       valid
Crawling page: https://www.chromalox.com/en/catalog/heat-trace/skin-effect-heating       valid
Crawling page: https://www.chromalox.com/en/catalog/heat-trace/heat-trace-accessories       valid
Crawling page: https://www.chromalox.com/en/catalog/heat-trace/heat-trace-connection-kits       valid
Crawling page: https://www.chromalox.com/en/catalog/heat-trace/wireless-sensing-solutions       valid
Crawling page: https://www.chromalox.com/en/catalog/heat-trace/heat-trace-controls       valid
Crawling page: https://www.chromalox.com/en/catalog/heat-trace/heat-trace-cables       valid
Crawling page: https://www.chromalox.com/en/catalog/heat-trace       valid
Crawling page: https://www.chromalox.com/en/about-us       valid
Crawling page: https://www.chromalox.com/en/resources-and-support       valid
Crawling page: https://www.chromalox.com/en/industries       valid
Crawling page: https://www.chromalox.com/en/technologies       valid
Crawling page: https://www.chromalox.com/en/solutions       valid
Crawling page: https://www.chromalox.com/en/catalog       valid
Crawling page: https://www.chromalox.com/en/resources-and-support/request-for-quote       valid
Crawling page: https://www.chromalox.com/       valid
Crawling page: https://www.chromalox.com/en/login?ReturnUrl=%2fen%2fmy-account%2fcarts%2fview-cart       valid
Crawling page: http://www.thoratec.com/vad-trials-outcomes/ongoing-clinical-trials/index.aspx       valid
Crawling page: http://www.thoratec.com/vad-trials-outcomes/ongoing-clinical-trials/hmiii-usa.aspx       valid
Crawling page: http://www.thoratec.com/vad-trials-outcomes/in-the-news.aspx       valid
Crawling page: https://www.chromalox.com/en/login?ReturnUrl=%2fen%2fmy-account       valid
Crawling page: https://www.chromalox.com/en/resources-and-support/locate-a-rep       valid
Crawling page: http://www.thoratec.com/vad-trials-outcomes/index.aspx       valid
Crawling page: http://www.thoratec.com/videos/pc-considering-vad.aspx       valid
Crawling page: http://www.thoratec.com/patients-caregivers/glossary.aspx       valid
Crawling page: http://www.thoratec.com/patients-caregivers/request-patient-materials.aspx       valid
Crawling page: http://www.thoratec.com/vad-trials-outcomes/patient-stories/index.aspx       valid
Crawling page: http://www.thoratec.com/patients-caregivers/index.aspx       valid
Crawling page: http://www.thoratec.com/videos/mp-mcs.aspx       valid
Crawling page: http://pdf.com/sitemap       valid
Crawling page: http://pdf.com/Legal       valid
Crawling page: http://www.thoratec.com/patients-caregivers/about-heart-failure.aspx       valid
Crawling page: http://www.thoratec.com/patients-caregivers/considering-vad.aspx       valid
Crawling page: http://www.thoratec.com/medical-professionals/heartmate-ii-reported-icd-experience.aspx       valid
Crawling page: http://www.thoratec.com/medical-professionals/vad-training.aspx       valid
Crawling page: http://www.thoratec.com/medical-professionals/events-ongoing-education/index.aspx       valid
Crawling page: http://www.thoratec.com/medical-professionals/patient-education/index.aspx       valid
Crawling page: http://pdf.com/press-releases/Doctor_Gerald_Zhiyao_Yin_Joins_PDF_Solutions_Board_of_Directors       valid
Crawling page: http://pdf.com/press-releases/PDF_Solutions_Reports_Fiscal_Year_2018_Second_Quarter_Results       valid
Crawling page: http://pdf.com/events/PDF_Solutions_Announces_Second_Fiscal_Quarter_2018_Financial_Results_Release_Date       valid
Crawling page: http://pdf.com/privacy-policy       valid
Crawling page: http://www.thoratec.com/medical-professionals/treating-advanced-heart-failure/index.aspx       valid
Crawling page: http://www.thoratec.com/medical-professionals/index.aspx       valid
Crawling page: http://www.thoratec.com/about-us/index.aspx       valid
Crawling page: http://pdf.com/press-releases/PDF_Solutions_Reports_Third_Fiscal_Quarter_2018_Results       valid
Crawling page: http://www.pdf.com/press-releases/PDF_Solutions_Adds_Complete_Device_Traceability_and_Process_Data_Collection_Through_Assembly_and_Test_to_the_Exensio_Platform_With_the_ALPS_Product_Acquisition       valid
Crawling page: http://pdf.com/press-releases/Michael_Gustafson_Joins_PDF_Solutions_Board_of_Directors       valid
Crawling page: http://pdf.com/events/PDF_Solutions_Announces_Third_Fiscal_Quarter_2018_Financial_Results_Release_Date       valid
Crawling page: http://www.thoratec.com/       valid
Crawling page: http://pdf.com/press-releases/PDF_Solutions_Announces_Appointment_of_Christine_Russell_as_Chief_Financial_Officer_       valid
Crawling page: http://www.thoratec.com/medical-professionals/reimbursement/login.aspx       valid
Crawling page: http://pdf.com/Exensio-Test       valid
Crawling page: http://pdf.com/Exensio-Yield       valid
Crawling page: http://www.thoratec.com/medical%2Dprofessionals/select%2Dlocation/       valid
Crawling page: http://pdf.com/yrs       valid
Crawling page: http://pdf.com/Exensio-Control       valid
Crawling page: https://www.heartmate.com       valid
Crawling page: http://pdf.com/exensio       valid
Crawling page: http://pdf.com/efficiency-aware-process-control       valid2018-11-12 02:55:02 [scrapy.extensions.logstats] INFO: Crawled 761 pages (at 19 pages/min), scraped 670 items (at 16 items/min)
2018-11-12 02:56:13 [scrapy.extensions.logstats] INFO: Crawled 783 pages (at 22 pages/min), scraped 691 items (at 21 items/min)
2018-11-12 02:57:08 [scrapy.extensions.logstats] INFO: Crawled 795 pages (at 12 pages/min), scraped 706 items (at 15 items/min)
2018-11-12 02:57:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.meyertool.com/tel:5138534400>: HTTP status code is not handled or not allowed
2018-11-12 02:58:02 [scrapy.extensions.logstats] INFO: Crawled 814 pages (at 19 pages/min), scraped 720 items (at 14 items/min)
2018-11-12 02:59:18 [scrapy.extensions.logstats] INFO: Crawled 821 pages (at 7 pages/min), scraped 731 items (at 11 items/min)
2018-11-12 03:00:20 [scrapy.extensions.logstats] INFO: Crawled 824 pages (at 3 pages/min), scraped 738 items (at 7 items/min)
/bin/sh: 1: kill: No such process

2018-11-12 03:01:11 [scrapy.extensions.logstats] INFO: Crawled 836 pages (at 12 pages/min), scraped 746 items (at 8 items/min)
2018-11-12 03:02:16 [scrapy.extensions.logstats] INFO: Crawled 844 pages (at 8 pages/min), scraped 757 items (at 11 items/min)
2018-11-12 03:03:03 [scrapy.extensions.logstats] INFO: Crawled 853 pages (at 9 pages/min), scraped 765 items (at 8 items/min)

Crawling page: http://pdf.com/new-product-introduction       valid
Crawling page: http://pdf.com/yield-aware-process-control       valid
Crawling page: http://ir.pdf.com/information-request       valid
Crawling page: http://ir.pdf.com/investor-faq       valid
Crawling page: http://pdf.com/integrated-yield-ramp       valid
Crawling page: http://pdf.com/silicon-wafer-manufacturing       valid
Crawling page: http://pdf.com/flat-panel-display       valid
Crawling page: http://pdf.com/fabless-semiconductor-industry       valid
Crawling page: http://ir.pdf.com/webcasts       valid
Crawling page: http://ir.pdf.com/financial-reports       valid
Crawling page: http://pdf.com/semiconductor-industry       valid
Crawling page: http://pdf.com/management-team       valid
Crawling page: http://ir.pdf.com/fundamentals       valid
Crawling page: http://pdf.com/bod2       valid
Crawling page: http://pdf.com/WWPDF       valid
Crawling page: http://ir.pdf.com/analyst-coverage       valid
Crawling page: http://ir.pdf.com/stock-quote       valid
Crawling page: http://pdf.com/corporate-overview       valid
Crawling page: http://pdf.com/by-product       valid
Crawling page: http://pdf.com/by-service       valid
Crawling page: http://pdf.com/by-industry       valid
Crawling page: http://pdf.com/ir-governance       valid
Crawling page: http://pdf.com/financial-news       valid
Crawling page: http://pdf.com/investors-management-team       valid
Crawling page: http://pdf.com/solutions       valid
Crawling page: http://pdf.com/investors-corporate-overview       valid
Crawling page: https://www.fike.com/terms-conditions/       valid
Crawling page: https://www.fike.com/resources/certifications-approvals/       valid
Crawling page: https://www.fike.com/privacy-policy-2/       valid
Crawling page: http://pdf.com/publications       valid
Crawling page: https://www.fike.com/terms/       valid
Crawling page: https://www.fike.com/markets/oil-gas/       valid
Crawling page: https://www.fike.com/markets/telecommunications/       valid
Crawling page: https://www.fike.com/markets/dry-bulk-solids-handling/       valid
Crawling page: http://pdf.com/events       valid
Crawling page: http://pdf.com/press-releases       valid
Crawling page: http://pdf.com/job-openings       valid
Crawling page: https://www.fike.com/markets/data-centers/       valid
Crawling page: https://www.fike.com/markets/chemical/       valid
Crawling page: http://pdf.com/working-at-pdf       valid
Crawling page: https://www.fike.com/solutions/overpressure-protection/       valid
Crawling page: https://www.fike.com/solutions/explosion-protection/       valid
Crawling page: https://www.fike.com/solutions/pressure-activation/       valid
Crawling page: https://www.fike.com/solutions/fire-protection/       valid
Crawling page: https://www.fike.com/myfike-login/       valid
Crawling page: http://pdf.com/request-info       valid
Crawling page: http://pdf.com/locations       valid
Crawling page: http://pdf.com/careers       valid
Crawling page: https://www.fike.com/contact-us/find-expert/       valid
Crawling page: https://www.fike.com/press-releases       valid
Crawling page: https://www.fike.com/markets/       valid
Crawling page: https://www.fike.com/resources/       valid
Crawling page: http://pdf.com/support       valid
Crawling page: https://www.fike.com/industry-events/       valid
Crawling page: https://www.fike.com/contact-us/       valid
Crawling page: https://www.fike.com/capabilities/       valid
Crawling page: https://www.meyertool.com/mt-repair-overhaul/       valid
Crawling page: https://www.meyertool.com/florida-aero-precision/       valid
Crawling page: https://www.meyertool.com/meyer-tool-huntersville/       valid
Crawling page: https://www.meyertool.com/amk-welding-location/       valid
Crawling page: https://www.meyertool.com/meyer-tool-canada/       valid
Crawling page: https://www.meyertool.com/mt-texas/       valid
Crawling page: https://www.pacb.com/legal-and-trademarks/site-usage/       valid
Crawling page: https://www.pacb.com/legal-and-trademarks/privacy-policy/       valid
Crawling page: https://www.pacb.com/company/news-events/media-resources/       valid
Crawling page: https://www.carbon3d.com/terms-of-use/       valid
Crawling page: https://www.meyertool.com/meyer-tool-media-kit/       valid
Crawling page: https://www.pacb.com/contact-us       valid
Crawling page: https://www.pacb.com/contact-us/distributors/       valid
Crawling page: https://www.carbon3d.com/stories/aptiv-qualifies-carbon-for-stringent-parts/       valid
Crawling page: https://www.carbon3d.com/stories/production-at-bd/       valid
Crawling page: https://www.carbon3d.com/stories/notes-from-the-lab/       valid
Crawling page: https://www.carbon3d.com/stories/wilson-tool-taps-carbon-for-quicktap/       valid
Crawling page: https://www.carbon3d.com/contact/       valid
Crawling page: https://www.carbon3d.com/news/       valid
Crawling page: https://www.carbon3d.com/about/       valid
Crawling page: https://www.carbon3d.com/materials/       valid
Crawling page: https://www.carbon3d.com/process/       valid
Crawling page: https://www.carbon3d.com/       valid
Crawling page: https://www.carbon3d.com/stories/vitamix-produces-10x-durable-parts-scale-carbon/       valid
Crawling page: https://www.kerrdental.com/resource-center?f[0]=field_resource_document_type%3A633       valid
Crawling page: https://www.kerrdental.com/sitemap       valid
Crawling page: https://www.kerrdental.com/contact       valid
Crawling page: https://www.kerrdental.com/resource-center?f[0]=field_resource_document_type%3A657       valid
Crawling page: https://www.kerrdental.com/kerr-restoratives/harmonize-harmonize-universal-composite       valid
Crawling page: https://www.kerrdental.com/kerr-rotary/kavo-kerr-trimming-finishing-carbides-kavo-kerr-trimming-finishing-carbides       valid
Crawling page: https://www.kerrdental.com/kerr-restoratives/sonicfill-3-single-fill-composite-system       valid
Crawling page: https://www.kerrdental.com/how-buy       valid
Crawling page: https://www.kerrdental.com/schedule-demo       valid
Crawling page: https://www.kerrdental.com/news       valid
Crawling page: https://www.kerrdental.com/our-company       valid
Crawling page: https://www.kerrdental.com/resource-center       valid
Crawling page: https://www.kerrdental.com/courses-and-events       valid
Crawling page: https://www.kerrdental.com/promotions       valid
Crawling page: https://www.kerrdental.com/lab-tech       valid
Crawling page: https://www.kerrdental.com/procedures       valid
Crawling page: https://www.kerrdental.com/hygienists       valid
Crawling page: https://www.kerrdental.com/dentists       valid
Crawling page: https://www.kerrdental.com/endodontist       valid
Crawling page: https://www.kerrdental.com/kerr-laboratory/bellewax-esthetic-wax-waxes       valid
Crawling page: https://www.kerrdental.com/kerr-laboratory/bellewax-dipping-wax-waxes       valid
Crawling page: https://www.kerrdental.com/kerr-laboratory/base-plate-wax-waxes       valid
Crawling page: https://www.kerrdental.com/kerr-laboratory/bellewax-sculpturing-wax-waxes       valid
Crawling page: https://www.babcockpower.com/privacy-policy/       valid
Crawling page: https://www.kerrdental.com/kerr-laboratory/solutions       valid
Crawling page: https://www.kerrdental.com/kerr-laboratory/instruments       valid
Crawling page: https://www.kerrdental.com/kerr-laboratory/restorative-lab-materials       valid
Crawling page: https://www.kerrdental.com/kerr-laboratory/millable-materials       valid
Crawling page: https://www.kerrdental.com/kerr-laboratory/equipment       valid
Crawling page: https://www.kerrdental.com/kerr-laboratory/gypsum       valid
Crawling page: https://www.babcockpower.com/resource-library/       valid
Crawling page: https://www.babcockpower.com/babcock-power-environmental-inc-secures-major-agreement-with-bhel/       valid
Crawling page: https://www.babcockpower.com/sulphur-conference-2018/       valid
Crawling page: https://www.babcockpower.com/2018/10/       valid
Crawling page: https://www.babcockpower.com/2018/08/       valid
Crawling page: https://www.babcockpower.com/contact-international/2018-11-12 03:04:08 [scrapy.extensions.logstats] INFO: Crawled 871 pages (at 18 pages/min), scraped 780 items (at 15 items/min)
2018-11-12 03:05:10 [scrapy.extensions.logstats] INFO: Crawled 885 pages (at 14 pages/min), scraped 794 items (at 14 items/min)
2018-11-12 03:06:34 [scrapy.extensions.logstats] INFO: Crawled 896 pages (at 11 pages/min), scraped 812 items (at 18 items/min)
2018-11-12 03:07:13 [scrapy.extensions.logstats] INFO: Crawled 905 pages (at 9 pages/min), scraped 819 items (at 7 items/min)
2018-11-12 03:08:06 [scrapy.extensions.logstats] INFO: Crawled 921 pages (at 16 pages/min), scraped 830 items (at 11 items/min)
2018-11-12 03:09:07 [scrapy.extensions.logstats] INFO: Crawled 929 pages (at 8 pages/min), scraped 842 items (at 12 items/min)
2018-11-12 03:10:25 [scrapy.extensions.logstats] INFO: Crawled 936 pages (at 7 pages/min), scraped 853 items (at 11 items/min)
2018-11-12 03:11:42 [scrapy.extensions.logstats] INFO: Crawled 947 pages (at 11 pages/min), scraped 860 items (at 7 items/min)
2018-11-12 03:12:41 [scrapy.extensions.logstats] INFO: Crawled 947 pages (at 0 pages/min), scraped 864 items (at 4 items/min)
2018-11-12 03:12:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://investor.pacificbiosciences.com/> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 03:13:27 [scrapy.extensions.logstats] INFO: Crawled 957 pages (at 10 pages/min), scraped 868 items (at 4 items/min)
2018-11-12 03:14:06 [scrapy.extensions.logstats] INFO: Crawled 968 pages (at 11 pages/min), scraped 874 items (at 6 items/min)
2018-11-12 03:15:11 [scrapy.extensions.logstats] INFO: Crawled 979 pages (at 11 pages/min), scraped 884 items (at 10 items/min)
       valid
Crawling page: https://www.babcockpower.com/contact-us/       valid
Crawling page: https://www.babcockpower.com/about/       valid
Crawling page: https://www.babcockpower.com/2018/       valid
Crawling page: https://www.babcockpower.com/waste-to-energy-aftermarket/       valid
Crawling page: https://www.babcockpower.com/biomass-aftermarket/       valid
Crawling page: https://www.babcockpower.com/cement-limestone/       valid
Crawling page: https://www.babcockpower.com/pulp-paper-aftermarket/       valid
Crawling page: https://www.babcockpower.com/oil-gas-petro-aftermarket/       valid
Crawling page: https://www.babcockpower.com/solar-aftermarket/       valid
Crawling page: https://www.babcockpower.com/nuclear-aftermarket/       valid
Crawling page: https://www.babcockpower.com/waste-to-energy/       valid
Crawling page: https://www.babcockpower.com/biomass/       valid
Crawling page: https://www.babcockpower.com/combined-cycle-aftermarket/       valid
Crawling page: https://www.babcockpower.com/coal-aftermarket/       valid
Crawling page: https://www.babcockpower.com/pulp-and-paper/       valid
Crawling page: https://www.babcockpower.com/flue-gas/       valid
Crawling page: https://www.babcockpower.com/oil-gas-petrochemical-chemical/       valid
Crawling page: https://www.babcockpower.com/solar/       valid
Crawling page: https://www.babcockpower.com/nuclear/       valid
Crawling page: https://www.babcockpower.com/coal/       valid
Crawling page: https://www.babcockpower.com/combined-cycle/       valid
Crawling page: https://www.babcockpower.com/vogt/       valid
Crawling page: https://www.babcockpower.com/teichx/       valid
Crawling page: https://www.babcockpower.com/teic/       valid
Crawling page: https://www.babcockpower.com/tei/       valid
Crawling page: https://www.babcockpower.com/bta/       valid
Crawling page: https://www.meyertool.com/meyer-tool-new-york/       valid
Crawling page: https://www.babcockpower.com/bps/       valid
Crawling page: https://www.meyertool.com/meyer-tool-greenville/       valid
Crawling page: https://www.babcockpower.com/imtec-door/       valid
Crawling page: https://www.babcockpower.com/riley-power/       valid
Crawling page: https://www.babcockpower.com/struthers-wells/       valid
Crawling page: https://www.meyertool.com/meyer-tool-poland/       valid
Crawling page: https://www.meyertool.com/meyer-tool-cox-plant/       valid
Crawling page: https://www.babcockpower.com/bpe/       valid
Crawling page: https://www.meyertool.com/repair-overhaul/       valid
Crawling page: https://www.meyertool.com/supply-sectors/       valid
Crawling page: https://www.meyertool.com/meyer-tool-headquarters/       valid
Crawling page: https://www.meyertool.com/public-documents/       valid
Crawling page: https://www.meyertool.com/locations/       valid
Crawling page: https://www.meyertool.com/aerospace/       valid
Crawling page: https://www.meyertool.com/capabilities-by-location/       valid
Crawling page: https://www.meyertool.com/materials-lab/       valid
Crawling page: https://www.meyertool.com/orion-spc/       valid
Crawling page: https://www.meyertool.com/robotic-eddy-current/       valid
Crawling page: https://www.meyertool.com/certifications/       valid
Crawling page: https://www.meyertool.com/ultrasonic-testing/       valid
Crawling page: https://www.meyertool.com/magnetic-particle-inspection/       valid
Crawling page: https://www.meyertool.com/fpi/       valid
Crawling page: https://www.meyertool.com/solaris/       valid
Crawling page: https://www.meyertool.com/efa/       valid
Crawling page: https://www.meyertool.com/airflow/       valid
Crawling page: https://www.meyertool.com/vision-systems/       valid
Crawling page: https://www.meyertool.com/ct-x-ray/       valid
Crawling page: https://www.meyertool.com/x-ray/       valid
Crawling page: https://www.meyertool.com/blue-light-scanning/       valid
Crawling page: https://www.meyertool.com/equator-cmms/       valid
Crawling page: https://www.meyertool.com/eb-welding/       valid
Crawling page: https://www.meyertool.com/measurement-and-testing/       valid
Crawling page: https://www.meyertool.com/platinum-plating/       valid
Crawling page: https://www.meyertool.com/chemical-stripping/       valid
Crawling page: https://www.meyertool.com/materials-joining/       valid
Crawling page: https://www.meyertool.com/coating/       valid
Crawling page: https://www.meyertool.com/design-tool-build/       valid
Crawling page: https://www.meyertool.com/turning-milling-drilling/       valid
Crawling page: https://www.meyertool.com/grinding/       valid
Crawling page: https://www.meyertool.com/laser/       valid
Crawling page: https://www.meyertool.com/waterjet/       valid
Crawling page: https://www.meyertool.com/meyer-tool-culture/       valid
Crawling page: https://www.meyertool.com/about-us/       valid
Crawling page: https://www.meyertool.com/news/       valid
Crawling page: https://www.pacb.com/blog/a-new-paradigm-in-dna-sequencing-highly-accurate-single-molecule-long-reads/       valid
Crawling page: https://www.pacb.com/products-and-services/sequel-system/latest-system-release/       valid
Crawling page: https://www.pacb.com/blog/smrt-grant-winner-preventing-infection-for-premmies/       valid
Crawling page: https://www.pacb.com/blog/global_sequencing_projects/       valid
Crawling page: https://www.pacb.com/press_releases/illumina-to-acquire-pacific-biosciences-for-approximately-1-2-billion-broadening-access-to-long-read-sequencing-and-accelerating-scientific-discovery/       valid
Crawling page: https://www.pacb.com/applications/whole-genome-sequencing       valid
Crawling page: https://www.pacb.com/company/news-events/       valid
Crawling page: https://www.pacb.com/request-pricing/       valid
Crawling page: https://www.pacb.com/ask-a-question/       valid
Crawling page: https://www.pacb.com/company/careers/       valid
Crawling page: https://www.pacb.com/company/leadership/       valid
Crawling page: https://www.pacb.com/company/about-us/       valid
Crawling page: https://www.pacb.com/company/       valid
Crawling page: https://www.pacb.com/support/software-downloads/       valid
Crawling page: https://www.pacb.com/support/documentation/       valid
Crawling page: http://investor.pacificbiosciences.com/       Crawling page: https://www.pacb.com/smrt-science/smrt-sequencing/       valid
Crawling page: https://www.pacb.com/smrt-science/smrt-grant/       valid
Crawling page: https://www.pacb.com/SMRT-science/       valid
Crawling page: https://www.pacb.com/applications/epigenetics/       valid
Crawling page: https://www.pacb.com/applications/targeted-sequencing/       valid
Crawling page: https://www.pacb.com/smrt-science/smrt-resources/       valid
Crawling page: https://www.pacb.com/applications/complex-populations/       valid
Crawling page: https://www.pacb.com/research-focus/microbiology/       valid
Crawling page: https://www.pacb.com/applications/rna-sequencing/       valid
Crawling page: https://www.sarepta.com/terms-use       valid
Crawling page: https://www.sarepta.com/our-team/working-sarepta       valid
Crawling page: https://www.sarepta.com/privacy-policy       valid
Crawling page: https://www.sarepta.com/our-team/career-opportunities       valid
Crawling page: https://www.sarepta.com/grants-and-sponsorships       valid
Crawling page: https://www.sarepta.com/join-our-team       valid
Crawling page: https://www.sarepta.com/community/managed-access-program       valid
Crawling page: https://www.sarepta.com/community/compassionate-use-policy       valid
Crawling page: https://www.sarepta.com/clinical-trials       valid
Crawling page: https://www.sarepta.com/disease-resources       valid
Crawling page: https://www.pacb.com/applications/whole-genome-sequencing/       valid
Crawling page: https://www.sarepta.com/route79       valid
Crawling page: https://www.sarepta.com/our-community       valid
Crawling page: https://www.sarepta.com/pipeline/exon-skipping-duchenne       valid
Crawling page: https://www.sarepta.com/our-pipeline       valid
Crawling page: https://www.pacb.com/applications/whole-genome-sequencing/structural-variation/       valid2018-11-12 03:16:08 [scrapy.extensions.logstats] INFO: Crawled 979 pages (at 0 pages/min), scraped 891 items (at 7 items/min)
2018-11-12 03:17:24 [scrapy.extensions.logstats] INFO: Crawled 990 pages (at 11 pages/min), scraped 904 items (at 13 items/min)
2018-11-12 03:18:19 [scrapy.extensions.logstats] INFO: Crawled 1003 pages (at 13 pages/min), scraped 912 items (at 8 items/min)
2018-11-12 03:19:19 [scrapy.extensions.logstats] INFO: Crawled 1006 pages (at 3 pages/min), scraped 919 items (at 7 items/min)
2018-11-12 03:20:02 [scrapy.extensions.logstats] INFO: Crawled 1017 pages (at 11 pages/min), scraped 925 items (at 6 items/min)
2018-11-12 03:21:10 [scrapy.extensions.logstats] INFO: Crawled 1022 pages (at 5 pages/min), scraped 933 items (at 8 items/min)
2018-11-12 03:22:02 [scrapy.extensions.logstats] INFO: Crawled 1036 pages (at 14 pages/min), scraped 943 items (at 10 items/min)
2018-11-12 03:22:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.rockwellautomation.com/es_cem/overview.page>: HTTP status code is not handled or not allowed
2018-11-12 03:23:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.rockwellautomation.com/es_ar/overview.page>: HTTP status code is not handled or not allowed
2018-11-12 03:23:21 [scrapy.extensions.logstats] INFO: Crawled 1052 pages (at 16 pages/min), scraped 959 items (at 16 items/min)
2018-11-12 03:24:03 [scrapy.extensions.logstats] INFO: Crawled 1060 pages (at 8 pages/min), scraped 970 items (at 11 items/min)
2018-11-12 03:25:04 [scrapy.extensions.logstats] INFO: Crawled 1080 pages (at 20 pages/min), scraped 986 items (at 16 items/min)

Crawling page: https://www.pacb.com/applications/       valid
Crawling page: https://www.pacb.com/research-focus/plant-animal-sciences/       valid
Crawling page: https://www.sarepta.com/our-product       valid
Crawling page: https://www.sarepta.com/technology/manufacturing-excellence       valid
Crawling page: https://www.sarepta.com/company/strategic-partnerships       valid
Crawling page: https://www.sarepta.com/technology/therapeutic-applications       valid
Crawling page: https://www.sarepta.com/technology/technology-platform       valid
Crawling page: https://www.sarepta.com/technology/rna-biology-sarepta-therapeutics       valid
Crawling page: https://www.sarepta.com/our-technology       valid
Crawling page: https://www.sarepta.com/contact-us       valid
Crawling page: https://www.sarepta.com/company/senior-leadership       valid
Crawling page: https://www.sarepta.com/company/strategic-scientific-advisory-board       valid
Crawling page: https://www.sarepta.com/company/locations       valid
Crawling page: https://www.sarepta.com/company/board-directors       valid
Crawling page: https://www.sarepta.com/our-company       valid
Crawling page: https://www.sarepta.com/our-product/sareptassist-es       valid
Crawling page: https://www.sarepta.com/our-product/sareptassist       valid
Crawling page: https://www.rockwellautomation.com/en_IN/overview.page       valid
Crawling page: https://www.rockwellautomation.com/ja_JP/overview.page       valid
Crawling page: https://www.rockwellautomation.com/en_SEA/overview.page       valid
Crawling page: https://www.rockwellautomation.com/ko_KR/overview.page       valid
Crawling page: https://www.rockwellautomation.com/zh_CN/overview.page       valid
Crawling page: https://www.rockwellautomation.com/en_AU/overview.page       valid
Crawling page: https://www.rockwellautomation.com/tr_TR/overview.page       valid
Crawling page: https://www.rockwellautomation.com/sv_SE/overview.page       valid
Crawling page: https://www.rockwellautomation.com/en_ZA/overview.page       valid
Crawling page: https://www.rockwellautomation.com/en_UK/overview.page       valid
Crawling page: https://www.rockwellautomation.com/en_MDE/overview.page       valid
Crawling page: https://www.rockwellautomation.com/ru_RU/overview.page       valid
Crawling page: https://www.rockwellautomation.com/pt_PT/overview.page       valid
Crawling page: https://www.rockwellautomation.com/fr_CH/overview.page       valid
Crawling page: https://www.rockwellautomation.com/de_CH/overview.page       valid
Crawling page: https://www.rockwellautomation.com/ro_RO/overview.page       valid
Crawling page: https://www.rockwellautomation.com/de_AT/overview.page       valid
Crawling page: https://www.rockwellautomation.com/it_IT/overview.page       valid
Crawling page: https://www.rockwellautomation.com/pl_PL/overview.page       valid
Crawling page: https://www.rockwellautomation.com/fr_FR/overview.page       valid
Crawling page: https://www.rockwellautomation.com/hu_HU/overview.page       valid
Crawling page: https://www.rockwellautomation.com/nl_NL/overview.page       valid
Crawling page: https://www.rockwellautomation.com/en_IL/overview.page       valid
Crawling page: https://www.rockwellautomation.com/en_IE/overview.page       valid
Crawling page: https://www.rockwellautomation.com/da_DK/overview.page       valid
Crawling page: https://www.rockwellautomation.com/es_ES/overview.page       valid
Crawling page: https://www.rockwellautomation.com/de_DE/overview.page       valid
Crawling page: https://www.rockwellautomation.com/cs_CZ/overview.page       valid
Crawling page: https://www.rockwellautomation.com/nl_BE/overview.page       valid
Crawling page: https://www.rockwellautomation.com/fr_BE/overview.page       valid
Crawling page: https://www.rockwellautomation.com/es_VE/overview.page       valid
Crawling page: https://www.rockwellautomation.com/es_PE/overview.page       valid
Crawling page: http://www.universalcorp.com/Home/VideoLibrary       valid
Crawling page: http://www.universalcorp.com/Home/SiteIndex       valid
Crawling page: http://www.universalcorp.com/Practices/SocialResponsibility       valid
Crawling page: http://www.universalcorp.com/Practices/LaborPractices       valid
Crawling page: http://www.universalcorp.com/Practices/PIT       valid
Crawling page: http://www.universalcorp.com/Practices/HealthSafety       valid
Crawling page: https://www.rockwellautomation.com       valid
Crawling page: https://www.rockwellautomation.com/en_CAR/overview.page       valid
Crawling page: https://www.rockwellautomation.com/es_CAR/overview.page       valid
Crawling page: http://www.universalcorp.com/Practices/GAP       valid
Crawling page: http://www.universalcorp.com/Practices/GMP       valid
Crawling page: http://www.universalcorp.com/Practices/FarmerTraining       valid
Crawling page: http://www.universalcorp.com/Practices/EnvironmentalPerformance       valid
Crawling page: https://www.rockwellautomation.com/es_EC/overview.page       valid
Crawling page: https://www.rockwellautomation.com/pt_BR/overview.page       valid
Crawling page: https://www.rockwellautomation.com/es_CO/overview.page       valid
Crawling page: https://www.rockwellautomation.com/es_CL/overview.page       valid
Crawling page: http://www.universalcorp.com/Practices       valid
Crawling page: http://www.universalcorp.com/Investors/WhyInvest       valid
Crawling page: http://www.universalcorp.com/Investors       valid
Crawling page: http://www.universalcorp.com/Compliance       valid
Crawling page: https://www.rockwellautomation.com/es_MX/overview.page       valid
Crawling page: https://www.rockwellautomation.com/en_NA/overview.page       valid
Crawling page: https://www.rockwellautomation.com/global/overview.page       valid
Crawling page: http://www.universalcorp.com/Foundation       valid
Crawling page: http://www.universalcorp.com/YouthSupport       valid
Crawling page: http://www.universalcorp.com/CommunitySupport       valid
Crawling page: http://www.universalcorp.com/Education       valid
Crawling page: http://www.universalcorp.com/Sustainability       valid
Crawling page: http://www.universalcorp.com/FarmerSupport       valid
Crawling page: http://www.universalcorp.com/Impact       valid
Crawling page: http://www.universalcorp.com/UniversalEnterprises       valid
Crawling page: http://www.universalcorp.com/OurCompany/UniversalIngredients       valid
Crawling page: http://www.universalcorp.com/UniversalLeaf       valid
Crawling page: http://www.universalcorp.com/OurCompany       valid
Crawling page: http://www.universalcorp.com/AboutUs/CorpLeadership       valid
Crawling page: http://www.universalcorp.com/AboutUs/NextHundred       valid
Crawling page: http://www.universalcorp.com/AboutUs/CoreBeliefs       valid
Crawling page: http://www.universalcorp.com/AboutUs/Communities       valid
Crawling page: http://www.universalcorp.com/AboutUs/History       valid
Crawling page: http://www.universalcorp.com/AboutUs       valid
Crawling page: http://www.universalcorp.com/Countries       valid
Crawling page: http://www.universalcorp.com/Careers       valid
Crawling page: http://www.universalcorp.com/Contact       valid
Crawling page: https://www.kerrdental.com/kerr-laboratory/sundries       valid
Crawling page: https://www.kerrdental.com/kerr-laboratory/waxes       valid
Crawling page: https://www.kerrdental.com/kerr-totalcare/cavicide1-caviwipes1-surface-disinfectants       valid
Crawling page: https://www.kerrdental.com/kerr-totalcare/caviwipes-af-cavicide-af-surface-disinfectants       valid
Crawling page: https://www.kerrdental.com/kerr-totalcare/cavicide-caviwipes-surface-disinfectants       valid
Crawling page: https://www.kerrdental.com/kerr-totalcare/accessory-products       valid
Crawling page: https://www.kerrdental.com/kerr-totalcare/kits       valid
Crawling page: https://www.kerrdental.com/kerr-totalcare/operatory-disposable       valid
Crawling page: https://www.kerrdental.com/kerr-totalcare/office-organization       valid
Crawling page: https://www.kerrdental.com/kerr-totalcare/instrument-reprocessing       valid
Crawling page: https://www.kerrdental.com/kerr-totalcare/sealants-applicators2018-11-12 03:26:11 [scrapy.extensions.logstats] INFO: Crawled 1088 pages (at 8 pages/min), scraped 998 items (at 12 items/min)
2018-11-12 03:27:21 [scrapy.extensions.logstats] INFO: Crawled 1100 pages (at 12 pages/min), scraped 1010 items (at 12 items/min)
2018-11-12 03:28:07 [scrapy.extensions.logstats] INFO: Crawled 1108 pages (at 8 pages/min), scraped 1018 items (at 8 items/min)
/bin/sh: 1: kill: No such process

2018-11-12 03:29:00 [scrapy.extensions.logstats] INFO: Crawled 1117 pages (at 9 pages/min), scraped 1027 items (at 9 items/min)
/bin/sh: 1: kill: No such process

2018-11-12 03:30:14 [scrapy.extensions.logstats] INFO: Crawled 1130 pages (at 13 pages/min), scraped 1040 items (at 13 items/min)
2018-11-12 03:31:08 [scrapy.extensions.logstats] INFO: Crawled 1138 pages (at 8 pages/min), scraped 1048 items (at 8 items/min)
2018-11-12 03:32:20 [scrapy.extensions.logstats] INFO: Crawled 1153 pages (at 15 pages/min), scraped 1059 items (at 11 items/min)
2018-11-12 03:33:06 [scrapy.extensions.logstats] INFO: Crawled 1155 pages (at 2 pages/min), scraped 1067 items (at 8 items/min)
/bin/sh: 1: kill: No such process

2018-11-12 03:34:41 [scrapy.extensions.logstats] INFO: Crawled 1170 pages (at 15 pages/min), scraped 1080 items (at 13 items/min)
2018-11-12 03:35:28 [scrapy.extensions.logstats] INFO: Crawled 1170 pages (at 0 pages/min), scraped 1084 items (at 4 items/min)
/bin/sh: 1: kill: No such process

2018-11-12 03:37:21 [scrapy.extensions.logstats] INFO: Crawled 1191 pages (at 21 pages/min), scraped 1099 items (at 15 items/min)
       valid
Crawling page: https://www.kerrdental.com/kerr-totalcare/prophy-angles-brush       valid
Crawling page: https://www.kerrdental.com/kerr-totalcare/x-ray       valid
Crawling page: https://www.kerrdental.com/kerr-totalcare/air-water-syringe-tips       valid
Crawling page: https://www.kerrdental.com/kerr-totalcare/digital-film-holders       valid
Crawling page: https://www.kerrdental.com/kerr-totalcare/splash-protection       valid
Crawling page: https://www.kerrdental.com/kerr-rotary/nti-diamonds-diamonds       valid
Crawling page: https://www.kerrdental.com/kerr-totalcare/hand-hygiene       valid
Crawling page: https://www.kerrdental.com/kerr-totalcare/surface-disinfectants       valid
Crawling page: https://www.kerrdental.com/kerr-totalcare/barriers       valid
Crawling page: https://www.kerrdental.com/kerr-rotary/specialty-products       valid
Crawling page: https://www.kerrdental.com/kerr-rotary/lab-rotary-products       valid
Crawling page: https://www.kerrdental.com/kerr-rotary/nti-abrasives       valid
Crawling page: https://www.kerrdental.com/kerr-rotary/endodontics       valid
Crawling page: https://www.kerrdental.com/kerr-rotary/nti-universal-cutters       valid
Crawling page: https://www.kerrdental.com/kerr-rotary/office-organization       valid
Crawling page: https://www.kerrdental.com/kerr-rotary/diamond-discs       valid
Crawling page: https://www.kerrdental.com/kerr-rotary/bur-blocks       valid
Crawling page: https://www.kerrdental.com/kerr-rotary/finishing-strips       valid
Crawling page: https://www.kerrdental.com/kerr-rotary/logic-sets       valid
Crawling page: https://www.kerrdental.com/kerr-rotary/specialty-carbides       valid
Crawling page: https://www.kerrdental.com/kerr-rotary/polishers       valid
Crawling page: https://www.kerrdental.com/kerr-rotary/diamonds       valid
Crawling page: https://www.kerrdental.com/kerr-rotary/operative-carbides       valid
Crawling page: https://www.kerrdental.com/kerr-rotary/trimming-finishing-carbides       valid
Crawling page: https://www.kerrdental.com/kerr-rotary/specialty-diamonds       valid
Crawling page: https://www.kerrdental.com/kerr-endodontics/k3-xf-niti-endo-files-shape       valid
Crawling page: https://www.kerrdental.com/kerr-endodontics/elements-motor-adaptive-motion-shape       valid
Crawling page: https://www.kerrdental.com/kerr-endodontics/tf-twisted-files-shape       valid
Crawling page: https://www.kerrdental.com/kerr-endodontics/tf-adaptive-niti-endo-file-system       valid
Crawling page: http://www.universalcorp.com/       valid
Crawling page: https://www.kerrdental.com/kerr-endodontics/ultrasonics       valid
Crawling page: https://www.kerrdental.com/kerr-endodontics/retreat       valid
Crawling page: https://www.kerrdental.com/kerr-endodontics/clean       valid
Crawling page: https://www.kerrdental.com/kerr-endodontics/microsurgery       valid
Crawling page: https://www.kerrdental.com/kerr-endodontics/shape       valid
Crawling page: https://www.kerrdental.com/kerr-endodontics/diagnose       valid
Crawling page: https://www.kerrdental.com/kerr-endodontics/access       valid
Crawling page: https://www.kerrdental.com/kerr-endodontics/fill-obturation       valid
Crawling page: https://www.sarepta.com/       valid
Crawling page: https://www.kerrdental.com/kerr-restoratives/optibond-fl-filled-light-cure-total-etch-dental-adhesive       valid
Crawling page: https://www.kerrdental.com/kerr-restoratives/optibond-universal-bonding-agents       valid
Crawling page: https://www.kerrdental.com/kerr-restoratives/optibond-all-one-single-component-self-etch-dental-adhesive       valid
Crawling page: https://www.kerrdental.com/kerr-restoratives/optibond-xtr-self-etch-light-cure-universal-dental-adhesive       valid
Crawling page: https://www.kerrdental.com/kerr-restoratives/accessory-products       valid
Crawling page: https://www.kerrdental.com/kerr-restoratives/temporization       valid
Crawling page: https://www.kerrdental.com/kerr-restoratives/alloys       valid
Crawling page: https://www.kerrdental.com/kerr-restoratives/tissue-management       valid
Crawling page: https://www.kerrdental.com/kerr-restoratives/cements       valid
Crawling page: https://www.kerrdental.com/kerr-restoratives/impression-materials       valid
Crawling page: https://www.kerrdental.com/kerr-restoratives/core-buildup       valid
Crawling page: https://www.kerrdental.com/kerr-restoratives/dental-curing-lights       valid
Crawling page: https://www.kerrdental.com/kerr-laboratory       valid
Crawling page: https://www.kerrdental.com/kerr-restoratives/bonding-agents       valid
Crawling page: https://www.kerrdental.com/kerr-totalcare       valid
Crawling page: https://www.kerrdental.com/kerr-restoratives/composites       valid
Crawling page: https://www.kerrdental.com/kerr-restoratives       valid
Crawling page: https://www.kerrdental.com/kerr-endodontics       valid
Crawling page: https://www.kerrdental.com/kerr-rotary       valid
Crawling page: https://www.kerrdental.com/search       valid
Crawling page: https://www.kerrdental.com/cart       valid
Crawling page: https://www.kerrdental.com/it-ch/       valid
Crawling page: https://www.kerrdental.com/de-ch/       valid
Crawling page: https://www.kerrdental.com/en-se/       valid
Crawling page: https://www.kerrdental.com/ru-ru/       valid
Crawling page: https://www.kerrdental.com/en-au/       valid
Crawling page: https://www.kerrdental.com/en-uk/       valid
Crawling page: https://www.kerrdental.com/fr-ch/       valid
Crawling page: https://www.kerrdental.com/en-no/       valid
Crawling page: https://www.kerrdental.com/it-it/       valid
Crawling page: https://www.kerrdental.com/pl-pl/       valid
Crawling page: https://www.kerrdental.com/en-nl/       valid
Crawling page: https://www.kerrdental.com/fr-fr/       valid
Crawling page: https://www.kerrdental.com/en-fi/       valid
Crawling page: https://www.kerrdental.com/es-es/       valid
Crawling page: https://www.kerrdental.com/en-eu/       valid
Crawling page: https://www.kerrdental.com/de-de/       valid
Crawling page: https://www.kerrdental.com/       valid
Crawling page: https://www.meyertool.com       valid
Crawling page: https://www.kerrdental.com/en-ae/       valid
Crawling page: https://www.kerrdental.com/ca/       valid
Crawling page: https://www.kerrdental.com/en-dk/       valid
Crawling page: https://www.kerrdental.com/en-be/       valid
Crawling page: https://www.pacb.com/products-and-services/       valid
Crawling page: https://www.pacb.com/products-and-services/sequel-system/       valid
Crawling page: https://www.pacb.com/products-and-services/consumables/       valid
Crawling page: https://www.pacb.com/products-and-services/analytical-software/       valid
Crawling page: https://www.kerrdental.com/fr-be/       valid
Crawling page: https://www.pacb.com/products-and-services/service-providers/       valid
Crawling page: https://www.pacb.com/products-and-services/smrt-compatible-products/       valid
Crawling page: https://www.pacb.com/research-focus/       valid
Crawling page: https://www.pacb.com/research-focus/human/       valid
Crawling page: http://www.semma-tx.com/       valid
Crawling page: http://www.semma-tx.com/about/leadership       valid
Crawling page: http://www.semma-tx.com/about       valid
Crawling page: http://www.unicharm.co.jp/products_zh/index.html       valid
Crawling page: http://www.semma-tx.com/about/board       valid
Crawling page: http://www.unicharm.co.jp/index.html       valid
Crawling page: http://www.semma-tx.com/about/sab       valid
Crawling page: http://www.semma-tx.com/research       valid
Crawling page: http://www.unicharm.co.jp/english/about/index.html       valid
Crawling page: http://www.unicharm.co.jp/english/ir/index.html       valid
Crawling page: http://www.semma-tx.com/media1       valid
Crawling page: https://www.pacb.com/support/technical-support/       valid
Crawling page: https://www.pacb.com/support/training/       valid
Crawling page: https://www.pacb.com/support/       valid
Crawling page: https://www.pacb.com/smrt-science/smrt-grant/2018-structural-variation-smrt-grant-program/       valid
Crawling page: http://www.unicharm.co.jp/english/supply.html2018-11-12 03:38:27 [scrapy.extensions.logstats] INFO: Crawled 1191 pages (at 0 pages/min), scraped 1105 items (at 6 items/min)
2018-11-12 03:44:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.vizio.com/e-series> (referer: https://www.vizio.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 58, in parse_links
    browser.get(response.url)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: Timeout loading page after 300000ms

2018-11-12 03:49:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.vizio.com/d-series> (referer: https://www.vizio.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 58, in parse_links
    browser.get(response.url)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: Timeout loading page after 300000ms

2018-11-12 03:49:21 [scrapy.extensions.logstats] INFO: Crawled 1204 pages (at 13 pages/min), scraped 1112 items (at 7 items/min)
2018-11-12 03:54:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.vizio.com/smartcast-crave-home-audio> (referer: https://www.vizio.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 58, in parse_links
    browser.get(response.url)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: Timeout loading page after 300000ms

2018-11-12 03:54:42 [scrapy.extensions.logstats] INFO: Crawled 1204 pages (at 0 pages/min), scraped 1115 items (at 3 items/min)
2018-11-12 03:55:10 [scrapy.extensions.logstats] INFO: Crawled 1213 pages (at 9 pages/min), scraped 1117 items (at 2 items/min)
2018-11-12 03:56:27 [scrapy.extensions.logstats] INFO: Crawled 1221 pages (at 8 pages/min), scraped 1124 items (at 7 items/min)
2018-11-12 03:57:37 [scrapy.extensions.logstats] INFO: Crawled 1224 pages (at 3 pages/min), scraped 1132 items (at 8 items/min)
2018-11-12 03:58:02 [scrapy.extensions.logstats] INFO: Crawled 1225 pages (at 1 pages/min), scraped 1135 items (at 3 items/min)
2018-11-12 03:58:31 [root] ERROR: Unable to find match for url: https://www.inscape.tv/
2018-11-12 03:59:03 [scrapy.extensions.logstats] INFO: Crawled 1240 pages (at 15 pages/min), scraped 1142 items (at 7 items/min)
2018-11-12 03:59:48 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.teamexos.com/idea-list/more-physical-therapy-patients/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:00:07 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.teamexos.com/industries/community/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:00:07 [scrapy.extensions.logstats] INFO: Crawled 1253 pages (at 13 pages/min), scraped 1155 items (at 13 items/min)
2018-11-12 04:00:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.teamexos.com/ideas/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:00:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <502 https://www.teamexos.com/sitemap/>: HTTP status code is not handled or not allowed
2018-11-12 04:00:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.teamexos.com/careers/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:01:16 [scrapy.extensions.logstats] INFO: Crawled 1261 pages (at 8 pages/min), scraped 1163 items (at 8 items/min)
2018-11-12 04:06:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.vizio.com/m-series> (referer: https://www.vizio.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 58, in parse_links
    browser.get(response.url)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: Timeout loading page after 300000ms

2018-11-12 04:11:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.vizio.com/p-series> (referer: https://www.vizio.com/)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 58, in parse_links
    browser.get(response.url)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: Timeout loading page after 300000ms

2018-11-12 04:12:09 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.teamexos.com/industries/sport/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:12:09 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.teamexos.com/capabilities/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:12:09 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.teamexos.com/employers/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:12:09 [scrapy.extensions.logstats] INFO: Crawled 1267 pages (at 6 pages/min), scraped 1169 items (at 6 items/min)
2018-11-12 04:12:14 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.teamexos.com/industries/military/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.teamexos.com/industries/military/ took longer than 180.0 seconds..
2018-11-12 04:12:14 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.chromalox.com/en/catalog/component-technologies/drimeg-temperature-controllers>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.chromalox.com/en/catalog/component-technologies/drimeg-temperature-controllers took longer than 180.0 seconds..
2018-11-12 04:12:14 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.bauschhealth.com/site-map>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.bauschhealth.com/site-map took longer than 180.0 seconds..
2018-11-12 04:12:14 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.carbon3d.com/privacy-policy/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.carbon3d.com/privacy-policy/ took longer than 180.0 seconds..
2018-11-12 04:12:14 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.fike.com/about-us/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.fike.com/about-us/ took longer than 180.0 seconds..
2018-11-12 04:12:14 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.teamexos.com/healthcare/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.teamexos.com/healthcare/ took longer than 180.0 seconds..
2018-11-12 04:12:14 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.fike.com/solutions>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.TimeoutError: User timeout caused connection failure.
2018-11-12 04:12:14 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.praxair.com/industries/laboratories>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/python/failure.py", line 491, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.praxair.com/industries/laboratories took longer than 180.0 seconds..
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 04:12:49 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.fike.com/nist-sp-800-171-cybersecurity>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:12:49 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.carbon3d.com/stories/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:12:49 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.meyertool.com/trusted-manufacturing-partner/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:12:49 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.meyertool.com/industrial-gas-turbine/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:12:49 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.rockwellautomation.com/global/detail.page?pagetitle=Privacy-and-Cookies-Policy&content_type=legal&docid=2a08b7b7fdd23bc2f17895f25bd46657>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:12:49 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.rockwellautomation.com/en_NZ>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:12:49 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.rockwellautomation.com/zh_TW>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:12:49 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.rockwellautomation.com/global/legal-notices/overview.page>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2018-11-12 04:12:49 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.meyertool.com/edm/>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
/bin/sh: 1: kill: No such process

2018-11-12 04:13:00 [scrapy.extensions.logstats] INFO: Crawled 1270 pages (at 3 pages/min), scraped 1176 items (at 7 items/min)
/bin/sh: 1: kill: No such process

/bin/sh: 1: kill: No such process

2018-11-12 04:14:02 [scrapy.extensions.logstats] INFO: Crawled 1291 pages (at 21 pages/min), scraped 1189 items (at 13 items/min)
/bin/sh: 1: kill: No such process

       valid
Crawling page: http://www.unicharm.co.jp/english/csr/index.html       valid
Crawling page: https://www.pacb.com/smrt-science/smrt-resources/scientific-publications/       valid
Crawling page: https://www.pacb.com/blog/puerto-rican-genome/       valid
Crawling page: http://www.unicharm.co.jp/english/recruit/index.html       valid
Crawling page: https://www.pacb.com/smrt-science/smrt-resources/blog/       valid
Crawling page: https://www.chromalox.com/en/global/news/2018/august/c4_c4-ir       valid
Crawling page: https://www.vizio.com/tv-overview       valid
Crawling page: https://www.chromalox.com/en/global/events/2018/november/ispe       valid
Crawling page: https://www.vizio.com/speaker-comparison       valid
Crawling page: https://www.chromalox.com/en/global/news/2017/december/new-directconnect       valid
Crawling page: https://www.vizio.com/e-series       Crawling page: https://www.vizio.com/d-series       Crawling page: https://www.chromalox.com/en/global/news/2017/july/spirax-sarco-news-announcement       valid
Crawling page: https://www.chromalox.com/en/catalog/component-technologies/powerv-temperature-controllers       valid
Crawling page: https://www.chromalox.com/en/global/events/2018/november/adipec       valid
Crawling page: https://www.chromalox.com/en/global/events/2018/august/electrification       valid
Crawling page: https://www.vizio.com/smartcast-crave-home-audio       Crawling page: https://www.vizio.com/audio/multi-room.html       valid
Crawling page: https://www.teamexos.com/media-room/       valid
Crawling page: https://www.vizio.com/youtubetv.html       valid
Crawling page: https://www.teamexos.com/idea-list/fitness-center-trends/       valid
Crawling page: https://www.vizio.com/myvizio/account/login/referer/aHR0cHM6Ly93d3cudml6aW8uY29tL3NhbGVzL29yZGVyL2hpc3Rvcnkv/       valid
Crawling page: https://www.teamexos.com/idea-list/body-positive-gym-marketing/       valid
Crawling page: https://www.vizio.com/myvizio/account/login/referer/aHR0cHM6Ly93d3cudml6aW8uY29tL215dml6aW8vYWNjb3VudC9sb2dvdXQv/       valid
Crawling page: https://www.vizio.com/e75f1.html       valid
Crawling page: https://www.teamexos.com/case-study/elite-tactical-forces/       valid
Crawling page: https://www.teamexos.com/idea-list/6-things-that-successful-fitness-centers-get-right/       valid
Crawling page: https://www.teamexos.com/case-study/ctca/       valid
Crawling page: https://www.teamexos.com/privacy-policy-security-notice/       valid
Crawling page: https://www.vizio.com/resellers       valid
Crawling page: https://www.teamexos.com/idea-list/5-coaching-strategies-to-engage-your-clients/       valid
Crawling page: https://www.teamexos.com/terms-of-service/       valid
Crawling page: https://www.vizio.com/terms       valid
Crawling page: https://www.vizio.com/termsofuse       valid
Crawling page: https://www.vizio.com/shipping-policy       valid
Crawling page: https://www.vizio.com/site-map       valid
Crawling page: https://www.vizio.com/fr-ca/?country_code=CA&region=QC       valid
Crawling page: https://www.vizio.com/en-ca/?country_code=CA       valid
Crawling page: https://www.vizio.com/?country_code=US       valid
Crawling page: https://www.inscape.tv/       valid
Crawling page: https://www.praxair.com/our-company/sustainable-development/awards-and-recognition       valid
Crawling page: https://www.teamexos.com/about-2/       valid
Crawling page: https://www.praxair.com/our-company/sustainable-development/reporting-center       valid
Crawling page: https://www.praxair.com/our-company/sustainable-development/measuring-impact       valid
Crawling page: https://www.praxair.com/our-company/sustainable-development/climate-change       valid
Crawling page: https://www.praxair.com/news/2018/praxair-to-build-new-liquid-hydrogen-plant-in-la-porte-texas       valid
Crawling page: https://www.praxair.com/news/2018/praxair-to-expand-nitrogen-capacity-to-support-increasing-demand-of-world-scale-semiconductor-complex-in-south-korea       valid
Crawling page: https://www.praxair.com/news/2018/praxair-signs-new-long-term-hydrogen-supply-agreement-for-phillips-66-sweeny-refinery       valid
Crawling page: https://www.praxair.com/praxairiseverywhere       valid
Crawling page: https://www.praxair.com/resource-library/reports-papers-case-studies-and-presentations       valid
Crawling page: https://www.praxair.com/industries/welding-and-metal-fabrication       valid
Crawling page: https://www.praxair.com/investor-relations       valid
Crawling page: https://www.praxair.com/our-company/safety-and-environment/environment       valid
Crawling page: https://www.teamexos.com/idea-list/patient-retention-strategies/       valid
Crawling page: https://www.praxair.com/our-company/sustainable-development/priority-factors       valid
Crawling page: https://www.praxair.com/our-company/sustainable-development       valid
Crawling page: https://www.praxair.com/our-company/sustainable-development/targets-and-performance       valid
Crawling page: https://www.praxair.com/our-company/safety-and-environment/product-stewardship       valid
Crawling page: https://www.praxair.com/our-company/safety-and-environment/process-safety       valid
Crawling page: https://www.vizio.com/audio/home-theater.html?filter_soundbar_configurations=779       valid
Crawling page: https://www.praxair.com/our-company/our-people/global-giving       valid
Crawling page: https://www.vizio.com/sp70d5.html       valid
Crawling page: https://www.vizio.com/audio/home-theater.html?filter_soundbar_configurations=773       valid
Crawling page: https://www.vizio.com/audio/sp50d5.html       valid
Crawling page: https://www.praxair.com/our-company/safety-and-environment/personnel-safety       valid
Crawling page: https://www.praxair.com/our-company/safety-and-environment       valid
Crawling page: https://www.praxair.com/our-company/our-people/community-engagement       valid
Crawling page: https://www.praxair.com/our-company/our-people/diversity       valid
Crawling page: https://www.praxair.com/industries/refining       valid
Crawling page: https://www.vizio.com/m-series       Crawling page: https://www.vizio.com/store       valid
Crawling page: https://www.vizio.com/p-series       Crawling page: https://www.vizio.com/p-series-quantum       valid
Crawling page: https://www.praxair.com/our-company/our-people/environmental-engagement       valid
Crawling page: https://www.praxair.com/industries/oil-and-gas       valid
Crawling page: http://www.hrl.com/       valid
Crawling page: https://www.vizio.com/       valid
Crawling page: https://www.praxair.com/industries/pharmaceuticals-and-biotechnology       valid
Crawling page: https://www.praxair.com/industries/pulp-and-paper       valid
Crawling page: https://www.praxair.com/industries/metal-production       valid
Crawling page: https://www.pacb.com/       valid
Crawling page: https://www.rockwellautomation.com/site-selection.html       valid
Crawling page: https://verily.com/       valid
Crawling page: https://verily.com/roles/       valid
Crawling page: https://verily.com/partners/       valid
Crawling page: https://verily.com/newsroom/       valid
Crawling page: https://verily.com/codeofconduct/       valid
Crawling page: https://verily.com/privacy/       valid
Crawling page: https://verily.com/leadership/       valid
Crawling page: https://verily.com/projects/       valid
Crawling page: https://verily.com/terms/       valid
Crawling page: https://blog.verily.com       valid
Crawling page: https://www.richtek.com/?sc_lang=en       valid
Crawling page: https://ag.energy/contact-us-2/       valid
Crawling page: https://ag.energy/technology/       valid
Crawling page: https://ag.energy/about-2/       valid
Crawling page: https://ag.energy/about/       valid
Crawling page: https://www.richtek.com/Design%20Support/Quality%20And%20Certification?sc_lang=en       valid
Crawling page: https://www.richtek.com/Design%20Support/Packaging%20and%20Reliability?sc_lang=en       valid
Crawling page: https://www.richtek.com/Design%20Support/Design%20Tools?sc_lang=en       valid
Crawling page: https://ag.energy/       valid
Crawling page: https://www.richtek.com/Design%20Support/Cross%20Reference?sc_lang=en2018-11-12 04:15:04 [scrapy.extensions.logstats] INFO: Crawled 1295 pages (at 4 pages/min), scraped 1197 items (at 8 items/min)
2018-11-12 04:16:15 [scrapy.extensions.logstats] INFO: Crawled 1305 pages (at 10 pages/min), scraped 1206 items (at 9 items/min)
2018-11-12 04:17:16 [scrapy.extensions.logstats] INFO: Crawled 1311 pages (at 6 pages/min), scraped 1214 items (at 8 items/min)
2018-11-12 04:18:16 [scrapy.extensions.logstats] INFO: Crawled 1318 pages (at 7 pages/min), scraped 1222 items (at 8 items/min)
/bin/sh: 1: kill: No such process

2018-11-12 04:19:12 [scrapy.extensions.logstats] INFO: Crawled 1325 pages (at 7 pages/min), scraped 1229 items (at 7 items/min)
2018-11-12 04:20:08 [scrapy.extensions.logstats] INFO: Crawled 1335 pages (at 10 pages/min), scraped 1236 items (at 7 items/min)
2018-11-12 04:21:11 [scrapy.extensions.logstats] INFO: Crawled 1341 pages (at 6 pages/min), scraped 1244 items (at 8 items/min)
2018-11-12 04:21:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.richtek.com/Applications/Mobile%20device-to%20be%20discussed/Wireless?utm_source=Banner&utm_medium=RTWWW&utm_campaign=WPS> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 04:21:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.richtek.com/Products/Switching%20Regulators/DC_DC%20StepDown%20Convertor/RT6210?utm_source=Banner&utm_medium=RTWWW&utm_campaign=RT6210> (referer: None)
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/scrapy/spiders/crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/EAGER/code/crawler/FirmDB/spiders/HTMLSpider.py", line 70, in parse_links
    referring_url = response.request.headers.get('Referer', None).decode('ASCII')
AttributeError: 'NoneType' object has no attribute 'decode'
2018-11-12 04:22:06 [scrapy.extensions.logstats] INFO: Crawled 1350 pages (at 9 pages/min), scraped 1249 items (at 5 items/min)
2018-11-12 04:23:00 [scrapy.extensions.logstats] INFO: Crawled 1355 pages (at 5 pages/min), scraped 1255 items (at 6 items/min)
2018-11-12 04:24:04 [scrapy.extensions.logstats] INFO: Crawled 1363 pages (at 8 pages/min), scraped 1262 items (at 7 items/min)
2018-11-12 04:25:16 [scrapy.extensions.logstats] INFO: Crawled 1370 pages (at 7 pages/min), scraped 1270 items (at 8 items/min)
       valid
Crawling page: https://www.richtek.com/Applications/USB%20PD%20Type-C%20Controller?sc_lang=en       valid
Crawling page: https://www.richtek.com/Design%20Support/Reference%20Design?sc_lang=en       valid
Crawling page: https://www.richtek.com/Design%20Support/Technical%20Document?sc_lang=en       valid
Crawling page: https://www.richtek.com/Applications/Notebook?sc_lang=en       valid
Crawling page: https://www.richtek.com/My%20Richtek/Privacy%20Policy?sc_lang=en       valid
Crawling page: https://www.richtek.com/Applications/Networked%20Commnucation?sc_lang=en       valid
Crawling page: https://www.richtek.com/About%20Richtek/News%20Release?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/Switching%20Regulators/DC_DC%20StepDown%20Convertor/RT6276ART6276B?sc_lang=en&activetab=prodcutordering       valid
Crawling page: https://www.richtek.com/Products/Switching%20Regulators/DC_DC%20StepDown%20Convertor/RT6276ART6276B?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/Audio/HV%20Audio%20Amplifier/RT9114B?sc_lang=en&activetab=prodcutordering       valid
Crawling page: https://www.richtek.com/Products/Power%20Management%20IC/RT5047B?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/Power%20Management%20IC/RT5047B?sc_lang=en&activetab=prodcutordering       valid
Crawling page: https://www.richtek.com/Products/Audio/HV%20Audio%20Amplifier/RT9114B?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/Power%20Management%20IC/RT5047BF?sc_lang=en&activetab=prodcutordering       valid
Crawling page: https://www.richtek.com/Products/Power%20Management%20IC/RT5047BF?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/Supervisor/OVP/RT9746H?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/Supervisor/OVP/RT9746H?sc_lang=en&activetab=prodcutordering       valid
Crawling page: https://www.richtek.com:443/Home/Design%20Support/Sample%20and%20Buy?ForceDevice=1&devicename=Richtekweb       valid
Crawling page: https://www.richtek.com/Products/Linear%20Regulator/Single%20Output%20Linear%20Regulator/RT9085A?sc_lang=en&activetab=prodcutordering       valid
Crawling page: https://www.richtek.com/Products/Switching%20Regulators/DC_DC%20StepDown%20Convertor/RT5707RT5707A?sc_lang=en&activetab=prodcutordering       valid
Crawling page: https://www.richtek.com/Products/Switching%20Regulators/DC_DC%20StepDown%20Convertor/RT5707RT5707A?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/Linear%20Regulator/Single%20Output%20Linear%20Regulator/RT9085A?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/Switching%20Regulators/DC_DC%20StepDown%20Convertor/RTQ2131B-QA?sc_lang=en&activetab=prodcutordering       valid
Crawling page: https://www.richtek.com/Products/DrMOS/RT9682ART9682B?sc_lang=en&activetab=prodcutordering       valid
Crawling page: https://www.richtek.com/Products/Switching%20Regulators/DC_DC%20StepDown%20Convertor/RTQ2131B-QA?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/DrMOS/RT9682ART9682B?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/Switching%20Regulators/DC_DC%20StepDown%20Convertor/RTQ2132B-QT?sc_lang=en&activetab=prodcutordering       valid
Crawling page: https://www.richtek.com/Products/MEMSandSensors/Vital%20Sign%20Monitor/RT1025?sc_lang=en&activetab=prodcutordering       valid
Crawling page: https://www.richtek.com/Products/MEMSandSensors/Vital%20Sign%20Monitor/RT1025?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/Linear%20Regulator/Single%20Output%20Linear%20Regulator/RTQ2569-QA?sc_lang=en&activetab=prodcutordering       valid
Crawling page: https://www.richtek.com/Products/Linear%20Regulator/Single%20Output%20Linear%20Regulator/RTQ2569-QA?sc_lang=en       valid
Crawling page: https://www.richtek.com/       valid
Crawling page: https://www.richtek.com/Products/Switching%20Regulators/DC_DC%20StepDown%20Convertor/RTQ2132B-QT?sc_lang=en       valid
Crawling page: https://www.richtek.com/Applications/Automotive?utm_source=Banner&utm_medium=RTWWW&utm_campaign=Automotive       valid
Crawling page: https://www.richtek.com/About%20Richtek/Newsletters?sc_lang=en       valid
Crawling page: https://www.richtek.com/My%20Richtek/Sample%20Cart?sc_lang=en       valid
Crawling page: https://www.richtek.com/My%20Richtek/Newsletters?sc_lang=en       valid
Crawling page: https://www.richtek.com/My%20Richtek/Order%20History?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/MEMSandSensors/Vital%20Sign%20Monitor/RT1025?utm_source=Banner&utm_medium=RTWWW&utm_campaign=RT1025       valid
Crawling page: https://www.richtek.com/My%20Richtek/Areas%20of%20Insterest?sc_lang=en       valid
Crawling page: https://www.richtek.com/My%20Richtek/Datasheet%20Alert?sc_lang=en       valid
Crawling page: https://www.richtek.com/My%20Richtek/Index?sc_lang=en       valid
Crawling page: https://www.richtek.com/My%20Richtek/My%20Profile?sc_lang=en       valid
Crawling page: https://www.richtek.com/My%20Richtek/Register?sc_lang=en       valid
Crawling page: https://www.richtek.com/My%20Richtek/Login?sc_lang=en       valid
Crawling page: https://www.richtek.com/Design%20Support/Design%20Tips?sc_lang=en       valid
Crawling page: https://www.richtek.com/Design%20Support/Videos?sc_lang=en       valid
Crawling page: https://www.richtek.com/Applications/LED%20Lighting?sc_lang=en       valid
Crawling page: https://www.richtek.com/Applications/Mobile%20device-to%20be%20discussed/Wireless?utm_source=Banner&utm_medium=RTWWW&utm_campaign=WPS       Crawling page: https://www.richtek.com/Products/Switching%20Regulators/DC_DC%20StepDown%20Convertor/RT6210?utm_source=Banner&utm_medium=RTWWW&utm_campaign=RT6210       Crawling page: https://www.richtek.com/Applications/Motherboard?sc_lang=en       valid
Crawling page: https://www.richtek.com/Applications/Large%20Flat%20Panel%20Display?sc_lang=en       valid
Crawling page: https://www.richtek.com/Applications/TV?sc_lang=en       valid
Crawling page: https://www.richtek.com/Applications/Vcore?sc_lang=en       valid
Crawling page: https://www.richtek.com/Applications/IoT?sc_lang=en       valid
Crawling page: https://www.richtek.com/Applications/Mobile%20device-to%20be%20discussed?sc_lang=en       valid
Crawling page: https://www.richtek.com/Applications/Automotive?sc_lang=en       valid
Crawling page: https://www.richtek.com/Applications/BLDC?sc_lang=en       valid
Crawling page: https://www.richtek.com/Applications/AC-DC%20Power%20Supply?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/Motor?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/MEMSandSensors?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/USB%20PD%20IF?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/Audio?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/Power%20Management%20IC?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/PC%20System%20Buck%20Controller?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/Supervisor?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/AC_DC?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/Operational%20Amplifier?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/Power%20Switch?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/DrMOS?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/MOSFET%20Driver?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/Battery%20Management?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/Vcore?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/LED%20Driver?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/Switching%20Regulators?sc_lang=en       valid
Crawling page: https://www.richtek.com/Parametric%20Search?sc_lang=en       valid
Crawling page: https://www.richtek.com/My%20Richtek?sc_lang=en       valid
Crawling page: https://www.richtek.com/History?sc_lang=en2018-11-12 04:26:28 [scrapy.extensions.logstats] INFO: Crawled 1376 pages (at 6 pages/min), scraped 1277 items (at 7 items/min)
2018-11-12 04:26:42 [scrapy.core.engine] INFO: Closing spider (finished)
2018-11-12 04:26:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 202,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 8,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 35,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 159,
 'downloader/request_bytes': 992429,
 'downloader/request_count': 1783,
 'downloader/request_method_count/GET': 1783,
 'downloader/response_bytes': 53470512,
 'downloader/response_count': 1581,
 'downloader/response_status_count/200': 1372,
 'downloader/response_status_count/301': 148,
 'downloader/response_status_count/302': 41,
 'downloader/response_status_count/404': 3,
 'downloader/response_status_count/502': 17,
 'dupefilter/filtered': 180,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 11, 12, 4, 26, 42, 303681),
 'httperror/response_ignored_count': 4,
 'httperror/response_ignored_status_count/404': 3,
 'httperror/response_ignored_status_count/502': 1,
 'item_scraped_count': 1279,
 'log_count/ERROR': 95,
 'log_count/INFO': 150,
 'log_count/WARNING': 1,
 'memusage/max': 233906176,
 'memusage/startup': 97689600,
 'offsite/domains': 107,
 'offsite/filtered': 232,
 'request_depth_max': 1,
 'response_received_count': 1376,
 'retry/count': 194,
 'retry/max_reached': 25,
 'retry/reason_count/502 Bad Gateway': 16,
 'retry/reason_count/twisted.internet.error.ConnectError': 8,
 'retry/reason_count/twisted.internet.error.TimeoutError': 27,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 143,
 'scheduler/dequeued': 1783,
 'scheduler/dequeued/memory': 1783,
 'scheduler/enqueued': 1783,
 'scheduler/enqueued/memory': 1783,
 'spider_exceptions/AttributeError': 52,
 'spider_exceptions/TimeoutException': 10,
 'spider_exceptions/TypeError': 1,
 'spider_exceptions/WebDriverException': 2,
 'start_time': datetime.datetime(2018, 11, 12, 1, 16, 59, 937980)}
2018-11-12 04:26:42 [scrapy.core.engine] INFO: Spider closed (finished)
       valid
Crawling page: https://www.richtek.com/Applications?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products?sc_lang=en       valid
Crawling page: https://www.richtek.com/Design%20Support?sc_lang=en       valid
Crawling page: https://www.richtek.com/Products/Linear%20Regulator?sc_lang=en       valid
Crawling page: https://www.richtek.com/About%20Richtek?sc_lang=en       valid
Crawling page: https://www.richtek.com/Contact%20Us?sc_lang=en       valid
